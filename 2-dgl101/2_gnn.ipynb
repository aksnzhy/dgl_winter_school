{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-supervised Community Detection using Graph Neural Networks\n",
    "\n",
    "Almost every computer 101 class starts with a \"Hello World\" example. Like MNIST for deep learning, in graph domain we have the Zachary's Karate Club problem. The karate club is a social network that includes 34 members and documents pairwise links between members who interact outside the club. The club later divides into two communities led by the instructor (node 0) and the club president (node 33). The network is visualized as follows with the color indicating the community.\n",
    "\n",
    "<img src='../asset/karat_club.png' align='center' width=\"400px\" height=\"300px\" />\n",
    "\n",
    "In this tutorial, you will learn:\n",
    "\n",
    "* Formulate the community detection problem as a semi-supervised node classification task.\n",
    "* Build a GraphSAGE model, a popular Graph Neural Network architecture proposed by [Hamilton et al.](https://arxiv.org/abs/1706.02216)\n",
    "* Train the model and understand the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community detection as node classification\n",
    "\n",
    "The study of community structure in graphs has a long history. Many proposed methods are *unsupervised* (or *self-supervised* by recent definition), where the model predicts the community labels only by connectivity. Recently, [Kipf et al.,](https://arxiv.org/abs/1609.02907) proposed to formulate the community detection problem as a semi-supervised node classification task. With the help of only a small portion of labeled nodes, a GNN can accurately predict the community labels of the others.\n",
    "\n",
    "In this tutorial, we apply Kipf's setting to the Zachery's Karate Club network to predict the community membership, where only the labels of a few nodes are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the graph and node labels as is covered in the [last session](./1_load_data.ipynb). Here, we have provided you a function for loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=34, num_edges=156,\n",
      "      ndata_schemes={'club': Scheme(shape=(), dtype=torch.int64), 'club_onehot': Scheme(shape=(2,), dtype=torch.int64)}\n",
      "      edata_schemes={})\n"
     ]
    }
   ],
   "source": [
    "from tutorial_utils import load_zachery\n",
    "\n",
    "# ----------- 0. load graph -------------- #\n",
    "g = load_zachery()\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original Zachery's Karate Club graph, nodes are feature-less. (The `'Age'` attribute is an artificial one mainly for tutorial purposes). For feature-less graph, a common practice is to use an embedding weight that is updated during training for every node.\n",
    "\n",
    "We can use PyTorch's `Embedding` module to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0198, -0.2025, -0.2163,  0.3326, -0.0799],\n",
      "        [ 0.2821,  0.2558,  0.1610, -0.3752,  0.3142],\n",
      "        [ 0.1328, -0.0017, -0.1863, -0.2835,  0.1020],\n",
      "        [ 0.1286,  0.1920, -0.0098, -0.0716,  0.1045],\n",
      "        [-0.1406, -0.2568,  0.3155, -0.2180,  0.0859],\n",
      "        [-0.2981,  0.1447,  0.0224,  0.0149, -0.1057],\n",
      "        [ 0.2976,  0.2175,  0.3228,  0.3839, -0.2936],\n",
      "        [ 0.2523,  0.1296, -0.0392,  0.1322,  0.1846],\n",
      "        [-0.3500, -0.3145, -0.3418, -0.1453, -0.2096],\n",
      "        [ 0.0257,  0.2476, -0.0852,  0.3569,  0.2647],\n",
      "        [ 0.1103, -0.3191,  0.1200,  0.3574,  0.2689],\n",
      "        [ 0.1190, -0.2395,  0.2617,  0.3697, -0.1608],\n",
      "        [ 0.0281,  0.1334,  0.0420, -0.2567, -0.1763],\n",
      "        [ 0.0019,  0.0061,  0.3576,  0.1254, -0.1403],\n",
      "        [-0.2442, -0.2989, -0.0877,  0.2636, -0.0194],\n",
      "        [-0.1751,  0.0462,  0.0049,  0.1428, -0.1204],\n",
      "        [ 0.1387,  0.3585,  0.3322,  0.3719, -0.0400],\n",
      "        [ 0.1394, -0.1515, -0.0665,  0.0098,  0.0400],\n",
      "        [-0.0606, -0.3703, -0.2932,  0.3377, -0.1488],\n",
      "        [ 0.0992, -0.3322,  0.0066,  0.2078, -0.0306],\n",
      "        [ 0.1117,  0.3153, -0.3220,  0.2605,  0.3462],\n",
      "        [-0.3114, -0.1643,  0.3691,  0.1036, -0.3810],\n",
      "        [-0.0960,  0.3699, -0.0635,  0.0374, -0.2335],\n",
      "        [ 0.3104,  0.1520,  0.2729,  0.2239, -0.0619],\n",
      "        [-0.1219, -0.0016,  0.0659, -0.3882,  0.3815],\n",
      "        [ 0.3394, -0.0314, -0.1750, -0.2456,  0.0805],\n",
      "        [ 0.0811, -0.3454, -0.3030,  0.2013, -0.0158],\n",
      "        [-0.2131,  0.0609,  0.2153, -0.0541, -0.2833],\n",
      "        [-0.2109,  0.2805,  0.2973,  0.1568,  0.3902],\n",
      "        [-0.0870, -0.1171, -0.1106,  0.3334, -0.0519],\n",
      "        [-0.0953, -0.0694,  0.3884, -0.1189, -0.1890],\n",
      "        [ 0.2817,  0.3760, -0.0583,  0.0830,  0.0258],\n",
      "        [ 0.3332,  0.0265, -0.2368,  0.3168, -0.0520],\n",
      "        [-0.3614, -0.3849, -0.3456,  0.0756,  0.1579]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# ----------- 1. node features -------------- #\n",
    "node_embed = nn.Embedding(g.number_of_nodes(), 5)  # Every node has an embedding of size 5.\n",
    "inputs = node_embed.weight                         # Use the embedding weight as the node features.\n",
    "nn.init.xavier_uniform_(inputs)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The community label is stored in the `'club'` node feature (0 for instructor, 1 for club president). Only nodes 0 and 33 are labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#nodes: 34\n",
      "#labeled nodes: 5\n",
      "Labels tensor([0, 0, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "random.seed(0)\n",
    "labels = g.ndata['club']\n",
    "print('#nodes:', len(labels))\n",
    "train_nodes = np.unique([0, 33] + random.sample(range(len(labels)), 3))\n",
    "test_nodes = np.delete(np.arange(len(labels)), train_nodes)\n",
    "print('#labeled nodes:', len(train_nodes))\n",
    "print('Labels', labels[train_nodes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message passing and GNNs\n",
    "\n",
    "DGL follows the *message passing paradigm* inspired by the Message Passing Neural Network proposed by [Gilmer et al.](https://arxiv.org/abs/1704.01212) Essentially, they found many GNN models can fit into the following framework:\n",
    "\n",
    "$$\n",
    "m_{u\\sim v}^{(l)} = M^{(l)}\\left(h_v^{(l-1)}, h_u^{(l-1)}, e_{u\\sim v}^{(l-1)}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "m_{v}^{(l)} = \\sum_{u\\in\\mathcal{N}(v)}m_{u\\sim v}^{(l)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_v^{(l)} = U^{(l)}\\left(h_v^{(l-1)}, m_v^{(l)}\\right)\n",
    "$$\n",
    "\n",
    "where DGL calls $M^{(l)}$ the *message function* and $\\sum$ the *reduce function*.  Note that $\\sum$ here can represent any function and is not necessarily a summation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a GraphSAGE model\n",
    "\n",
    "Our model consists of two layers, each computes new node representations by aggregating neighbor information. The equations are:\n",
    "\n",
    "$$\n",
    "h_{\\mathcal{N}(v)}^k\\leftarrow \\text{AGGREGATE}_k\\{h_u^{k-1},\\forall u\\in\\mathcal{N}(v)\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_v^k\\leftarrow \\sigma\\left(W^k\\cdot \\text{CONCAT}(h_v^{k-1}, h_{\\mathcal{N}(v)}^k) \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "You can see that message passing is directional: the message sent from one node $u$ to other node $v$ is not necessarily the same as the other message sent from node $v$ to node $u$ in the opposite direction.\n",
    "\n",
    "DGL graphs provide two members `srcdata` and `dstdata` for the purpose of message passing.  You first put the input node features in `srcdata`.  After you perform message passing, you can retrieve the result of message passing from `dstdata`.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Note: </b>In full graph message passing, both the input nodes and the output nodes are the full node set.  Therefore, <code>srcdata</code> and <code>dstdata</code> in homogeneous graph (i.e. with only one node type and one edge type) are identical to <code>ndata</code>.  All graphs in this tutorial section are homogeneous.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "\n",
    "class SAGEConv(nn.Module):\n",
    "    \"\"\"Graph convolution module used by the GraphSAGE model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_feat : int\n",
    "        Input feature size.\n",
    "    out_feat : int\n",
    "        Output feature size.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_feat, out_feat):\n",
    "        super(SAGEConv, self).__init__()\n",
    "        # A linear submodule for projecting the input and neighbor feature to the output.\n",
    "        self.linear = nn.Linear(in_feat * 2, out_feat)\n",
    "    \n",
    "    def forward(self, g, h):\n",
    "        \"\"\"Forward computation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        g : Graph\n",
    "            The input graph.\n",
    "        h : Tensor\n",
    "            The input node feature.\n",
    "        \"\"\"\n",
    "        with g.local_scope():\n",
    "            g.srcdata['h'] = h\n",
    "            # update_all is a message passing API.\n",
    "            # equation 1 and 2\n",
    "            g.update_all(message_func=fn.copy_u('h', 'm'), reduce_func=fn.mean('m', 'h_neigh'))\n",
    "            # equation 3\n",
    "            h_neigh = g.dstdata['h_neigh']\n",
    "            h_total = torch.cat([h, h_neigh], dim=1)\n",
    "            return F.relu(self.linear(h_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a multi-layer GraphSage model.\n",
    "\n",
    "<img src='../asset/multi_layer.png' align='center' width=\"600px\" height=\"450px\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- 2. create model -------------- #\n",
    "# build a two-layer GraphSAGE model\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats)\n",
    "        self.conv2 = SAGEConv(h_feats, num_classes)\n",
    "    \n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "    \n",
    "# Create the model with given dimensions \n",
    "# input layer dimension: 5, node embeddings\n",
    "# hidden layer dimension: 16\n",
    "# output layer dimension: 2, the two classes, 0 and 1\n",
    "net = GraphSAGE(5, 16, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 0.7718736529350281\n",
      "In epoch 5, loss: 0.5637348890304565\n",
      "In epoch 10, loss: 0.40559178590774536\n",
      "In epoch 15, loss: 0.2468864917755127\n",
      "In epoch 20, loss: 0.11854676902294159\n",
      "In epoch 25, loss: 0.04251787066459656\n",
      "In epoch 30, loss: 0.012699922546744347\n",
      "In epoch 35, loss: 0.004077625460922718\n",
      "In epoch 40, loss: 0.0016410115640610456\n",
      "In epoch 45, loss: 0.0008527038735337555\n",
      "In epoch 50, loss: 0.0005473890341818333\n",
      "In epoch 55, loss: 0.0004072349111083895\n",
      "In epoch 60, loss: 0.0003339156974107027\n",
      "In epoch 65, loss: 0.0002915450604632497\n",
      "In epoch 70, loss: 0.0002648533845786005\n",
      "In epoch 75, loss: 0.00024674052838236094\n",
      "In epoch 80, loss: 0.00023348924878519028\n",
      "In epoch 85, loss: 0.00022319315758068115\n",
      "In epoch 90, loss: 0.0002145891048712656\n",
      "In epoch 95, loss: 0.00020715282880701125\n"
     ]
    }
   ],
   "source": [
    "# ----------- 3. set up loss and optimizer -------------- #\n",
    "# in this case, loss will in training loop\n",
    "optimizer = torch.optim.Adam(itertools.chain(net.parameters(), node_embed.parameters()), lr=0.01)\n",
    "\n",
    "# ----------- 4. training -------------------------------- #\n",
    "all_logits = []\n",
    "for e in range(100):\n",
    "    # forward\n",
    "    logits = net(g, inputs)\n",
    "    \n",
    "    # compute loss\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[train_nodes], labels[train_nodes])\n",
    "    \n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    all_logits.append(logits.detach())\n",
    "    \n",
    "    if e % 5 == 0:\n",
    "        print('In epoch {}, loss: {}'.format(e, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.7941176470588235\n"
     ]
    }
   ],
   "source": [
    "# ----------- 5. check results ------------------------ #\n",
    "pred = torch.argmax(logits, axis=1)\n",
    "print('Accuracy', (pred == labels)[test_nodes].sum().item() / len(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement GraphSage with nn modules\n",
    "\n",
    "DGL provides implementation of many popular neighbor aggregation modules. . They all can be invoked easily with one line of code. See the full list of supported [graph convolution modules](https://docs.dgl.ai/api/python/nn.pytorch.html#module-dgl.nn.pytorch.conv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import SAGEConv\n",
    "\n",
    "# ----------- 2. create model -------------- #\n",
    "# build a two-layer GraphSAGE model\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\n",
    "        self.conv2 = SAGEConv(h_feats, num_classes, 'mean')\n",
    "    \n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "    \n",
    "# Create the model with given dimensions \n",
    "# input layer dimension: 5, node embeddings\n",
    "# hidden layer dimension: 16\n",
    "# output layer dimension: 2, the two classes, 0 and 1\n",
    "net = GraphSAGE(5, 16, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Play with the GNN models by using other [graph convolution modules](https://docs.dgl.ai/api/python/nn.pytorch.html#module-dgl.nn.pytorch.conv). For example, how about graph attention networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
