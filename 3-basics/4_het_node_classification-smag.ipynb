{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Semi-supervised node classification using Heterogenous Graph Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this tutorial, you will learn:\n",
    "\n",
    "* Build a relational graph neural network model, a popular GNN architecture proposed by [Schlichtkrull et al.](https://arxiv.org/abs/1703.06103)\n",
    "* Train the model and understand the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the graph and node labels as is covered in the [last session](./1_load_data.ipynb). Here, we have provided you a function for loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data.rdf import AMDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes={'author': 1579, 'field_of_study': 584, 'institution': 421, 'paper': 99},\n",
      "      num_edges={('author', 'affiliated_with', 'institution'): 2312, ('author', 'writes', 'paper'): 1790, ('paper', 'cites', 'paper'): 197, ('paper', 'has_topic', 'field_of_study'): 959},\n",
      "      metagraph=[('author', 'institution', 'affiliated_with'), ('author', 'paper', 'writes'), ('paper', 'paper', 'cites'), ('paper', 'field_of_study', 'has_topic')])\n"
     ]
    }
   ],
   "source": [
    "node_features = pd.read_csv(\"mag_small/node-feat.csv\").values[:,1:].squeeze()\n",
    "\n",
    "node_labels = pd.read_csv(\"mag_small/node-label.csv\").values[:,1:].squeeze()\n",
    "\n",
    "author_write_paper = pd.read_csv(\"mag_small/author_write_paper_edge.csv\")\n",
    "author_affiliated_with_institution = pd.read_csv(\"mag_small/author_affiliated_with_institution_edge.csv\")\n",
    "paper_cites_paper = pd.read_csv(\"mag_small/paper_cites_paper_edge.csv\")\n",
    "paper_has_topic_field_of_study = pd.read_csv(\"mag_small/paper_has_topic_field_of_study_edge.csv\")\n",
    "\n",
    "edges = {\n",
    "    ('author', 'affiliated_with', 'institution'): list(author_affiliated_with_institution.itertuples(index=False)),\n",
    "    ('author', 'writes', 'paper'): list(author_write_paper.itertuples(index=False)),\n",
    "    ('paper', 'cites', 'paper'): list(paper_cites_paper.itertuples(index=False)),\n",
    "    ('paper', 'has_topic', 'field_of_study'): list(paper_has_topic_field_of_study.itertuples(index=False)),\n",
    "}\n",
    "g = dgl.heterograph(edges)\n",
    "print(g)\n",
    "gpu=-1\n",
    "category = 'paper'\n",
    "num_classes = len(np.unique(node_labels))\n",
    "train_pct=0.9\n",
    "ids=list(range(len(node_labels)))\n",
    "random.shuffle(ids)\n",
    "node_features=torch.tensor(node_features).float()\n",
    "train_idx = torch.tensor(ids[:round(train_pct*len(ids))])\n",
    "test_idx = torch.tensor(ids[round(train_pct*len(ids)):])\n",
    "trans_id={}\n",
    "c=0\n",
    "for i in node_labels:\n",
    "    if i not in trans_id:\n",
    "        trans_id[i]=c\n",
    "        c+=1\n",
    "n_labels=[]\n",
    "for i in node_labels:\n",
    "    n_labels.append(trans_id[i])\n",
    "node_labels=n_labels\n",
    "labels = torch.tensor(node_labels)\n",
    "category_id = len(g.ntypes)\n",
    "for i, ntype in enumerate(g.ntypes):\n",
    "        if ntype == category:\n",
    "            category_id = i\n",
    "\n",
    "# split dataset into train, validate, test\n",
    "val_idx = train_idx[:len(train_idx) // 8]\n",
    "train_idx = train_idx[len(train_idx) // 8:]\n",
    "\n",
    "# check cuda\n",
    "use_cuda = gpu >= 0 and torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "        torch.cuda.set_device(gpu)\n",
    "        g = g.to('cuda:%d' % gpu)\n",
    "        labels = labels.cuda()\n",
    "        train_idx = train_idx.cuda()\n",
    "        test_idx = test_idx.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a HeteroGraphConv model\n",
    "\n",
    "HeteroGraphConv is a module-level encapsulation to run DGL NN module on heterogeneous graphs. The implementation logic is the same as message passing level API multi_update_all(), including:\n",
    "\n",
    "DGL NN module within each relation ùëü.\n",
    "\n",
    "Reduction that merges the results on the same node type from multiple relations.\n",
    "\n",
    "$$\n",
    "h_{dst}^{(l+1)} = \\underset{r\\in\\mathcal{R}, r_{dst}=dst}{AGG} (f_r(g_r, h_{r_{src}}^l, h_{r_{dst}}^l))$$\n",
    "\n",
    "https://docs.dgl.ai/guide/nn-heterograph.html?highlight=heterogenous%20graphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your graph is heterogeneous, you may want to gather message from neighbors along all edge types. You can use the module dgl.nn.pytorch.HeteroGraphConv (also available in MXNet and Tensorflow) to perform message passing on all edge types, then combining different graph convolution modules for each edge type.\n",
    "\n",
    "The following code will define a heterogeneous graph convolution module that first performs a separate graph convolution on each edge type, then sums the message aggregations on each edge type as the final result for all node types.\n",
    "\n",
    "dgl.nn.HeteroGraphConv takes in a dictionary of node types and node feature tensors as input, and returns another dictionary of node types and node features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- 2. create model -------------- #\n",
    "# build a two-layer RGCN model\n",
    "import dgl.nn as dglnn\n",
    "\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, rel_names):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(in_feats, hid_feats)\n",
    "            for rel in rel_names}, aggregate='sum')\n",
    "        self.conv2 = dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(hid_feats, out_feats)\n",
    "            for rel in rel_names}, aggregate='sum')\n",
    "\n",
    "    def forward(self, graph, inputs):\n",
    "        # inputs are features of nodes\n",
    "        h = self.conv1(graph, inputs)\n",
    "        h = {k: F.relu(v) for k, v in h.items()}\n",
    "        h = self.conv2(graph, h)\n",
    "        return h\n",
    "class NodeEmbed(nn.Module):\n",
    "    def __init__(self, num_nodes, embed_size):\n",
    "        super(NodeEmbed, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.node_embeds = nn.ParameterDict()\n",
    "        self.num_nodes=num_nodes\n",
    "        for ntype in num_nodes:\n",
    "            embed = nn.Parameter(torch.Tensor(g.number_of_nodes(ntype), self.embed_size))\n",
    "            nn.init.xavier_uniform_(embed, gain=nn.init.calculate_gain('relu'))\n",
    "            self.node_embeds[ntype] = embed\n",
    "    \n",
    "    def forward(self):\n",
    "\n",
    "        return self.node_embeds\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGCN(\n",
      "  (conv1): HeteroGraphConv(\n",
      "    (mods): ModuleDict(\n",
      "      (affiliated_with): GraphConv(in=128, out=16, normalization=both, activation=None)\n",
      "      (writes): GraphConv(in=128, out=16, normalization=both, activation=None)\n",
      "      (cites): GraphConv(in=128, out=16, normalization=both, activation=None)\n",
      "      (has_topic): GraphConv(in=128, out=16, normalization=both, activation=None)\n",
      "    )\n",
      "  )\n",
      "  (conv2): HeteroGraphConv(\n",
      "    (mods): ModuleDict(\n",
      "      (affiliated_with): GraphConv(in=16, out=46, normalization=both, activation=None)\n",
      "      (writes): GraphConv(in=16, out=46, normalization=both, activation=None)\n",
      "      (cites): GraphConv(in=16, out=46, normalization=both, activation=None)\n",
      "      (has_topic): GraphConv(in=16, out=46, normalization=both, activation=None)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_nodes = {ntype: g.number_of_nodes(ntype) for ntype in g.ntypes}\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device=\"cpu\"\n",
    "h_embed=128\n",
    "embed = NodeEmbed(num_nodes, h_embed).to(device)\n",
    "ntype_with_features='paper'\n",
    "h_hidden=16\n",
    "model = RGCN(h_embed, h_hidden, num_classes,g.etypes).to(device)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Train Acc: 0.0128 | Train Loss: 3.8620 | Valid Acc: 0.0000 | Valid loss: 3.8624\n",
      "Epoch 00005 | Train Acc: 0.2564 | Train Loss: 3.1242 | Valid Acc: 0.0909 | Valid loss: 4.1033\n",
      "Epoch 00010 | Train Acc: 0.3974 | Train Loss: 2.2899 | Valid Acc: 0.0909 | Valid loss: 4.7368\n",
      "Epoch 00015 | Train Acc: 0.5897 | Train Loss: 1.5503 | Valid Acc: 0.0000 | Valid loss: 6.0581\n",
      "Epoch 00020 | Train Acc: 0.7949 | Train Loss: 1.0077 | Valid Acc: 0.0000 | Valid loss: 7.9902\n",
      "Epoch 00025 | Train Acc: 0.8205 | Train Loss: 0.7557 | Valid Acc: 0.0000 | Valid loss: 10.1148\n",
      "Epoch 00030 | Train Acc: 0.8462 | Train Loss: 0.6441 | Valid Acc: 0.0000 | Valid loss: 12.1000\n",
      "Epoch 00035 | Train Acc: 0.8462 | Train Loss: 0.6067 | Valid Acc: 0.0000 | Valid loss: 13.7175\n",
      "Epoch 00040 | Train Acc: 0.8462 | Train Loss: 0.5891 | Valid Acc: 0.0000 | Valid loss: 14.9389\n",
      "Epoch 00045 | Train Acc: 0.8462 | Train Loss: 0.5781 | Valid Acc: 0.0000 | Valid loss: 15.7831\n"
     ]
    }
   ],
   "source": [
    "# ----------- 3. set up loss and optimizer -------------- #\n",
    "# in this case, loss will in training loop\n",
    "optimizer = torch.optim.Adam(itertools.chain(model.parameters(), embed.parameters()), lr=0.01)\n",
    "\n",
    "# ----------- 4. training -------------------------------- #\n",
    "all_logits = []\n",
    "for e in range(50):\n",
    "    # forward\n",
    "    # Get node embeddings for node types that don't have input features and copy to gpu\n",
    "    embeddings = {ntype: node_embedding.cuda().to(device) for ntype, node_embedding in embed().items() if ntype!=ntype_with_features}\n",
    "            \n",
    "    # Get input features for node type 'paper' which has input features\n",
    "    embeds = {'paper': node_features.to(device)}\n",
    "            \n",
    "    # Merge feature inputs with input that has features\n",
    "    embeds.update(embeddings)\n",
    "    logits= model(g,embeds)[category]\n",
    "    \n",
    "    # compute loss\n",
    "    loss = F.cross_entropy(logits[train_idx], labels[train_idx])\n",
    "    \n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    all_logits.append(logits.detach())\n",
    "    \n",
    "    if e % 5 == 0:\n",
    "        train_acc = torch.sum(logits[train_idx].argmax(dim=1) == labels[train_idx]).item() / len(train_idx)\n",
    "        val_loss = F.cross_entropy(logits[val_idx], labels[val_idx])\n",
    "        val_acc = torch.sum(logits[val_idx].argmax(dim=1) == labels[val_idx]).item() / len(val_idx)\n",
    "        print(\"Epoch {:05d} | Train Acc: {:.4f} | Train Loss: {:.4f} | Valid Acc: {:.4f} | Valid loss: {:.4f}\".\n",
    "              format(e, train_acc, loss.item(), val_acc, val_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.0000 | Test loss: 12.4099\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------- 5. check results ------------------------ #\n",
    "    model.eval()\n",
    "    embed.eval()\n",
    "    embeds = embed()\n",
    "    logits= model.forward(g,embeds)[category]\n",
    "    test_loss = F.cross_entropy(logits[test_idx], labels[test_idx])\n",
    "    test_acc = torch.sum(logits[test_idx].argmax(dim=1) == labels[test_idx]).item() / len(test_idx)\n",
    "    print(\"Test Acc: {:.4f} | Test loss: {:.4f}\".format(test_acc, test_loss.item()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
