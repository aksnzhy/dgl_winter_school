{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Semi-supervised node classification using Heterogenous Graph Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this tutorial, you will learn:\n",
    "\n",
    "* Build a relational graph neural network model, a popular GNN architecture proposed by [Schlichtkrull et al.](https://arxiv.org/abs/1703.06103)\n",
    "* Train the model and understand the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the graph and node labels as is covered in the last session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "from dgl.data.rdf import AIFBDataset\n",
    "\n",
    "dataset = AIFBDataset()\n",
    "g = dataset[0]\n",
    "\n",
    "category = dataset.predict_category\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "# obtain the training testing splits stored as graph node attributes\n",
    "train_mask = g.nodes[category].data.pop('train_mask')\n",
    "test_mask = g.nodes[category].data.pop('test_mask')\n",
    "train_idx = torch.nonzero(train_mask, as_tuple=False).squeeze()\n",
    "test_idx = torch.nonzero(test_mask, as_tuple=False).squeeze()\n",
    "labels = g.nodes[category].data.pop('labels')\n",
    "\n",
    "# split dataset into train, validate, test\n",
    "val_idx = train_idx[:len(train_idx) // 5]\n",
    "train_idx = train_idx[len(train_idx) // 5:]\n",
    "\n",
    "# check cuda\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "g = g.to(device)\n",
    "labels = labels.to(device)\n",
    "train_idx = train_idx.to(device)\n",
    "test_idx = test_idx.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a HeteroGraphConv model\n",
    "\n",
    "- HeteroGraphConv is a encapsulation to run DGL NN module on heterogeneous graphs. \n",
    "- A DGL NN module has to defined per relation ùëü.\n",
    "- A reduction function to merge the results on the same node type from multiple relations.\n",
    "\n",
    "$$\n",
    "h_{dst}^{(l+1)} = \\underset{r\\in\\mathcal{R}, r_{dst}=dst}{AGG} (f_r(g_r, h_{r_{src}}^l, h_{r_{dst}}^l))$$\n",
    "\n",
    "See also the [link](https://docs.dgl.ai/guide/nn-heterograph.html?highlight=heterogenous%20graphs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- 2. create model -------------- #\n",
    "# build a two-layer RGCN model\n",
    "import dgl.nn as dglnn\n",
    "\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, rel_names):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(in_feats, hid_feats)\n",
    "            for rel in rel_names}, aggregate='sum')\n",
    "        self.conv2 = dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(hid_feats, out_feats)\n",
    "            for rel in rel_names}, aggregate='sum')\n",
    "\n",
    "    def forward(self, graph, inputs):\n",
    "        # inputs are features of nodes\n",
    "        h = self.conv1(graph, inputs)\n",
    "        h = {k: F.relu(v) for k, v in h.items()}\n",
    "        h = self.conv2(graph, h)\n",
    "        return h\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Performs a separate graph convolution on each edge type\n",
    "- Sums the message aggregations on each edge type as the final result for all node types.\n",
    "- HeteroGraphConv takes in a dictionary of node types and node feature tensors as input, and returns another dictionary of node types and node features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since AIFB does not have node feature we will use learnable embeddings.\n",
    "- In heterogenous graphs a dictionary of embeddings is used.\n",
    "- The embeddings will be updated on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEmbed(nn.Module):\n",
    "    def __init__(self, num_nodes, embed_size,decice):\n",
    "        super(NodeEmbed, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.node_embeds = nn.ModuleDict()\n",
    "        self.device=device\n",
    "        self.num_nodes=num_nodes\n",
    "        for ntype in num_nodes:\n",
    "            node_embed = torch.nn.Embedding(num_nodes[ntype], self.embed_size)\n",
    "            nn.init.uniform_(node_embed.weight, -1.0, 1.0)\n",
    "            self.node_embeds[str(ntype)] = node_embed\n",
    "    \n",
    "    def forward(self):\n",
    "        embeds = {}\n",
    "        num_nodes=self.num_nodes\n",
    "        for ntype in num_nodes:\n",
    "            embeds[ntype] = self.node_embeds[ntype](torch.tensor(list(range(num_nodes[ntype]))).to(self.device))\n",
    "        return embeds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = {ntype: g.number_of_nodes(ntype) for ntype in g.ntypes}\n",
    "\n",
    "h_hidden=16\n",
    "embed = NodeEmbed(num_nodes, h_hidden,device).to(device)\n",
    "model = RGCN(h_hidden, h_hidden, num_classes,g.etypes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Train Acc: 0.2054 | Train Loss: 2.6939 | Valid Acc: 0.3214 | Valid loss: 2.1187\n",
      "Epoch 00005 | Train Acc: 0.9018 | Train Loss: 0.3326 | Valid Acc: 0.6071 | Valid loss: 1.4107\n",
      "Epoch 00010 | Train Acc: 0.9554 | Train Loss: 0.1472 | Valid Acc: 0.6786 | Valid loss: 1.1779\n",
      "Epoch 00015 | Train Acc: 0.9643 | Train Loss: 0.1166 | Valid Acc: 0.7500 | Valid loss: 1.1948\n",
      "Epoch 00020 | Train Acc: 0.9643 | Train Loss: 0.0915 | Valid Acc: 0.8214 | Valid loss: 1.2564\n",
      "Epoch 00025 | Train Acc: 0.9821 | Train Loss: 0.0703 | Valid Acc: 0.8214 | Valid loss: 1.3179\n",
      "Epoch 00030 | Train Acc: 0.9821 | Train Loss: 0.0534 | Valid Acc: 0.8214 | Valid loss: 1.3900\n",
      "Epoch 00035 | Train Acc: 0.9821 | Train Loss: 0.0382 | Valid Acc: 0.8214 | Valid loss: 1.5029\n",
      "Epoch 00040 | Train Acc: 0.9911 | Train Loss: 0.0250 | Valid Acc: 0.7500 | Valid loss: 1.6442\n",
      "Epoch 00045 | Train Acc: 1.0000 | Train Loss: 0.0151 | Valid Acc: 0.6786 | Valid loss: 1.7814\n"
     ]
    }
   ],
   "source": [
    "# ----------- 3. set up optimizer -------------- #\n",
    "\n",
    "optimizer = torch.optim.Adam(itertools.chain(model.parameters(), embed.parameters()), lr=0.01)\n",
    "\n",
    "# ----------- 4. training -------------------------------- #\n",
    "all_logits = []\n",
    "for e in range(50):\n",
    "    # forward\n",
    "    embeds = embed()\n",
    "    logits= model(g,embeds)[category]\n",
    "    \n",
    "    # compute loss\n",
    "    loss = F.cross_entropy(logits[train_idx], labels[train_idx])\n",
    "    \n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    all_logits.append(logits.detach())\n",
    "    \n",
    "    if e % 5 == 0:\n",
    "        train_acc = torch.sum(logits[train_idx].argmax(dim=1) == labels[train_idx]).item() / len(train_idx)\n",
    "        val_loss = F.cross_entropy(logits[val_idx], labels[val_idx])\n",
    "        val_acc = torch.sum(logits[val_idx].argmax(dim=1) == labels[val_idx]).item() / len(val_idx)\n",
    "        print(\"Epoch {:05d} | Train Acc: {:.4f} | Train Loss: {:.4f} | Valid Acc: {:.4f} | Valid loss: {:.4f}\".\n",
    "              format(e, train_acc, loss.item(), val_acc, val_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.8333 | Test loss: 0.7807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------- 5. check results ------------------------ #\n",
    "    model.eval()\n",
    "    embed.eval()\n",
    "    embeds = embed()\n",
    "    logits= model.forward(g,embeds)[category]\n",
    "    test_loss = F.cross_entropy(logits[test_idx], labels[test_idx])\n",
    "    test_acc = torch.sum(logits[test_idx].argmax(dim=1) == labels[test_idx]).item() / len(test_idx)\n",
    "    print(\"Test Acc: {:.4f} | Test loss: {:.4f}\".format(test_acc, test_loss.item()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}