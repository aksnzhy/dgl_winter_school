{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Semi-supervised node classification using Heterogenous Graph Neural Networks\n",
    "\n",
    "In this tutorial, you will learn:\n",
    "\n",
    "* Build a relational graph neural network model proposed by [Schlichtkrull et al.](https://arxiv.org/abs/1703.06103)\n",
    "* Train the model and understand the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem formulation\n",
    "\n",
    "- Given the graph structure, node features, and node labels on a subset of nodes of a certain type\n",
    "- Predict the labels on the rest of the nodes of the labeled type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "# We first load the graph and node labels as is covered in the last session.\n",
    "\n",
    "from dgl.data.rdf import AIFBDataset\n",
    "\n",
    "dataset = AIFBDataset()\n",
    "g = dataset[0]\n",
    "\n",
    "category = dataset.predict_category\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "# obtain the training testing splits stored as graph node attributes\n",
    "train_mask = g.nodes[category].data.pop('train_mask')\n",
    "test_mask = g.nodes[category].data.pop('test_mask')\n",
    "train_idx = torch.nonzero(train_mask, as_tuple=False).squeeze()\n",
    "test_idx = torch.nonzero(test_mask, as_tuple=False).squeeze()\n",
    "labels = g.nodes[category].data.pop('labels')\n",
    "\n",
    "# split dataset into train, validate, test\n",
    "val_idx = train_idx[:len(train_idx) // 5]\n",
    "train_idx = train_idx[len(train_idx) // 5:]\n",
    "\n",
    "# check cuda\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "g = g.to(device)\n",
    "labels = labels.to(device)\n",
    "train_idx = train_idx.to(device)\n",
    "test_idx = test_idx.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Heterogenous models\n",
    "\n",
    "- Heterogenous graphs have multiple edge types\n",
    "- Messages arrive to nodes from different edge type\n",
    "- One class of models can be defined by selecting how to aggregate messages per edge-type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Relational GCN model\n",
    "\n",
    "- Relational GCN sums the messages per each relation type\n",
    "- Neighborhood per relation\n",
    "\n",
    "\n",
    "$$\n",
    " h_i^{(l+1)} = \\sigma \\big(\\sum_r \\sum_{j\\in\\mathcal{N}_{(i)}^r}\\frac{1}{c_{i,r}}h_j^{(l)}W_r^{(l)}\\big)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The HeteroRGCNLayer is used to implement the previous equation.\n",
    "- It takes in a dictionary of node types and node feature tensors as input, and returns another dictionary of node types and node features.\n",
    "- For a graph with R relations it uses\n",
    " - R message passing functions\n",
    " - R aggregation functions\n",
    " - A single function to aggregate the messages across relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class HeteroRGCNLayer(nn.Module):\n",
    "    def __init__(self, in_size, out_size, etypes):\n",
    "        super(HeteroRGCNLayer, self).__init__()\n",
    "        # W_r for each relation\n",
    "        self.weight = nn.ModuleDict({\n",
    "                name: nn.Linear(in_size, out_size) for name in etypes\n",
    "            })\n",
    "\n",
    "    def forward(self, G, feat_dict):\n",
    "        # The input is a dictionary of node features for each type\n",
    "        funcs = {}\n",
    "        for srctype, etype, dsttype in G.canonical_etypes:\n",
    "            # Compute W_r * h\n",
    "            if srctype in feat_dict:\n",
    "                Wh = self.weight[etype](feat_dict[srctype])\n",
    "                # Save it in graph for message passing\n",
    "                G.nodes[srctype].data['Wh_%s' % etype] = Wh\n",
    "                # Specify per-relation message passing functions: (message_func, reduce_func).\n",
    "                # Note that the results are saved to the same destination feature 'h', which\n",
    "                # hints the type wise reducer for aggregation.\n",
    "                funcs[etype] = (fn.copy_u('Wh_%s' % etype, 'm'), fn.mean('m', 'h'))\n",
    "        # Trigger message passing of multiple types.\n",
    "        # The first argument is the message passing functions for each relation.\n",
    "        # The second one is the type wise reducer, could be \"sum\", \"max\",\n",
    "        # \"min\", \"mean\", \"stack\"\n",
    "        G.multi_update_all(funcs, 'sum')\n",
    "        # return the updated node feature dictionary\n",
    "        return {ntype: G.dstnodes[ntype].data['h'] for ntype in G.ntypes if 'h' in G.dstnodes[ntype].data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define a HeteroGraphConv model\n",
    "\n",
    "- HeteroGraphConv is a encapsulation to run DGL NN module on heterogeneous graphs.\n",
    " - $f_r(\\cdot,\\cdot)$: A DGL NN module has to defined per relation ùëü, e.g., GraphConv()\n",
    " - A DGL NN module corresponds to a pair of message passing and aggregation functions\n",
    " - $G(\\cdot)$: A reduction function to merge the results on the same node type from multiple relations, e.g., $\\sum$.  \n",
    " - $g_r$: Graph per relation $r$\n",
    "$$\n",
    "h_{x}^{(l+1)} = \\underset{r\\in\\mathcal{R}, r_{dst}=x}\n",
    "{G} (f_r(g_r, h_{r_{src}}^l, h_{r_{dst}}^l))$$\n",
    "\n",
    "- RGCN implementation is abstracted by the HeteroGraphConv model. (Exercise: Map the function above to RGCN)\n",
    "\n",
    "See also the [link](https://docs.dgl.ai/guide/nn-heterograph.html?highlight=heterogenous%20graphs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# ----------- 2. create model -------------- #\n",
    "# build a two-layer RGCN model\n",
    "import dgl.nn as dglnn\n",
    "\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, rel_names):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(in_feats, hid_feats)\n",
    "            for rel in rel_names}, aggregate='sum')\n",
    "        self.conv2 = dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(hid_feats, out_feats)\n",
    "            for rel in rel_names}, aggregate='sum')\n",
    "\n",
    "    def forward(self, graph, inputs):\n",
    "        # inputs are features of nodes\n",
    "        h = self.conv1(graph, inputs)\n",
    "        h = {k: F.relu(v) for k, v in h.items()}\n",
    "        h = self.conv2(graph, h)\n",
    "        return h\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Flexibility of HeteroGraphConv\n",
    "\n",
    "- Performs a separate graph convolution on each edge type\n",
    "- Sums the message aggregations on each edge type as the final result for all node types.\n",
    "- By replacing the GraphConv with GraphAtt we get a different model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Node embedding layer for heterogenous graphs\n",
    "\n",
    "- Since AIFB does not have node feature we will use learnable embeddings.\n",
    "- In heterogenous graphs a dictionary of embeddings is used.\n",
    "- The embeddings will be updated on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class NodeEmbed(nn.Module):\n",
    "    def __init__(self, num_nodes, embed_size,decice):\n",
    "        super(NodeEmbed, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.node_embeds = nn.ModuleDict()\n",
    "        self.device=device\n",
    "        self.num_nodes=num_nodes\n",
    "        for ntype in num_nodes:\n",
    "            node_embed = torch.nn.Embedding(num_nodes[ntype], self.embed_size)\n",
    "            nn.init.uniform_(node_embed.weight, -1.0, 1.0)\n",
    "            self.node_embeds[str(ntype)] = node_embed\n",
    "    \n",
    "    def forward(self):\n",
    "        embeds = {}\n",
    "        num_nodes=self.num_nodes\n",
    "        for ntype in num_nodes:\n",
    "            embeds[ntype] = self.node_embeds[ntype](torch.tensor(list(range(num_nodes[ntype]))).to(self.device))\n",
    "        return embeds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "num_nodes = {ntype: g.number_of_nodes(ntype) for ntype in g.ntypes}\n",
    "\n",
    "h_hidden=16\n",
    "embed = NodeEmbed(num_nodes, h_hidden,device).to(device)\n",
    "model = RGCN(h_hidden, h_hidden, num_classes,g.etypes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Train Acc: 0.1161 | Train Loss: 4.6996 | Valid Acc: 0.1429 | Valid loss: 2.8667\n",
      "Epoch 00005 | Train Acc: 0.8929 | Train Loss: 0.4026 | Valid Acc: 0.5000 | Valid loss: 0.9255\n",
      "Epoch 00010 | Train Acc: 0.9375 | Train Loss: 0.2100 | Valid Acc: 0.6786 | Valid loss: 0.7023\n",
      "Epoch 00015 | Train Acc: 0.9643 | Train Loss: 0.1209 | Valid Acc: 0.7500 | Valid loss: 0.5429\n",
      "Epoch 00020 | Train Acc: 0.9643 | Train Loss: 0.0961 | Valid Acc: 0.7143 | Valid loss: 0.4996\n",
      "Epoch 00025 | Train Acc: 0.9821 | Train Loss: 0.0744 | Valid Acc: 0.7500 | Valid loss: 0.4819\n",
      "Epoch 00030 | Train Acc: 0.9821 | Train Loss: 0.0535 | Valid Acc: 0.7857 | Valid loss: 0.4839\n",
      "Epoch 00035 | Train Acc: 0.9821 | Train Loss: 0.0390 | Valid Acc: 0.7857 | Valid loss: 0.5103\n",
      "Epoch 00040 | Train Acc: 0.9911 | Train Loss: 0.0293 | Valid Acc: 0.7857 | Valid loss: 0.5434\n",
      "Epoch 00045 | Train Acc: 0.9911 | Train Loss: 0.0211 | Valid Acc: 0.7500 | Valid loss: 0.5718\n"
     ]
    }
   ],
   "source": [
    "# ----------- 3. set up optimizer -------------- #\n",
    "\n",
    "optimizer = torch.optim.Adam(itertools.chain(model.parameters(), embed.parameters()), lr=0.01)\n",
    "\n",
    "# ----------- 4. training -------------------------------- #\n",
    "all_logits = []\n",
    "for e in range(50):\n",
    "    # forward\n",
    "    embeds = embed()\n",
    "    logits= model(g,embeds)[category]\n",
    "    \n",
    "    # compute loss\n",
    "    loss = F.cross_entropy(logits[train_idx], labels[train_idx])\n",
    "    \n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    all_logits.append(logits.detach())\n",
    "    \n",
    "    if e % 5 == 0:\n",
    "        train_acc = torch.sum(logits[train_idx].argmax(dim=1) == labels[train_idx]).item() / len(train_idx)\n",
    "        val_loss = F.cross_entropy(logits[val_idx], labels[val_idx])\n",
    "        val_acc = torch.sum(logits[val_idx].argmax(dim=1) == labels[val_idx]).item() / len(val_idx)\n",
    "        print(\"Epoch {:05d} | Train Acc: {:.4f} | Train Loss: {:.4f} | Valid Acc: {:.4f} | Valid loss: {:.4f}\".\n",
    "              format(e, train_acc, loss.item(), val_acc, val_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.8889 | Test loss: 0.4004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------- 5. check results ------------------------ #\n",
    "    model.eval()\n",
    "    embed.eval()\n",
    "    embeds = embed()\n",
    "    logits= model.forward(g,embeds)[category]\n",
    "    test_loss = F.cross_entropy(logits[test_idx], labels[test_idx])\n",
    "    test_acc = torch.sum(logits[test_idx].argmax(dim=1) == labels[test_idx]).item() / len(test_idx)\n",
    "    print(\"Test Acc: {:.4f} | Test loss: {:.4f}\".format(test_acc, test_loss.item()))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
