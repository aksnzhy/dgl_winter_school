{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stochastic Training of GNN for Node Classification on Large Graphs\n",
    "\n",
    "*Note: this tutorial requires a GPU enabled machine*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This tutorial shows how to train a GraphSAGE model for node classification on the Amazon Co-purchase Network provided by OGB.\n",
    "\n",
    "The dataset contains 2.4 million nodes and 61 million edges, hence the full graph will not fit in a single GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "By the end of this tutorial you will learn how to \n",
    "\n",
    "* Create a DGL graph from your own data in other formats such as CSV.\n",
    "* Train a GNN model with a single machine, a single GPU, on a graph of any size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load Dataset\n",
    "\n",
    "Although you can directly use the Python package provided by OGB, for demonstration, we will instead manually download the dataset, peek into its contents, and process it with only `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-01-14 01:09:50--  https://snap.stanford.edu/ogb/data/nodeproppred/products.zip\n",
      "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
      "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1480993786 (1.4G) [application/zip]\n",
      "Saving to: ‘products.zip.1’\n",
      "\n",
      "products.zip.1      100%[===================>]   1.38G  15.2MB/s    in 2m 37s  \n",
      "\n",
      "2021-01-14 01:12:28 (8.98 MB/s) - ‘products.zip.1’ saved [1480993786/1480993786]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://snap.stanford.edu/ogb/data/nodeproppred/products.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  products.zip\n",
      "  inflating: products/split/sales_ranking/test.csv.gz  \n",
      "  inflating: products/split/sales_ranking/train.csv.gz  \n",
      "  inflating: products/split/sales_ranking/valid.csv.gz  \n",
      "  inflating: products/raw/node-label.csv.gz  \n",
      " extracting: products/raw/num-node-list.csv.gz  \n",
      " extracting: products/raw/num-edge-list.csv.gz  \n",
      "  inflating: products/raw/node-feat.csv.gz  \n",
      "  inflating: products/raw/edge.csv.gz  \n",
      "  inflating: products/mapping/README.md  \n",
      " extracting: products/mapping/labelidx2productcategory.csv.gz  \n",
      "  inflating: products/mapping/nodeidx2asin.csv.gz  \n",
      "  inflating: products/RELEASE_v1.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip -o products.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The dataset contains these files:\n",
    "\n",
    "* `products/raw/edge.csv` (source-destination pairs)\n",
    "* `products/raw/node-feat.csv` (node features)\n",
    "* `products/raw/node-label.csv` (node labels)\n",
    "* `products/raw/num-edge-list.csv` (number of edges)\n",
    "* `products/raw/num-node-list.csv` (number of nodes)\n",
    "\n",
    "We will only use the first three CSV files.\n",
    "\n",
    "In addition, it also contains the following files defining the training-validation-test split in the directory `products/split/sales_ranking`.  All `train.csv`, `valid.csv` and `test.csv` are text files containing the node IDs in the training/validation/test set, one number per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "edges = pd.read_csv('products/raw/edge.csv.gz', header=None).values\n",
    "node_features = pd.read_csv('products/raw/node-feat.csv.gz', header=None).values\n",
    "node_labels = pd.read_csv('products/raw/node-label.csv.gz', header=None).values[:, 0]\n",
    "\n",
    "# pd.read_csv yields a DataFrame with one column, so we make them one-dimensional arrays.\n",
    "train_nids = pd.read_csv('products/split/sales_ranking/train.csv.gz', header=None).values[:, 0]\n",
    "valid_nids = pd.read_csv('products/split/sales_ranking/valid.csv.gz', header=None).values[:, 0]\n",
    "test_nids = pd.read_csv('products/split/sales_ranking/test.csv.gz', header=None).values[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loading Node IDs into DGL\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Note:</b> The node IDs should be consecutive integers from 0 to the number of nodes minus 1.  If your node ID is not consecutive or not starting from 0 (e.g., starting from 100000), you need to relabel them yourself.  The <code>astype</code> method in pandas DataFrame can conveniently relabel the IDs by converting the type to <code>\"category\"</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Construct DGL Graph\n",
    "We construct the graph as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "\n",
    "graph = dgl.graph((edges[:, 0], edges[:, 1]))\n",
    "node_features = torch.FloatTensor(node_features)\n",
    "node_labels = torch.LongTensor(node_labels)\n",
    "\n",
    "# Save the graph, features and training-validation-test split for use for future tutorials.\n",
    "import pickle\n",
    "with open('data.pkl', 'wb') as f:\n",
    "    pickle.dump((graph, node_features, node_labels, train_nids, valid_nids, test_nids), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Load the graph back from the file we saved\n",
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "with open('data.pkl', 'rb') as f:\n",
    "    graph, node_features, node_labels, train_nids, valid_nids, test_nids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can see the size of the graph, features, and labels as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph\n",
      "Graph(num_nodes=2449029, num_edges=61859140,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={})\n",
      "Shape of node features: torch.Size([2449029, 100])\n",
      "Shape of node labels: torch.Size([2449029])\n",
      "Number of classes: 47\n"
     ]
    }
   ],
   "source": [
    "print('Graph')\n",
    "print(graph)\n",
    "print('Shape of node features:', node_features.shape)\n",
    "print('Shape of node labels:', node_labels.shape)\n",
    "\n",
    "num_features = node_features.shape[1]\n",
    "num_classes = (node_labels.max() + 1).item()\n",
    "print('Number of classes:', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Define a Data Loader with Neighbor Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Message passing overview\n",
    "\n",
    "The formulation of message passing usually has the following form:\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "  \\boldsymbol{a}_v^{(l)} = \\rho^{(l)} \\left(\n",
    "    \\left\\lbrace\n",
    "      \\boldsymbol{h}_u^{(l-1)} : u \\in \\mathcal{N} \\left( v \\right)\n",
    "    \\right\\rbrace\n",
    "  \\right)\n",
    "\\\\\n",
    "  \\boldsymbol{h}_v^{(l)} = \\phi^{(l)} \\left(\n",
    "    \\boldsymbol{h}_v^{(l-1)}, \\boldsymbol{a}_v^{(l)}\n",
    "  \\right)\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "where $\\rho^{(l)}$ and $\\phi^{(l)}$ are parameterized functions, and $\\mathcal{N}(v)$ represents the set of predecessors (or equivalently *neighbors*) of $v$ on graph $\\mathcal{G}$:\n",
    "$$\n",
    "\\mathcal{N} \\left( v \\right) = \\left\\lbrace\n",
    "  s \\left( e \\right) : e \\in \\mathbb{E}, t \\left( e \\right) = v\n",
    "\\right\\rbrace\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "For instance, to perform a message passing for updating the red node in the following graph:\n",
    "\n",
    "![Imgur](assets/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You need to aggregate the node features of its neighbors, shown as green nodes:\n",
    "\n",
    "![Imgur](assets/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let's consider how multi-layer message passing works for computing the output of a single node.  In the following text, we refer to the nodes whose outputs GNN will compute as seed nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Multi-layer message passing \n",
    "\n",
    "Consider computing with a 2-layer GNN the output of the seed node 8, colored red, in the following graph:\n",
    "\n",
    "![Imgur](assets/seed.png)\n",
    "\n",
    "By the formulation:\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "  \\boldsymbol{a}_8^{(2)} = \\rho^{(2)} \\left(\n",
    "    \\left\\lbrace\n",
    "      \\boldsymbol{h}_u^{(1)} : u \\in \\mathcal{N} \\left( 8 \\right)\n",
    "    \\right\\rbrace\n",
    "  \\right) = \\rho^{(2)} \\left(\n",
    "    \\left\\lbrace\n",
    "      \\boldsymbol{h}_4^{(1)}, \\boldsymbol{h}_5^{(1)},\n",
    "      \\boldsymbol{h}_7^{(1)}, \\boldsymbol{h}_{11}^{(1)}\n",
    "    \\right\\rbrace\n",
    "  \\right)\n",
    "\\\\\n",
    "  \\boldsymbol{h}_8^{(2)} = \\phi^{(2)} \\left(\n",
    "    \\boldsymbol{h}_8^{(1)}, \\boldsymbol{a}_8^{(2)}\n",
    "  \\right)\n",
    "\\end{gathered}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can tell that, to compute $\\boldsymbol{h}_8^{(2)}$, we need messages from node 4, 5, 7, and 11 (colored green) along the edges visualized below.\n",
    "\n",
    "![Imgur](assets/3.png)\n",
    "\n",
    "The values of $\\boldsymbol{h}_\\cdot^{(1)}$ are the outputs from the first GNN layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To compute those values for the red and green nodes, we further need to perform message passing on the edges visualized below.\n",
    "\n",
    "![Imgur](assets/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Therefore, to compute the 2-layer GNN representation of the red node, we need the input features from the red node as well as the green and yellow nodes.  Note that we should take red node's neighbors again for this layer.\n",
    "\n",
    "You may notice that the procedure which determines computation dependency is in the reverse direction of message aggregation: you start from the layer closest to the output and work backward to the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "* Computing representation for a small number of nodes still often requires input features of a significantly larger number of nodes.  \n",
    "\n",
    "* Taking all neighbors for message aggregation is often too costly since the nodes needed would easily cover a large portion of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Neighbour sampling addresses this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neighbour Sampling Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Neighbor sampling addresses this issue by selecting a random subset of the neighbors to perform aggregation.\n",
    "\n",
    "For example, to compute $\\boldsymbol{h}_8^{(1)}$, we can choose to sample 2 neighbors and aggregate.\n",
    "\n",
    "![Imgur](assets/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Similarly, to compute the red and green nodes' first layer representation, we can also do neighbor sampling that takes 2 neighbors for each node.  Note that we should take the red node's neighbors again for this layer.\n",
    "\n",
    "![Imgur](assets/6.png)\n",
    "\n",
    "You can see that this method could give us fewer nodes needed for input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Other graph sampling strategies\n",
    "* Neighborhood sampling (GraphSAGE)\n",
    "* Control-variate-based sampling (VRGCN)\n",
    "* Layer-wise sampling (FastGCN, LADIES)\n",
    "* Random-walk-based sampling (PinSage)\n",
    "* Subgraph sampling (ClusterGCN, GraphSAINT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining neighbor sampler and node data loader in DGL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "DGL provides useful tools to generate such computation dependencies while iterating over the dataset in minibatches and performing neighbor sampling.\n",
    "\n",
    "For node classification, you can use\n",
    "* `dgl.dataloading.NodeDataLoader` for iterating over the dataset, and\n",
    "* `dgl.dataloading.MultiLayerNeighborSampler` to generate computation dependencies of the nodes with neighbor sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The syntax of `dgl.dataloading.NodeDataLoader` is mostly similar to a PyTorch `DataLoader`, with the addition that it needs a graph to generate computation dependency from, a set of node IDs to iterate on, and the neighbor sampler you defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's consider training a 3-layer GraphSAGE with neighbor sampling, and each node will gather message from 4 neighbors on each layer.  The code defining the data loader and neighbor sampler will look like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4, 4])\n",
    "train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, train_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can peek at the first item in the data loader we created and see what it gives us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([185967,  29396,  35032,  ...,  10059,  15183,    676]), tensor([185967,  29396,  35032,  ..., 134351,  60992,  40989]), [Block(num_src_nodes=35310, num_dst_nodes=16046, num_edges=52314), Block(num_src_nodes=16046, num_dst_nodes=4632, num_edges=16299), Block(num_src_nodes=4632, num_dst_nodes=1024, num_edges=3759)])\n"
     ]
    }
   ],
   "source": [
    "example_minibatch = next(iter(train_dataloader))\n",
    "print(example_minibatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`NodeDataLoader` gives us three items per iteration.\n",
    "\n",
    "* The input node list for the nodes whose input features are needed to compute the outputs.\n",
    "* The output node list whose GNN representation are to be computed.\n",
    "* The list of computation dependency for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To compute 1024 nodes' output we need 35310 nodes' input features\n"
     ]
    }
   ],
   "source": [
    "input_nodes, output_nodes, bipartites = example_minibatch\n",
    "print(\"To compute {} nodes' output we need {} nodes' input features\".format(len(output_nodes), len(input_nodes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The variable `bipartites` has the message passing computation dependency for each layer.\n",
    "\n",
    "It is named suggestively, because it can be thought of as a **list** of bipartite graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So why does DGL return a list of *bipartite* graphs for training a *homogeneous* graph? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To distinguish between the source nodes sending the messages and the destination nodes being updated at each layer.\n",
    "\n",
    "Recall the sampled sub-graph from the example above:\n",
    "\n",
    "![Imgur](assets/6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The first GNN layer outputs the representation of three nodes (two green nodes and one red node), but requires input from 7 nodes (the green nodes and red node, plus 4 yellow nodes).  \n",
    "\n",
    "A bipartite graph easily captures the computation dependency?:\n",
    "\n",
    "![](assets/bipartite.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at each *bipartite* graph in `bipartites`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block(num_src_nodes=35310, num_dst_nodes=16046, num_edges=52314)\n",
      "Block(num_src_nodes=16046, num_dst_nodes=4632, num_edges=16299)\n",
      "Block(num_src_nodes=4632, num_dst_nodes=1024, num_edges=3759)\n"
     ]
    }
   ],
   "source": [
    "for block in bipartites:\n",
    "    print(block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Minibatch training of GNNs usually involves message passing on such bipartite graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining Model\n",
    "\n",
    "We are training a GraphSage GNN model that was previously introduced.\n",
    "\n",
    "The model can be written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn as dglnn\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(dglnn.SAGEConv(in_feats, n_hidden, 'mean'))\n",
    "        for i in range(1, n_layers - 1):\n",
    "            self.layers.append(dglnn.SAGEConv(n_hidden, n_hidden, 'mean'))\n",
    "        self.layers.append(dglnn.SAGEConv(n_hidden, n_classes, 'mean'))\n",
    "        \n",
    "    def forward(self, bipartites, x):\n",
    "        for l, (layer, bipartite) in enumerate(zip(self.layers, bipartites)):\n",
    "            x = layer(bipartite, x)\n",
    "            if l != self.n_layers - 1:\n",
    "                x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can see that here we are iterating over the pairs of NN module layer and bipartite graphs generated by the data loader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining the Training Loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following initializes the model and defines the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = SAGE(num_features, 128, num_classes, 3).cuda()\n",
    "opt = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataloader for validation\n",
    "\n",
    "When computing the validation score for model selection, usually you can also do neighbor sampling.  To do that, you need to define another data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "valid_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, valid_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following is a training loop that performs validation every epoch.  It also saves the model with the best validation accuracy into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:07<00:00, 24.35it/s, loss=0.845, acc=0.571]\n",
      "100%|██████████| 39/39 [00:01<00:00, 24.39it/s]\n",
      "  2%|▏         | 3/193 [00:00<00:08, 22.91it/s, loss=0.780, acc=0.812]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.8259288457137044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:07<00:00, 24.97it/s, loss=0.693, acc=0.714]\n",
      "100%|██████████| 39/39 [00:01<00:00, 24.48it/s]\n",
      "  2%|▏         | 3/193 [00:00<00:08, 22.54it/s, loss=0.664, acc=0.827]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Accuracy 0.8476464155837551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:07<00:00, 25.05it/s, loss=1.057, acc=0.714]\n",
      "100%|██████████| 39/39 [00:01<00:00, 24.30it/s]\n",
      "  2%|▏         | 3/193 [00:00<00:08, 22.87it/s, loss=0.674, acc=0.823]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Accuracy 0.8549195127533504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:07<00:00, 25.06it/s, loss=0.176, acc=0.857]\n",
      "100%|██████████| 39/39 [00:01<00:00, 24.41it/s]\n",
      "  2%|▏         | 3/193 [00:00<00:08, 22.20it/s, loss=0.515, acc=0.862]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Accuracy 0.8643796251557613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:07<00:00, 24.96it/s, loss=0.414, acc=0.857]\n",
      "100%|██████████| 39/39 [00:01<00:00, 24.47it/s]\n",
      "  2%|▏         | 3/193 [00:00<00:08, 22.53it/s, loss=0.546, acc=0.848]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Accuracy 0.8671515398113063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:07<00:00, 24.76it/s, loss=0.533, acc=0.857]\n",
      "100%|██████████| 39/39 [00:01<00:00, 24.04it/s]\n",
      "  2%|▏         | 3/193 [00:00<00:08, 22.53it/s, loss=0.473, acc=0.886]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Accuracy 0.871296696589782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:07<00:00, 24.53it/s, loss=1.526, acc=0.571]\n",
      "100%|██████████| 39/39 [00:01<00:00, 24.46it/s]\n",
      "  2%|▏         | 3/193 [00:00<00:08, 22.32it/s, loss=0.579, acc=0.830]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation Accuracy 0.8660326017852148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:07<00:00, 24.74it/s, loss=0.020, acc=1.000]\n",
      "100%|██████████| 39/39 [00:01<00:00, 24.35it/s]\n",
      "  2%|▏         | 3/193 [00:00<00:08, 22.36it/s, loss=0.460, acc=0.869]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation Accuracy 0.8780866159753834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:07<00:00, 24.52it/s, loss=0.259, acc=1.000]\n",
      "100%|██████████| 39/39 [00:01<00:00, 24.26it/s]\n",
      "  2%|▏         | 3/193 [00:00<00:08, 22.70it/s, loss=0.456, acc=0.875]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Validation Accuracy 0.8790021107240038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:07<00:00, 24.43it/s, loss=0.047, acc=1.000]\n",
      "100%|██████████| 39/39 [00:01<00:00, 24.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Validation Accuracy 0.8800956183404115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import sklearn.metrics\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model_path = 'model.pt'\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    \n",
    "    with tqdm.tqdm(train_dataloader) as tq:\n",
    "        for step, (input_nodes, output_nodes, bipartites) in enumerate(tq):\n",
    "            bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "            inputs = node_features[input_nodes].cuda()\n",
    "            labels = node_labels[output_nodes].cuda()\n",
    "            predictions = model(bipartites, inputs)\n",
    "\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "            \n",
    "            tq.set_postfix({'loss': '%.03f' % loss.item(), 'acc': '%.03f' % accuracy}, refresh=False)\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with tqdm.tqdm(valid_dataloader) as tq, torch.no_grad():\n",
    "        for input_nodes, output_nodes, bipartites in tq:\n",
    "            bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "            inputs = node_features[input_nodes].cuda()\n",
    "            labels.append(node_labels[output_nodes].numpy())\n",
    "            predictions.append(model(bipartites, inputs).argmax(1).cpu().numpy())\n",
    "        predictions = np.concatenate(predictions)\n",
    "        labels = np.concatenate(labels)\n",
    "        accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "        print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
    "        if best_accuracy < accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Offline Inference without Neighbor Sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Usually for offline inference it is desirable to aggregate over the entire neighborhood to eliminate randomness introduced by neighbor sampling.  However, using the same methodology in training is not efficient, because there will be a lot of redundant computation.  Moreover, simply doing neighbor sampling by taking all neighbors will often exhaust GPU memory because the number of nodes required for input features may be too large to fit into GPU memory.\n",
    "\n",
    "Instead, you need to compute the representations layer by layer: you first compute the output of the first GNN layer for all nodes, then you compute the output of second GNN layer for all nodes using the first GNN layer's output as input, etc.  This gives us a different algorithm from what is being used in training.  During training we have an outer loop that iterates over the nodes, and an inner loop that iterates over the layers.  In contrast, during inference we have an outer loop that iterates over the layers, and an inner loop that iterates over the nodes.\n",
    "\n",
    "If you do not care about randomness too much (e.g., during model selection in validation), you can still use the `dgl.dataloading.MultiLayerNeighborSampler` and `dgl.dataloading.NodeDataLoader` to do offline inference, since it is usually faster for evaluating a small number of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Imgur](assets/anim.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def inference(model, graph, input_features, batch_size):\n",
    "    nodes = torch.arange(graph.number_of_nodes())\n",
    "    \n",
    "    sampler = dgl.dataloading.MultiLayerNeighborSampler([None])  # one layer at a time, taking all neighbors\n",
    "    dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        graph, nodes, sampler,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for l, layer in enumerate(model.layers):\n",
    "            # Allocate a buffer of output representations for every node\n",
    "            # Note that the buffer is on CPU memory.\n",
    "            output_features = torch.zeros(\n",
    "                graph.number_of_nodes(), model.n_hidden if l != model.n_layers - 1 else model.n_classes)\n",
    "\n",
    "            for input_nodes, output_nodes, bipartites in tqdm.tqdm(dataloader):\n",
    "                bipartite = bipartites[0].to(torch.device('cuda'))\n",
    "\n",
    "                x = input_features[input_nodes].cuda()\n",
    "\n",
    "                # the following code is identical to the loop body in model.forward()\n",
    "                x = layer(bipartite, x)\n",
    "                if l != model.n_layers - 1:\n",
    "                    x = F.relu(x)\n",
    "\n",
    "                output_features[output_nodes] = x.cpu()\n",
    "            input_features = output_features\n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following code loads the best model from the file saved previously and performs offline inference.  It computes the accuracy on the test set afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 299/299 [00:34<00:00,  8.56it/s]\n",
      "100%|██████████| 299/299 [00:26<00:00, 11.19it/s]\n",
      "100%|██████████| 299/299 [00:26<00:00, 11.29it/s]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(best_model_path))\n",
    "all_predictions = inference(model, graph, node_features, 8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7298741895385232\n"
     ]
    }
   ],
   "source": [
    "test_predictions = all_predictions[test_nids].argmax(1)\n",
    "test_labels = node_labels[test_nids]\n",
    "test_accuracy = sklearn.metrics.accuracy_score(test_predictions.numpy(), test_labels.numpy())\n",
    "print('Test accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you have learned how to train a multi-layer GraphSAGE with neighbor sampling on a large dataset that cannot fit into GPU.  The method you have learned can scale to a graph of any size, and works on a single machine with a single GPU.\n",
    "\n",
    "## What's next?\n",
    "\n",
    "The next tutorial will be about training the same GraphSAGE model in an unsupervised manner with link prediction, i.e. predicting whether an edge exist between two nodes."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
