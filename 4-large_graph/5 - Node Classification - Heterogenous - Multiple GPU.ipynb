{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stochastic Training of GNN for Node Classification on Large Heterogeneous  Graphs using Multiple GPUs\n",
    "\n",
    "*Note: this tutorial requires a GPU enabled machine with multiple gpu devices*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This tutorial shows how to train a multi-layer R-GCN for node classification on the `ogbn-mag` dataset provided by OGB using multiple GPUs on a single machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "At the end of this tutorial you will be able to\n",
    "\n",
    "* Use multiple GPUs on a single machine to train a GNN model for a large heterogeneous graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dgl\n",
    "import torch\n",
    "import dgl.nn as dglnn\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import sklearn.metrics\n",
    "import tqdm\n",
    "\n",
    "from utils import thread_wrapped_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Dataset\n",
    "\n",
    "We are re-using the same dataset from the single gpu heterogeneous mini-batch tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "dataset = DglNodePropPredDataset(name='ogbn-mag')\n",
    "\n",
    "graph, label = dataset[0] # graph: dgl graph object, label: torch tensor of shape (num_nodes, 1)\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_nids, valid_nids, test_nids = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes={'author': 1134649, 'field_of_study': 59965, 'institution': 8740, 'paper': 736389},\n",
      "      num_edges={('author', 'affiliated_with', 'institution'): 1043998, ('author', 'writes', 'paper'): 7145660, ('paper', 'cites', 'paper'): 5416271, ('paper', 'has_topic', 'field_of_study'): 7505078},\n",
      "      metagraph=[('author', 'institution', 'affiliated_with'), ('author', 'paper', 'writes'), ('paper', 'paper', 'cites'), ('paper', 'field_of_study', 'has_topic')])\n",
      "Node labels\n",
      "Shape of target node labels: torch.Size([736389])\n",
      "Number of classes: 349\n",
      "Node features\n",
      "Shape of features of paper node type: 128\n"
     ]
    }
   ],
   "source": [
    "print(graph)\n",
    "\n",
    "print('Node labels')\n",
    "node_labels = label['paper'].flatten()\n",
    "print('Shape of target node labels:', node_labels.shape)\n",
    "num_classes = (node_labels.max() + 1).item()\n",
    "print('Number of classes:', num_classes)\n",
    "\n",
    "print('Node features')\n",
    "node_features = graph.nodes['paper'].data['feat']\n",
    "num_features = node_features.shape[1]\n",
    "print('Shape of features of paper node type: {}'.format(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "src_writes, dst_writes = graph.all_edges(etype=\"writes\")\n",
    "src_topic, dst_topic = graph.all_edges(etype=\"has_topic\")\n",
    "src_aff, dst_aff = graph.all_edges(etype=\"affiliated_with\")\n",
    "\n",
    "\n",
    "graph = dgl.heterograph({\n",
    "    (\"author\", \"writes\", \"paper\"): (src_writes, dst_writes),\n",
    "    (\"paper\", \"has_topic\", \"field_of_study\"): (src_topic, dst_topic),\n",
    "    (\"author\", \"affiliated_with\", \"institution\"): (src_aff, dst_aff),\n",
    "    (\"paper\", \"writes-rev\", \"author\"): (dst_writes, src_writes),\n",
    "    (\"field_of_study\", \"has_topic-rev\", \"paper\"): (dst_topic, src_topic),\n",
    "    (\"institution\", \"affiliated_with-rev\", \"author\"): (dst_aff, src_aff),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining data loader for DDP\n",
    "\n",
    "We need to partition the dataset so each gpu has it's part.\n",
    "\n",
    "nids is a dictionary of node types to node ids because the graph is heterogeneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataloader(rank, world_size, graph, nids, fanout):\n",
    "    part_nids = {}\n",
    "    for ntype, ids in nids.items():\n",
    "        partition_size = len(ids) // world_size\n",
    "        partition_offset = partition_size * rank\n",
    "        ids = ids[partition_offset:partition_offset+partition_size]\n",
    "        part_nids[ntype] = ids\n",
    "    \n",
    "    sampler = dgl.dataloading.MultiLayerNeighborSampler(fanout)\n",
    "    dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        graph, part_nids, sampler,\n",
    "        batch_size=1024,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Defining Model\n",
    "\n",
    "We reuse the same RGCN model from the previous tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn as dglnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class RGCN(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes, n_layers, rel_names):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(in_feats, n_hidden)\n",
    "            for rel in rel_names}, aggregate='sum'))\n",
    "        \n",
    "        for i in range(1, n_layers - 1):\n",
    "            self.layers.append(dglnn.HeteroGraphConv({\n",
    "                rel: dglnn.GraphConv(n_hidden, n_hidden)\n",
    "                for rel in rel_names}, aggregate='sum'))\n",
    "            \n",
    "        self.layers.append(dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(n_hidden, n_classes)\n",
    "            for rel in rel_names}, aggregate='sum'))\n",
    "\n",
    "    def forward(self, bipartites, x):\n",
    "        # inputs are features of nodes\n",
    "        for l, (layer, bipartite) in enumerate(zip(self.layers, bipartites)):\n",
    "            x = layer(bipartite, x)\n",
    "            if l != self.n_layers - 1:\n",
    "                x = {k: F.relu(v) for k, v in x.items()}\n",
    "        return x\n",
    "    \n",
    "\n",
    "class NodeEmbed(nn.Module):\n",
    "    def __init__(self, num_nodes, embed_size, rank=0):\n",
    "        super(NodeEmbed, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.embed_size = embed_size\n",
    "        self.node_embeds = nn.ModuleDict()\n",
    "        for ntype in num_nodes:\n",
    "            node_embed = torch.nn.Embedding(num_nodes[ntype], self.embed_size)\n",
    "            nn.init.uniform_(node_embed.weight, -1.0, 1.0)\n",
    "            self.node_embeds[ntype] = node_embed\n",
    "    \n",
    "    def forward(self, node_ids):\n",
    "        embeds = {}\n",
    "        for ntype in node_ids:\n",
    "            embeds[ntype] = self.node_embeds[ntype](node_ids[ntype]).to(self.rank)\n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wrapping model with `DistributedDataParallel`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_model(rank, in_feats, n_hidden, n_classes, n_layers, rel_names):\n",
    "    model = RGCN(in_feats, n_hidden, n_classes, n_layers, rel_names).to(rank)\n",
    "    return DistributedDataParallel(model, device_ids=[rank], output_device=rank, find_unused_parameters=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining Training Loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@thread_wrapped_func\n",
    "def train(rank, world_size, data):\n",
    "    # data is the output of load_data\n",
    "    torch.distributed.init_process_group(\n",
    "        backend='gloo',\n",
    "        init_method='tcp://127.0.0.1:12345',\n",
    "        world_size=world_size,\n",
    "        rank=rank)\n",
    "    torch.cuda.set_device(rank)\n",
    "    \n",
    "    graph, node_features, node_labels, train_nids, valid_nids, test_nids, num_features, num_classes = data\n",
    "    fanout = [15, 15]\n",
    "    train_dataloader = create_dataloader(rank, world_size, graph, train_nids, fanout)\n",
    "    # We only use one worker for validation\n",
    "    valid_dataloader = create_dataloader(0, 1, graph, valid_nids, fanout)\n",
    "    \n",
    "    num_nodes = {ntype: graph.number_of_nodes(ntype) for ntype in graph.ntypes if ntype != 'paper'}\n",
    "    num_layers = 2\n",
    "    hidden_dim = 128\n",
    "    embed = NodeEmbed(num_nodes, hidden_dim, rank)\n",
    "    \n",
    "    model = init_model(rank, num_features, hidden_dim, num_classes, num_layers, graph.etypes)\n",
    "    opt = torch.optim.Adam(list(model.parameters()) + list(embed.parameters()))\n",
    "    torch.distributed.barrier()\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    best_model_path = 'model.pt'\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "\n",
    "        for step, (input_nodes, output_nodes, bipartites) in enumerate(train_dataloader):\n",
    "            bipartites = [b.to(rank) for b in bipartites]\n",
    "            \n",
    "            featureless_nodes = {ntype: node_ids for ntype, node_ids in input_nodes.items() if ntype != 'paper'}\n",
    "            embeddings = embed(featureless_nodes)\n",
    "            \n",
    "            inputs = {'paper': node_features[input_nodes['paper']].to(rank)}\n",
    "            inputs.update(embeddings)\n",
    "            \n",
    "            labels = node_labels[output_nodes['paper']].to(rank)\n",
    "            predictions = model(bipartites, inputs)['paper']\n",
    "            \n",
    "\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "\n",
    "            if rank == 0 and step % 10 == 0:\n",
    "                print('Epoch {:05d} Step {:05d} Loss {:.04f}'.format(epoch, step, loss.item()))\n",
    "\n",
    "        torch.distributed.barrier()\n",
    "        \n",
    "        if rank == 0:\n",
    "            model.eval()\n",
    "            predictions = []\n",
    "            labels = []\n",
    "            with torch.no_grad():\n",
    "                for input_nodes, output_nodes, bipartites in valid_dataloader:\n",
    "                    bipartites = [b.to(rank) for b in bipartites]\n",
    "                    \n",
    "                    featureless_nodes = {ntype: node_ids for ntype, node_ids in input_nodes.items() if ntype != \"paper\"}\n",
    "                    embeddings = {ntype: node_embedding.cuda() for ntype, node_embedding in embed(featureless_nodes).items()}\n",
    "                    inputs = {'paper': node_features[input_nodes['paper']].cuda()}\n",
    "                    inputs.update(embeddings)\n",
    "            \n",
    "                    labels.append(node_labels[output_nodes['paper']].numpy())\n",
    "                    predictions.append(model(bipartites, inputs)['paper'].argmax(1).cpu().numpy())\n",
    "                predictions = np.concatenate(predictions)\n",
    "                labels = np.concatenate(labels)\n",
    "                accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "                print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
    "                if best_accuracy < accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    torch.save(model.module.state_dict(), best_model_path)\n",
    "                    \n",
    "        torch.distributed.barrier()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Spawning multiple processes for the Multi GPU training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "if __name__ == '__main__':\n",
    "    procs = []\n",
    "    data = graph, node_features, node_labels, train_nids, valid_nids, test_nids, num_features, num_classes\n",
    "    for proc_id in range(4):    # 4 gpus\n",
    "        p = mp.Process(target=train, args=(proc_id, 4, data))\n",
    "        p.start()\n",
    "        procs.append(p)\n",
    "    for p in procs:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def init_model(rank, in_feats, n_hidden, n_classes, n_layers, rel_names):\n",
    "    model = RGCN(in_feats, n_hidden, n_classes, n_layers, rel_names).to(rank)\n",
    "    return DistributedDataParallel(model, device_ids=[rank], output_device=rank, find_unused_parameters=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@thread_wrapped_func\n",
    "def train(rank, world_size, data):\n",
    "    # data is the output of load_data\n",
    "    torch.distributed.init_process_group(\n",
    "        backend='gloo',\n",
    "        init_method='tcp://127.0.0.1:12345',\n",
    "        world_size=world_size,\n",
    "        rank=rank)\n",
    "    torch.cuda.set_device(rank)\n",
    "    \n",
    "    graph, node_features, node_labels, train_nids, valid_nids, test_nids, num_features, num_classes = data\n",
    "    fanout = [15, 15]\n",
    "    train_dataloader = create_dataloader(rank, world_size, graph, train_nids, fanout)\n",
    "    # We only use one worker for validation\n",
    "    valid_dataloader = create_dataloader(0, 1, graph, valid_nids, fanout)\n",
    "    \n",
    "    num_nodes = {ntype: graph.number_of_nodes(ntype) for ntype in graph.ntypes if ntype != 'paper'}\n",
    "    num_layers = 2\n",
    "    hidden_dim = 128\n",
    "    embed = NodeEmbed(num_nodes, hidden_dim, rank)\n",
    "    \n",
    "    model = init_model(rank, num_features, hidden_dim, num_classes, num_layers, graph.etypes)\n",
    "    opt = torch.optim.Adam(list(model.parameters()) + list(embed.parameters()))\n",
    "    torch.distributed.barrier()\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    best_model_path = 'model.pt'\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "\n",
    "        for step, (input_nodes, output_nodes, bipartites) in enumerate(train_dataloader):\n",
    "            bipartites = [b.to(rank) for b in bipartites]\n",
    "            \n",
    "            featureless_nodes = {ntype: node_ids for ntype, node_ids in input_nodes.items() if ntype != 'paper'}\n",
    "            embeddings = embed(featureless_nodes)\n",
    "            \n",
    "            inputs = {'paper': node_features[input_nodes['paper']].to(rank)}\n",
    "            inputs.update(embeddings)\n",
    "            \n",
    "            labels = node_labels[output_nodes['paper']].to(rank)\n",
    "            predictions = model(bipartites, inputs)['paper']\n",
    "            \n",
    "\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "\n",
    "            if rank == 0 and step % 10 == 0:\n",
    "                print('Epoch {:05d} Step {:05d} Loss {:.04f}'.format(epoch, step, loss.item()))\n",
    "\n",
    "        torch.distributed.barrier()\n",
    "        \n",
    "        if rank == 0:\n",
    "            model.eval()\n",
    "            predictions = []\n",
    "            labels = []\n",
    "            with torch.no_grad():\n",
    "                for input_nodes, output_nodes, bipartites in valid_dataloader:\n",
    "                    bipartites = [b.to(rank) for b in bipartites]\n",
    "                    \n",
    "                    featureless_nodes = {ntype: node_ids for ntype, node_ids in input_nodes.items() if ntype != \"paper\"}\n",
    "                    embeddings = {ntype: node_embedding.cuda() for ntype, node_embedding in embed(featureless_nodes).items()}\n",
    "                    inputs = {'paper': node_features[input_nodes['paper']].cuda()}\n",
    "                    inputs.update(embeddings)\n",
    "            \n",
    "                    labels.append(node_labels[output_nodes['paper']].numpy())\n",
    "                    predictions.append(model(bipartites, inputs)['paper'].argmax(1).cpu().numpy())\n",
    "                predictions = np.concatenate(predictions)\n",
    "                labels = np.concatenate(labels)\n",
    "                accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "                print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
    "                if best_accuracy < accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    torch.save(model.module.state_dict(), best_model_path)\n",
    "                    \n",
    "        torch.distributed.barrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spawning multiple processes for the Multi GPU training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 Step 00000 Loss 6.2387\n",
      "Epoch 00000 Step 00010 Loss 4.9378\n",
      "Epoch 00000 Step 00020 Loss 4.6241\n",
      "Epoch 00000 Step 00030 Loss 4.3468\n",
      "Epoch 00000 Step 00040 Loss 4.1040\n",
      "Epoch 00000 Step 00050 Loss 3.9855\n",
      "Epoch 00000 Step 00060 Loss 3.7055\n",
      "Epoch 00000 Step 00070 Loss 3.5657\n",
      "Epoch 00000 Step 00080 Loss 3.4811\n",
      "Epoch 00000 Step 00090 Loss 3.3586\n",
      "Epoch 00000 Step 00100 Loss 3.2200\n",
      "Epoch 00000 Step 00110 Loss 3.2090\n",
      "Epoch 00000 Step 00120 Loss 3.1430\n",
      "Epoch 00000 Step 00130 Loss 3.1262\n",
      "Epoch 00000 Step 00140 Loss 3.0292\n",
      "Epoch 00000 Step 00150 Loss 3.0444\n",
      "Epoch 0 Validation Accuracy 0.2701336333790595\n",
      "Epoch 00001 Step 00000 Loss 3.0211\n",
      "Epoch 00001 Step 00010 Loss 2.9341\n",
      "Epoch 00001 Step 00020 Loss 2.9294\n",
      "Epoch 00001 Step 00030 Loss 3.0074\n",
      "Epoch 00001 Step 00040 Loss 2.8715\n",
      "Epoch 00001 Step 00050 Loss 2.8772\n",
      "Epoch 00001 Step 00060 Loss 2.8220\n",
      "Epoch 00001 Step 00070 Loss 2.8864\n",
      "Epoch 00001 Step 00080 Loss 2.7738\n",
      "Epoch 00001 Step 00090 Loss 2.7418\n",
      "Epoch 00001 Step 00100 Loss 2.6536\n",
      "Epoch 00001 Step 00110 Loss 2.8516\n",
      "Epoch 00001 Step 00120 Loss 2.6878\n",
      "Epoch 00001 Step 00130 Loss 2.8043\n",
      "Epoch 00001 Step 00140 Loss 2.6815\n",
      "Epoch 00001 Step 00150 Loss 2.7333\n",
      "Epoch 1 Validation Accuracy 0.3087902094668537\n",
      "Epoch 00002 Step 00000 Loss 2.5762\n",
      "Epoch 00002 Step 00010 Loss 2.6383\n",
      "Epoch 00002 Step 00020 Loss 2.7193\n",
      "Epoch 00002 Step 00030 Loss 2.6076\n",
      "Epoch 00002 Step 00040 Loss 2.6803\n",
      "Epoch 00002 Step 00050 Loss 2.7118\n",
      "Epoch 00002 Step 00060 Loss 2.5854\n",
      "Epoch 00002 Step 00070 Loss 2.6365\n",
      "Epoch 00002 Step 00080 Loss 2.7188\n",
      "Epoch 00002 Step 00090 Loss 2.5452\n",
      "Epoch 00002 Step 00100 Loss 2.5939\n",
      "Epoch 00002 Step 00110 Loss 2.5740\n",
      "Epoch 00002 Step 00120 Loss 2.6382\n",
      "Epoch 00002 Step 00130 Loss 2.5162\n",
      "Epoch 00002 Step 00140 Loss 2.5875\n",
      "Epoch 00002 Step 00150 Loss 2.6048\n",
      "Epoch 2 Validation Accuracy 0.32377194469705145\n",
      "Epoch 00003 Step 00000 Loss 2.3995\n",
      "Epoch 00003 Step 00010 Loss 2.5648\n",
      "Epoch 00003 Step 00020 Loss 2.5481\n",
      "Epoch 00003 Step 00030 Loss 2.6018\n",
      "Epoch 00003 Step 00040 Loss 2.4949\n",
      "Epoch 00003 Step 00050 Loss 2.5706\n",
      "Epoch 00003 Step 00060 Loss 2.4977\n",
      "Epoch 00003 Step 00070 Loss 2.4446\n",
      "Epoch 00003 Step 00080 Loss 2.5010\n",
      "Epoch 00003 Step 00090 Loss 2.5736\n",
      "Epoch 00003 Step 00100 Loss 2.5411\n",
      "Epoch 00003 Step 00110 Loss 2.4802\n",
      "Epoch 00003 Step 00120 Loss 2.4350\n",
      "Epoch 00003 Step 00130 Loss 2.4306\n",
      "Epoch 00003 Step 00140 Loss 2.5536\n",
      "Epoch 00003 Step 00150 Loss 2.4764\n",
      "Epoch 3 Validation Accuracy 0.31860848656730223\n",
      "Epoch 00004 Step 00000 Loss 2.4921\n",
      "Epoch 00004 Step 00010 Loss 2.3965\n",
      "Epoch 00004 Step 00020 Loss 2.5237\n",
      "Epoch 00004 Step 00030 Loss 2.4766\n",
      "Epoch 00004 Step 00040 Loss 2.4709\n",
      "Epoch 00004 Step 00050 Loss 2.4400\n",
      "Epoch 00004 Step 00060 Loss 2.3990\n",
      "Epoch 00004 Step 00070 Loss 2.4395\n",
      "Epoch 00004 Step 00080 Loss 2.4029\n",
      "Epoch 00004 Step 00090 Loss 2.5072\n",
      "Epoch 00004 Step 00100 Loss 2.4681\n",
      "Epoch 00004 Step 00110 Loss 2.3089\n",
      "Epoch 00004 Step 00120 Loss 2.3930\n",
      "Epoch 00004 Step 00130 Loss 2.4229\n",
      "Epoch 00004 Step 00140 Loss 2.4267\n",
      "Epoch 00004 Step 00150 Loss 2.3180\n",
      "Epoch 4 Validation Accuracy 0.32776399143020085\n",
      "Epoch 00005 Step 00000 Loss 2.3489\n",
      "Epoch 00005 Step 00010 Loss 2.3658\n",
      "Epoch 00005 Step 00020 Loss 2.4365\n",
      "Epoch 00005 Step 00030 Loss 2.3611\n",
      "Epoch 00005 Step 00040 Loss 2.3317\n",
      "Epoch 00005 Step 00050 Loss 2.3485\n",
      "Epoch 00005 Step 00060 Loss 2.3961\n",
      "Epoch 00005 Step 00070 Loss 2.4599\n",
      "Epoch 00005 Step 00080 Loss 2.4152\n",
      "Epoch 00005 Step 00090 Loss 2.4236\n",
      "Epoch 00005 Step 00100 Loss 2.3650\n",
      "Epoch 00005 Step 00110 Loss 2.3710\n",
      "Epoch 00005 Step 00120 Loss 2.3995\n",
      "Epoch 00005 Step 00130 Loss 2.3602\n",
      "Epoch 00005 Step 00140 Loss 2.4430\n",
      "Epoch 00005 Step 00150 Loss 2.3463\n",
      "Epoch 5 Validation Accuracy 0.3259452211039011\n",
      "Epoch 00006 Step 00000 Loss 2.3114\n",
      "Epoch 00006 Step 00010 Loss 2.3029\n",
      "Epoch 00006 Step 00020 Loss 2.3822\n",
      "Epoch 00006 Step 00030 Loss 2.2977\n",
      "Epoch 00006 Step 00040 Loss 2.3990\n",
      "Epoch 00006 Step 00050 Loss 2.2457\n",
      "Epoch 00006 Step 00060 Loss 2.3645\n",
      "Epoch 00006 Step 00070 Loss 2.3335\n",
      "Epoch 00006 Step 00080 Loss 2.3723\n",
      "Epoch 00006 Step 00090 Loss 2.2911\n",
      "Epoch 00006 Step 00100 Loss 2.3973\n",
      "Epoch 00006 Step 00110 Loss 2.3766\n",
      "Epoch 00006 Step 00120 Loss 2.3206\n",
      "Epoch 00006 Step 00130 Loss 2.3917\n",
      "Epoch 00006 Step 00140 Loss 2.3489\n",
      "Epoch 00006 Step 00150 Loss 2.2976\n",
      "Epoch 6 Validation Accuracy 0.3214599485195518\n",
      "Epoch 00007 Step 00000 Loss 2.3536\n",
      "Epoch 00007 Step 00010 Loss 2.2813\n",
      "Epoch 00007 Step 00020 Loss 2.2486\n",
      "Epoch 00007 Step 00030 Loss 2.3851\n",
      "Epoch 00007 Step 00040 Loss 2.2326\n",
      "Epoch 00007 Step 00050 Loss 2.3365\n",
      "Epoch 00007 Step 00060 Loss 2.2702\n",
      "Epoch 00007 Step 00070 Loss 2.1998\n",
      "Epoch 00007 Step 00080 Loss 2.3676\n",
      "Epoch 00007 Step 00090 Loss 2.3138\n",
      "Epoch 00007 Step 00100 Loss 2.3326\n",
      "Epoch 00007 Step 00110 Loss 2.3423\n",
      "Epoch 00007 Step 00120 Loss 2.2814\n",
      "Epoch 00007 Step 00130 Loss 2.2820\n",
      "Epoch 00007 Step 00140 Loss 2.2755\n",
      "Epoch 00007 Step 00150 Loss 2.2434\n",
      "Epoch 7 Validation Accuracy 0.3310624393101003\n",
      "Epoch 00008 Step 00000 Loss 2.1793\n",
      "Epoch 00008 Step 00010 Loss 2.2615\n",
      "Epoch 00008 Step 00020 Loss 2.3071\n",
      "Epoch 00008 Step 00030 Loss 2.3041\n",
      "Epoch 00008 Step 00040 Loss 2.1948\n",
      "Epoch 00008 Step 00050 Loss 2.3508\n",
      "Epoch 00008 Step 00060 Loss 2.1368\n",
      "Epoch 00008 Step 00070 Loss 2.2728\n",
      "Epoch 00008 Step 00080 Loss 2.2583\n",
      "Epoch 00008 Step 00090 Loss 2.3527\n",
      "Epoch 00008 Step 00100 Loss 2.3271\n",
      "Epoch 00008 Step 00110 Loss 2.3711\n",
      "Epoch 00008 Step 00120 Loss 2.2575\n",
      "Epoch 00008 Step 00130 Loss 2.2106\n",
      "Epoch 00008 Step 00140 Loss 2.2590\n",
      "Epoch 00008 Step 00150 Loss 2.3061\n",
      "Epoch 8 Validation Accuracy 0.3321105442439002\n",
      "Epoch 00009 Step 00000 Loss 2.2640\n",
      "Epoch 00009 Step 00010 Loss 2.1001\n",
      "Epoch 00009 Step 00020 Loss 2.3167\n",
      "Epoch 00009 Step 00030 Loss 2.1211\n",
      "Epoch 00009 Step 00040 Loss 2.2255\n",
      "Epoch 00009 Step 00050 Loss 2.2286\n",
      "Epoch 00009 Step 00060 Loss 2.3465\n",
      "Epoch 00009 Step 00070 Loss 2.2722\n",
      "Epoch 00009 Step 00080 Loss 2.2321\n",
      "Epoch 00009 Step 00090 Loss 2.2500\n",
      "Epoch 00009 Step 00100 Loss 2.2451\n",
      "Epoch 00009 Step 00110 Loss 2.1789\n",
      "Epoch 00009 Step 00120 Loss 2.2018\n",
      "Epoch 00009 Step 00130 Loss 2.2469\n",
      "Epoch 00009 Step 00140 Loss 2.2535\n",
      "Epoch 00009 Step 00150 Loss 2.2228\n",
      "Epoch 9 Validation Accuracy 0.34165138180304877\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    procs = []\n",
    "    data = graph, node_features, node_labels, train_nids, valid_nids, test_nids, num_features, num_classes\n",
    "    for proc_id in range(4):    # 4 gpus\n",
    "        p = mp.Process(target=train, args=(proc_id, 4, data))\n",
    "        p.start()\n",
    "        procs.append(p)\n",
    "    for p in procs:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you have learned how to train a multi-layer RGCN with neighbor sampling on a large heterogeneous dataset that cannot fit into a single GPU.  The method you have learned can scale to a graph of any size, and works on a single machine with a single GPU."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}