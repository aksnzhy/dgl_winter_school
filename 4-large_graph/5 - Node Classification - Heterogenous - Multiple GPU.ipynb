{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Stochastic Training of GNN for Node Classification on Large Heterogeneous  Graphs\n",
    "\n",
    "This tutorial shows how to train a multi-layer R-GCN for node classification on `ogbn-mag` dataset provided by OGB.\n",
    "\n",
    "The ogbn-mag dataset is a heterogeneous network composed of a subset of the Microsoft Academic Graph (MAG) [1]. It contains four types of entities—papers (736,389 nodes), authors (1,134,649 nodes), institutions (8,740 nodes), and fields of study (59,965 nodes)—as well as four types of directed relations connecting two types of entities—an author is “affiliated with” an institution, an author “writes” a paper, a paper “cites” a paper, and a paper “has a topic of” a field of study.\n",
    "\n",
    "This tutorial's contents include\n",
    "\n",
    "* Creating a DGL graph using the dgl ogb data loader.\n",
    "* Training a GNN model with a single machine, a single GPU, on a graph of any size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dgl\n",
    "import torch\n",
    "import dgl.nn as dglnn\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import sklearn.metrics\n",
    "import tqdm\n",
    "\n",
    "import utils"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Dataset\n",
    "\n",
    "Although you can directly use the Python package provided by OGB, for demonstration, we will instead manually download the dataset, peek into its contents, and process it with only `numpy`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "dataset = DglNodePropPredDataset(name='ogbn-mag')\n",
    "\n",
    "graph, label = dataset[0] # graph: dgl graph object, label: torch tensor of shape (num_nodes, 1)\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_nids, valid_nids, test_nids = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(graph)\n",
    "\n",
    "print('Node labels')\n",
    "node_labels = label['paper'].flatten()\n",
    "print('Shape of target node labels:', node_labels.shape)\n",
    "num_classes = (node_labels.max() + 1).item()\n",
    "print('Number of classes:', num_classes)\n",
    "\n",
    "print('Node features')\n",
    "node_features = graph.nodes['paper'].data['feat']\n",
    "num_features = node_features.shape[1]\n",
    "print('Shape of features of paper node type: {}'.format(num_features))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "src_writes, dst_writes = graph.all_edges(etype=\"writes\")\n",
    "src_topic, dst_topic = graph.all_edges(etype=\"has_topic\")\n",
    "src_aff, dst_aff = graph.all_edges(etype=\"affiliated_with\")\n",
    "\n",
    "\n",
    "graph = dgl.heterograph({\n",
    "    (\"author\", \"writes\", \"paper\"): (src_writes, dst_writes),\n",
    "    (\"paper\", \"has_topic\", \"field_of_study\"): (src_topic, dst_topic),\n",
    "    (\"author\", \"affiliated_with\", \"institution\"): (src_aff, dst_aff),\n",
    "    (\"paper\", \"writes-rev\", \"author\"): (dst_writes, src_writes),\n",
    "    (\"field_of_study\", \"has_topic-rev\", \"paper\"): (dst_topic, src_topic),\n",
    "    (\"institution\", \"affiliated_with-rev\", \"author\"): (dst_aff, src_aff),\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining neighbor sampler and data loader in DGL\n",
    "\n",
    "DGL provides useful tools to generate such computation dependencies while iterating over the dataset in minibatches.  For node classification, you can use `dgl.dataloading.NodeDataLoader` for iterating over the dataset, and `dgl.dataloading.MultiLayerNeighborSampler` to generate computation dependencies of the nodes from a multi-layer GNN with neighbor sampling.\n",
    "\n",
    "The syntax of `dgl.dataloading.NodeDataLoader` is mostly similar to a PyTorch `DataLoader`, with the addition that it needs a graph to generate computation dependency from, a set of node IDs to iterate on, and the neighbor sampler you defined.\n",
    "\n",
    "Let's consider training a 2-layer R-GCN with neighbor sampling, and each node will gather message from 15 neighbors on each layer.  The code defining the data loader and neighbor sampler will look like the following."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_dataloader(rank, world_size, graph, nids, fanout):\n",
    "    part_nids = {}\n",
    "    for ntype, ids in nids.items():\n",
    "        partition_size = len(ids) // world_size\n",
    "        partition_offset = partition_size * rank\n",
    "        ids = ids[partition_offset:partition_offset+partition_size]\n",
    "        part_nids[ntype] = ids\n",
    "    \n",
    "    sampler = dgl.dataloading.MultiLayerNeighborSampler(fanout)\n",
    "    dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        graph, part_nids, sampler,\n",
    "        batch_size=1024,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    return dataloader\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining Model\n",
    "\n",
    "The model can be written as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn as dglnn\n",
    "\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes, n_layers, rel_names):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(in_feats, n_hidden)\n",
    "            for rel in rel_names}, aggregate='sum'))\n",
    "        \n",
    "        for i in range(1, n_layers - 1):\n",
    "            self.layers.append(dglnn.HeteroGraphConv({\n",
    "                rel: dglnn.GraphConv(n_hidden, n_hidden)\n",
    "                for rel in rel_names}, aggregate='sum'))\n",
    "            \n",
    "        self.layers.append(dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(n_hidden, n_classes)\n",
    "            for rel in rel_names}, aggregate='sum'))\n",
    "\n",
    "    def forward(self, bipartites, x):\n",
    "        # inputs are features of nodes\n",
    "        for l, (layer, bipartite) in enumerate(zip(self.layers, bipartites)):\n",
    "            x = layer(bipartite, x)\n",
    "            if l != self.n_layers - 1:\n",
    "                x = {k: F.relu(v) for k, v in x.items()}\n",
    "        return x\n",
    "    \n",
    "\n",
    "class NodeEmbed(nn.Module):\n",
    "    def __init__(self, num_nodes, embed_size,):\n",
    "        super(NodeEmbed, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.node_embeds = nn.ModuleDict()\n",
    "        for ntype in num_nodes:\n",
    "            node_embed = torch.nn.Embedding(num_nodes[ntype], self.embed_size)\n",
    "            nn.init.uniform_(node_embed.weight, -1.0, 1.0)\n",
    "            self.node_embeds[str(ntype)] = node_embed\n",
    "    \n",
    "    def forward(self, node_ids):\n",
    "        embeds = {}\n",
    "        for ntype in node_ids:\n",
    "            embeds[ntype] = self.node_embeds[ntype](node_ids[ntype])\n",
    "        return embeds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_model(rank, in_feats, n_hidden, n_classes, n_layers, rel_names):\n",
    "    model = RGCN(in_feats, n_hidden, n_classes, n_layers, rel_names).to(rank)\n",
    "    return DistributedDataParallel(model, device_ids=[rank], output_device=rank, find_unused_parameters=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining Training Loop\n",
    "\n",
    "The following initializes the model and defines the optimizer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@utils.fix_openmp\n",
    "def train(rank, world_size, data):\n",
    "    # data is the output of load_data\n",
    "    torch.distributed.init_process_group(\n",
    "        backend='nccl',\n",
    "        init_method='tcp://127.0.0.1:12345',\n",
    "        world_size=world_size,\n",
    "        rank=rank)\n",
    "    torch.cuda.set_device(rank)\n",
    "    \n",
    "    graph, node_features, node_labels, train_nids, valid_nids, test_nids, num_features, num_classes = data\n",
    "    \n",
    "    fanout = [15, 15]\n",
    "    \n",
    "    train_dataloader = create_dataloader(rank, world_size, graph, train_nids, fanout)\n",
    "    # We only use one worker for validation\n",
    "    valid_dataloader = create_dataloader(0, 1, graph, valid_nids, fanout)\n",
    "    \n",
    "    num_nodes = {ntype: graph.number_of_nodes(ntype) for ntype in graph.ntypes if ntype != 'paper'}\n",
    "    num_layers = 2\n",
    "    hidden_dim = 128\n",
    "    embed = NodeEmbed(num_nodes, hidden_dim)\n",
    "    \n",
    "    model = init_model(rank, num_features, hidden_dim, num_classes, num_layers, graph.etypes)\n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "    torch.distributed.barrier()\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    best_model_path = 'model.pt'\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "\n",
    "        for step, (input_nodes, output_nodes, bipartites) in enumerate(train_dataloader):\n",
    "            bipartites = [b.to(rank) for b in bipartites]\n",
    "            \n",
    "            nodes_to_embed = {ntype: node_ids for ntype, node_ids in input_nodes.items() if ntype != \"paper\"}\n",
    "            embeddings = {ntype: node_embedding.cuda() for ntype, node_embedding in embed(nodes_to_embed).items()}\n",
    "            \n",
    "            inputs = {'paper': node_features[input_nodes['paper']].cuda()}\n",
    "            inputs.update(embeddings)\n",
    "            \n",
    "            labels = node_labels[output_nodes['paper']].cuda()\n",
    "            predictions = model(bipartites, inputs)['paper']\n",
    "\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "\n",
    "            if rank == 0 and step % 10 == 0:\n",
    "                print('Epoch {:05d} Step {:05d} Loss {:.04f}'.format(epoch, step, loss.item()))\n",
    "\n",
    "        torch.distributed.barrier()\n",
    "        \n",
    "        if rank == 0:\n",
    "            model.eval()\n",
    "            predictions = []\n",
    "            labels = []\n",
    "            with torch.no_grad():\n",
    "                for input_nodes, output_nodes, bipartites in valid_dataloader:\n",
    "                    bipartites = [b.to(rank) for b in bipartites]\n",
    "                    \n",
    "                    nodes_to_embed = {ntype: node_ids for ntype, node_ids in input_nodes.items() if ntype != \"paper\"}\n",
    "                    embeddings = {ntype: node_embedding.cuda() for ntype, node_embedding in embed(nodes_to_embed).items()}\n",
    "                    inputs = {'paper': node_features[input_nodes['paper']].cuda()}\n",
    "                    inputs.update(embeddings)\n",
    "            \n",
    "                    labels.append(node_labels[output_nodes['paper']].numpy())\n",
    "                    predictions.append(model(bipartites, inputs)['paper'].argmax(1).cpu().numpy())\n",
    "                predictions = np.concatenate(predictions)\n",
    "                labels = np.concatenate(labels)\n",
    "                accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "                print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
    "                if best_accuracy < accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    torch.save(model.module.state_dict(), best_model_path)\n",
    "                    \n",
    "        torch.distributed.barrier()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    procs = []\n",
    "    data = graph, node_features, node_labels, train_nids, valid_nids, test_nids, num_features, num_classes\n",
    "    for proc_id in range(4):    # 4 gpus\n",
    "        p = mp.Process(target=train, args=(proc_id, 4, data))\n",
    "        p.start()\n",
    "        procs.append(p)\n",
    "    for p in procs:\n",
    "        p.join()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you have learned how to train a multi-layer RGCN with neighbor sampling on a large heterogeneous dataset that cannot fit into a single GPU.  The method you have learned can scale to a graph of any size, and works on a single machine with a single GPU."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you have learned how to train a multi-layer RGCN with neighbor sampling on a large heterogeneous dataset that cannot fit into a single GPU.  The method you have learned can scale to a graph of any size, and works on a single machine with a single GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Although you can directly use the Python package provided by OGB, for demonstration, we will instead manually download the dataset, peek into its contents, and process it with only `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "dataset = DglNodePropPredDataset(name='ogbn-mag')\n",
    "\n",
    "graph, label = dataset[0] # graph: dgl graph object, label: torch tensor of shape (num_nodes, 1)\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_nids, valid_nids, test_nids = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes={'author': 1134649, 'field_of_study': 59965, 'institution': 8740, 'paper': 736389},\n",
      "      num_edges={('author', 'affiliated_with', 'institution'): 1043998, ('author', 'writes', 'paper'): 7145660, ('paper', 'cites', 'paper'): 5416271, ('paper', 'has_topic', 'field_of_study'): 7505078},\n",
      "      metagraph=[('author', 'institution', 'affiliated_with'), ('author', 'paper', 'writes'), ('paper', 'paper', 'cites'), ('paper', 'field_of_study', 'has_topic')])\n",
      "Node labels\n",
      "Shape of target node labels: torch.Size([736389])\n",
      "Number of classes: 349\n",
      "Node features\n",
      "Shape of features of paper node type: 128\n"
     ]
    }
   ],
   "source": [
    "print(graph)\n",
    "\n",
    "print('Node labels')\n",
    "node_labels = label['paper'].flatten()\n",
    "print('Shape of target node labels:', node_labels.shape)\n",
    "num_classes = (node_labels.max() + 1).item()\n",
    "print('Number of classes:', num_classes)\n",
    "\n",
    "print('Node features')\n",
    "node_features = graph.nodes['paper'].data['feat']\n",
    "num_features = node_features.shape[1]\n",
    "print('Shape of features of paper node type: {}'.format(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_writes, dst_writes = graph.all_edges(etype=\"writes\")\n",
    "src_topic, dst_topic = graph.all_edges(etype=\"has_topic\")\n",
    "src_aff, dst_aff = graph.all_edges(etype=\"affiliated_with\")\n",
    "\n",
    "\n",
    "graph = dgl.heterograph({\n",
    "    (\"author\", \"writes\", \"paper\"): (src_writes, dst_writes),\n",
    "    (\"paper\", \"has_topic\", \"field_of_study\"): (src_topic, dst_topic),\n",
    "    (\"author\", \"affiliated_with\", \"institution\"): (src_aff, dst_aff),\n",
    "    (\"paper\", \"writes-rev\", \"author\"): (dst_writes, src_writes),\n",
    "    (\"field_of_study\", \"has_topic-rev\", \"paper\"): (dst_topic, src_topic),\n",
    "    (\"institution\", \"affiliated_with-rev\", \"author\"): (dst_aff, src_aff),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining neighbor sampler and data loader in DGL\n",
    "\n",
    "DGL provides useful tools to generate such computation dependencies while iterating over the dataset in minibatches.  For node classification, you can use `dgl.dataloading.NodeDataLoader` for iterating over the dataset, and `dgl.dataloading.MultiLayerNeighborSampler` to generate computation dependencies of the nodes from a multi-layer GNN with neighbor sampling.\n",
    "\n",
    "The syntax of `dgl.dataloading.NodeDataLoader` is mostly similar to a PyTorch `DataLoader`, with the addition that it needs a graph to generate computation dependency from, a set of node IDs to iterate on, and the neighbor sampler you defined.\n",
    "\n",
    "Let's consider training a 2-layer R-GCN with neighbor sampling, and each node will gather message from 15 neighbors on each layer.  The code defining the data loader and neighbor sampler will look like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(rank, world_size, graph, nids, fanout):\n",
    "    part_nids = {}\n",
    "    for ntype, ids in nids.items():\n",
    "        partition_size = len(ids) // world_size\n",
    "        partition_offset = partition_size * rank\n",
    "        ids = ids[partition_offset:partition_offset+partition_size]\n",
    "        part_nids[ntype] = ids\n",
    "    \n",
    "    sampler = dgl.dataloading.MultiLayerNeighborSampler(fanout)\n",
    "    dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        graph, part_nids, sampler,\n",
    "        batch_size=1024,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model\n",
    "\n",
    "The model can be written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn as dglnn\n",
    "\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes, n_layers, rel_names):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(in_feats, n_hidden)\n",
    "            for rel in rel_names}, aggregate='sum'))\n",
    "        \n",
    "        for i in range(1, n_layers - 1):\n",
    "            self.layers.append(dglnn.HeteroGraphConv({\n",
    "                rel: dglnn.GraphConv(n_hidden, n_hidden)\n",
    "                for rel in rel_names}, aggregate='sum'))\n",
    "            \n",
    "        self.layers.append(dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(n_hidden, n_classes)\n",
    "            for rel in rel_names}, aggregate='sum'))\n",
    "\n",
    "    def forward(self, bipartites, x):\n",
    "        # inputs are features of nodes\n",
    "        for l, (layer, bipartite) in enumerate(zip(self.layers, bipartites)):\n",
    "            x = layer(bipartite, x)\n",
    "            if l != self.n_layers - 1:\n",
    "                x = {k: F.relu(v) for k, v in x.items()}\n",
    "        return x\n",
    "    \n",
    "\n",
    "class NodeEmbed(nn.Module):\n",
    "    def __init__(self, num_nodes, embed_size,):\n",
    "        super(NodeEmbed, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.node_embeds = nn.ModuleDict()\n",
    "        for ntype in num_nodes:\n",
    "            node_embed = torch.nn.Embedding(num_nodes[ntype], self.embed_size)\n",
    "            nn.init.uniform_(node_embed.weight, -1.0, 1.0)\n",
    "            self.node_embeds[str(ntype)] = node_embed\n",
    "    \n",
    "    def forward(self, node_ids):\n",
    "        embeds = {}\n",
    "        for ntype in node_ids:\n",
    "            embeds[ntype] = self.node_embeds[ntype](node_ids[ntype])\n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(rank, in_feats, n_hidden, n_classes, n_layers, rel_names):\n",
    "    model = RGCN(in_feats, n_hidden, n_classes, n_layers, rel_names).to(rank)\n",
    "    return DistributedDataParallel(model, device_ids=[rank], output_device=rank, find_unused_parameters=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Training Loop\n",
    "\n",
    "The following initializes the model and defines the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@utils.fix_openmp\n",
    "def train(rank, world_size, data):\n",
    "    # data is the output of load_data\n",
    "    torch.distributed.init_process_group(\n",
    "        backend='nccl',\n",
    "        init_method='tcp://127.0.0.1:12345',\n",
    "        world_size=world_size,\n",
    "        rank=rank)\n",
    "    torch.cuda.set_device(rank)\n",
    "    \n",
    "    graph, node_features, node_labels, train_nids, valid_nids, test_nids, num_features, num_classes = data\n",
    "    \n",
    "    fanout = [15, 15]\n",
    "    \n",
    "    train_dataloader = create_dataloader(rank, world_size, graph, train_nids, fanout)\n",
    "    # We only use one worker for validation\n",
    "    valid_dataloader = create_dataloader(0, 1, graph, valid_nids, fanout)\n",
    "    \n",
    "    num_nodes = {ntype: graph.number_of_nodes(ntype) for ntype in graph.ntypes if ntype != 'paper'}\n",
    "    num_layers = 2\n",
    "    hidden_dim = 128\n",
    "    embed = NodeEmbed(num_nodes, hidden_dim)\n",
    "    \n",
    "    model = init_model(rank, num_features, hidden_dim, num_classes, num_layers, graph.etypes)\n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "    torch.distributed.barrier()\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    best_model_path = 'model.pt'\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "\n",
    "        for step, (input_nodes, output_nodes, bipartites) in enumerate(train_dataloader):\n",
    "            bipartites = [b.to(rank) for b in bipartites]\n",
    "            \n",
    "            nodes_to_embed = {ntype: node_ids for ntype, node_ids in input_nodes.items() if ntype != \"paper\"}\n",
    "            embeddings = {ntype: node_embedding.cuda() for ntype, node_embedding in embed(nodes_to_embed).items()}\n",
    "            \n",
    "            inputs = {'paper': node_features[input_nodes['paper']].cuda()}\n",
    "            inputs.update(embeddings)\n",
    "            \n",
    "            labels = node_labels[output_nodes['paper']].cuda()\n",
    "            predictions = model(bipartites, inputs)['paper']\n",
    "\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "\n",
    "            if rank == 0 and step % 10 == 0:\n",
    "                print('Epoch {:05d} Step {:05d} Loss {:.04f}'.format(epoch, step, loss.item()))\n",
    "\n",
    "        torch.distributed.barrier()\n",
    "        \n",
    "        if rank == 0:\n",
    "            model.eval()\n",
    "            predictions = []\n",
    "            labels = []\n",
    "            with torch.no_grad():\n",
    "                for input_nodes, output_nodes, bipartites in valid_dataloader:\n",
    "                    bipartites = [b.to(rank) for b in bipartites]\n",
    "                    \n",
    "                    nodes_to_embed = {ntype: node_ids for ntype, node_ids in input_nodes.items() if ntype != \"paper\"}\n",
    "                    embeddings = {ntype: node_embedding.cuda() for ntype, node_embedding in embed(nodes_to_embed).items()}\n",
    "                    inputs = {'paper': node_features[input_nodes['paper']].cuda()}\n",
    "                    inputs.update(embeddings)\n",
    "            \n",
    "                    labels.append(node_labels[output_nodes['paper']].numpy())\n",
    "                    predictions.append(model(bipartites, inputs)['paper'].argmax(1).cpu().numpy())\n",
    "                predictions = np.concatenate(predictions)\n",
    "                labels = np.concatenate(labels)\n",
    "                accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "                print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
    "                if best_accuracy < accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    torch.save(model.module.state_dict(), best_model_path)\n",
    "                    \n",
    "        torch.distributed.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 Step 00000 Loss 6.4136\n",
      "Epoch 00000 Step 00010 Loss 4.9499\n",
      "Epoch 00000 Step 00020 Loss 4.5907\n",
      "Epoch 00000 Step 00030 Loss 4.3993\n",
      "Epoch 00000 Step 00040 Loss 4.0387\n",
      "Epoch 00000 Step 00050 Loss 3.9396\n",
      "Epoch 00000 Step 00060 Loss 3.7282\n",
      "Epoch 00000 Step 00070 Loss 3.5739\n",
      "Epoch 00000 Step 00080 Loss 3.5383\n",
      "Epoch 00000 Step 00090 Loss 3.3513\n",
      "Epoch 00000 Step 00100 Loss 3.3080\n",
      "Epoch 00000 Step 00110 Loss 3.2446\n",
      "Epoch 00000 Step 00120 Loss 3.1437\n",
      "Epoch 00000 Step 00130 Loss 3.1746\n",
      "Epoch 00000 Step 00140 Loss 3.0759\n",
      "Epoch 00000 Step 00150 Loss 3.0929\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    procs = []\n",
    "    data = graph, node_features, node_labels, train_nids, valid_nids, test_nids, num_features, num_classes\n",
    "    for proc_id in range(4):    # 4 gpus\n",
    "        p = mp.Process(target=train, args=(proc_id, 4, data))\n",
    "        p.start()\n",
    "        procs.append(p)\n",
    "    for p in procs:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you have learned how to train a multi-layer RGCN with neighbor sampling on a large heterogeneous dataset that cannot fit into a single GPU.  The method you have learned can scale to a graph of any size, and works on a single machine with a single GPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}