{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stochastic Training of GNN with Multiple GPUs\n",
    "\n",
    "*Note: this tutorial requires a GPU enabled machine with multiple gpu devices*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This tutorials will show you to \n",
    "\n",
    "* train a GNN model on a single machine with multiple GPUs on a graph of any size with `torch.nn.parallel.DistributedDataParallel`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "At the end of this tutorial you will be able to \n",
    "* Parallelize model training across multiple GPUs on a single device\n",
    "* Distribute the model parameters using PyTorch DDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distributed training overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Training models on very large datasets can take hours or even days to converge\n",
    "\n",
    "In deep learning, we can get substantial speed-ups by distributing the training workload across multiple workers.\n",
    "\n",
    "Typically, workers run in parallel and can communicate their updates (directly or via a central hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Workers can be individual machines in a cluster (**not covered in this tutorial**)\n",
    "\n",
    "In a single machine with multiple gpus, workers can be each gpu device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Parallelism\n",
    "\n",
    "For Multi-GPU training on a single machine, Data parallelism is an easy-to-implement and effective distributed training approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here is how it works:\n",
    "\n",
    "* The data is divided into `k` partitions where `k` is the number of gpu workers\n",
    "* The model is copied to each of the gpu workers\n",
    "* Each worker operates on its own subset of the data\n",
    "* Each worker communicates of its changes to the other workers to update their corresponding models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "PyTorch `DistributedDataParallel` (DDP) is the recommended built-in solution for multi-GPU training.\n",
    "\n",
    "You can use PyTorch DDP for DGL models in the same way you would for any other PyTorch applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Pytorch DDP implements data parallelism at the module level, therefore it wraps the model implementation\n",
    "\n",
    "* To use it, your code needs to spawn multiple processes each with it's own DDP instance\n",
    "\n",
    "* DDP uses collective communications to synchronize gradients and buffers\n",
    "\n",
    "* For machines with Nvidia GPUs it's common use `nccl` as the communications backend\n",
    "\n",
    "See https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html for more\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dgl\n",
    "import torch\n",
    "import dgl.nn as dglnn\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import sklearn.metrics\n",
    "import tqdm\n",
    "\n",
    "import utils\n",
    "from utils import thread_wrapped_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load Dataset\n",
    "\n",
    "The following code is loading the dataset from the first tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    import pickle\n",
    "\n",
    "    with open('data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    graph, node_features, node_labels, train_nids, valid_nids, test_nids = data\n",
    "    utils.prepare_mp(graph)\n",
    "    \n",
    "    num_features = node_features.shape[1]\n",
    "    num_classes = (node_labels.max() + 1).item()\n",
    "    \n",
    "    return graph, node_features, node_labels, train_nids, valid_nids, test_nids, num_features, num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Customize Neighborhood Sampling\n",
    "\n",
    "Previously we have seen how to use `NodeDataLoader` together with `MultiLayerNeighborSampler`.  In fact, you can replace `MultiLayerNeighborSampler` with your own sampling strategy.\n",
    "\n",
    "The customization is simple.  For each GNN layer, you only need to specify the edges involved in the message passing as a graph.  Such a graph will have the same nodes as the original graph.  For example, here is how `MultiLayerNeighborSampler` is implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class MultiLayerNeighborSampler(dgl.dataloading.BlockSampler):\n",
    "    def __init__(self, fanouts):\n",
    "        super().__init__(len(fanouts), return_eids=False)\n",
    "        self.fanouts = fanouts\n",
    "        \n",
    "    def sample_frontier(self, layer_id, g, seed_nodes):\n",
    "        fanout = self.fanouts[layer_id]\n",
    "        return dgl.sampling.sample_neighbors(g, seed_nodes, fanout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining Data Loader for Distributed Data Parallel (DDP)\n",
    "\n",
    "In PyTorch DDP each worker process is assigned an integer *rank*.  \n",
    "\n",
    "The rank indicates which partition of the dataset the worker process will handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataloader(rank, world_size, graph, nids):\n",
    "    partition_size = len(nids) // world_size\n",
    "    partition_offset = partition_size * rank\n",
    "    nids = nids[partition_offset:partition_offset+partition_size]\n",
    "    \n",
    "    sampler = MultiLayerNeighborSampler([4, 4, 4])\n",
    "    dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        graph, nids, sampler,\n",
    "        batch_size=1024,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining Model\n",
    "\n",
    "The model implementation will be exactly the same as what you have seen in the first tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(dglnn.SAGEConv(in_feats, n_hidden, 'mean'))\n",
    "        for i in range(1, n_layers - 1):\n",
    "            self.layers.append(dglnn.SAGEConv(n_hidden, n_hidden, 'mean'))\n",
    "        self.layers.append(dglnn.SAGEConv(n_hidden, n_classes, 'mean'))\n",
    "        \n",
    "    def forward(self, bipartites, x):\n",
    "        for l, (layer, bipartite) in enumerate(zip(self.layers, bipartites)):\n",
    "            x = layer(bipartite, x)\n",
    "            if l != self.n_layers - 1:\n",
    "                x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distributing the Model to GPUs\n",
    "\n",
    "PyTorch DDP manages the distribution of models and synchronization of the gradients for you.  \n",
    "\n",
    "For a DGL you can simply wrap the model with `torch.nn.parallel.DistributedDataParallel`.\n",
    "\n",
    "Here we make a simple function that does that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def init_model(rank, in_feats, n_hidden, n_classes, n_layers):\n",
    "    model = SAGE(in_feats, n_hidden, n_classes, n_layers).to(rank)\n",
    "    return DistributedDataParallel(model, device_ids=[rank], output_device=rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The recommended way to distribute training is to have one training process per GPU\n",
    "\n",
    "During model instantiation we also specify the process rank, which is equal to the GPU ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Training Loop for one Process\n",
    "\n",
    "The training loop for a single process running with a single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@thread_wrapped_func\n",
    "def train(rank, world_size, data):\n",
    "    # data is the output of load_data\n",
    "    torch.distributed.init_process_group(\n",
    "        backend='nccl',\n",
    "        init_method='tcp://127.0.0.1:12345',\n",
    "        world_size=world_size,\n",
    "        rank=rank)\n",
    "    torch.cuda.set_device(rank)\n",
    "    \n",
    "    graph, node_features, node_labels, train_nids, valid_nids, test_nids, num_features, num_classes = data\n",
    "    \n",
    "    train_dataloader = create_dataloader(rank, world_size, graph, train_nids)\n",
    "    # We only use one worker for validation\n",
    "    valid_dataloader = create_dataloader(0, 1, graph, valid_nids)\n",
    "    \n",
    "    model = init_model(rank, num_features, 128, num_classes, 3)\n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "    torch.distributed.barrier()\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    best_model_path = 'model.pt'\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "\n",
    "        for step, (input_nodes, output_nodes, bipartites) in enumerate(train_dataloader):\n",
    "            bipartites = [b.to(rank) for b in bipartites]\n",
    "            inputs = node_features[input_nodes].cuda()\n",
    "            labels = node_labels[output_nodes].cuda()\n",
    "            predictions = model(bipartites, inputs)\n",
    "\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "\n",
    "            if rank == 0 and step % 10 == 0:\n",
    "                print('Epoch {:05d} Step {:05d} Loss {:.04f}'.format(epoch, step, loss.item()))\n",
    "\n",
    "        torch.distributed.barrier()\n",
    "        \n",
    "        if rank == 0:\n",
    "            model.eval()\n",
    "            predictions = []\n",
    "            labels = []\n",
    "            with torch.no_grad():\n",
    "                for input_nodes, output_nodes, bipartites in valid_dataloader:\n",
    "                    bipartites = [b.to(rank) for b in bipartites]\n",
    "                    inputs = node_features[input_nodes].cuda()\n",
    "                    labels.append(node_labels[output_nodes].numpy())\n",
    "                    predictions.append(model.module(bipartites, inputs).argmax(1).cpu().numpy())\n",
    "                predictions = np.concatenate(predictions)\n",
    "                labels = np.concatenate(labels)\n",
    "                accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "                print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
    "                if best_accuracy < accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    torch.save(model.module.state_dict(), best_model_path)\n",
    "                    \n",
    "        torch.distributed.barrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spawning multiple processes for the Multi GPU training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 Step 00000 Loss 6.7778\n",
      "Epoch 00000 Step 00010 Loss 3.0505\n",
      "Epoch 00000 Step 00020 Loss 2.2261\n",
      "Epoch 00000 Step 00030 Loss 1.8884\n",
      "Epoch 00000 Step 00040 Loss 1.6211\n",
      "Epoch 0 Validation Accuracy 0.7000991785977672\n",
      "Epoch 00001 Step 00000 Loss 1.5559\n",
      "Epoch 00001 Step 00010 Loss 1.3617\n",
      "Epoch 00001 Step 00020 Loss 1.3690\n",
      "Epoch 00001 Step 00030 Loss 1.3224\n",
      "Epoch 00001 Step 00040 Loss 1.1171\n",
      "Epoch 1 Validation Accuracy 0.7940645423797777\n",
      "Epoch 00002 Step 00000 Loss 1.2186\n",
      "Epoch 00002 Step 00010 Loss 1.1311\n",
      "Epoch 00002 Step 00020 Loss 1.0952\n",
      "Epoch 00002 Step 00030 Loss 1.0875\n",
      "Epoch 00002 Step 00040 Loss 0.9908\n",
      "Epoch 2 Validation Accuracy 0.8124507285812375\n",
      "Epoch 00003 Step 00000 Loss 1.0880\n",
      "Epoch 00003 Step 00010 Loss 1.0708\n",
      "Epoch 00003 Step 00020 Loss 1.1645\n",
      "Epoch 00003 Step 00030 Loss 0.9818\n",
      "Epoch 00003 Step 00040 Loss 0.9260\n",
      "Epoch 3 Validation Accuracy 0.825267655061923\n",
      "Epoch 00004 Step 00000 Loss 1.0013\n",
      "Epoch 00004 Step 00010 Loss 0.8391\n",
      "Epoch 00004 Step 00020 Loss 0.9800\n",
      "Epoch 00004 Step 00030 Loss 0.9255\n",
      "Epoch 00004 Step 00040 Loss 0.8191\n",
      "Epoch 4 Validation Accuracy 0.838516898507235\n",
      "Epoch 00005 Step 00000 Loss 0.8881\n",
      "Epoch 00005 Step 00010 Loss 0.9145\n",
      "Epoch 00005 Step 00020 Loss 0.9187\n",
      "Epoch 00005 Step 00030 Loss 0.9237\n",
      "Epoch 00005 Step 00040 Loss 0.8962\n",
      "Epoch 5 Validation Accuracy 0.8435521196246472\n",
      "Epoch 00006 Step 00000 Loss 0.9429\n",
      "Epoch 00006 Step 00010 Loss 0.8541\n",
      "Epoch 00006 Step 00020 Loss 0.8271\n",
      "Epoch 00006 Step 00030 Loss 0.8841\n",
      "Epoch 00006 Step 00040 Loss 0.8993\n",
      "Epoch 6 Validation Accuracy 0.8420262950436131\n",
      "Epoch 00007 Step 00000 Loss 0.8235\n",
      "Epoch 00007 Step 00010 Loss 0.8671\n",
      "Epoch 00007 Step 00020 Loss 0.8656\n",
      "Epoch 00007 Step 00030 Loss 0.8364\n",
      "Epoch 00007 Step 00040 Loss 0.7596\n",
      "Epoch 7 Validation Accuracy 0.8478498588612262\n",
      "Epoch 00008 Step 00000 Loss 0.8584\n",
      "Epoch 00008 Step 00010 Loss 0.8204\n",
      "Epoch 00008 Step 00020 Loss 0.8605\n",
      "Epoch 00008 Step 00030 Loss 0.7990\n",
      "Epoch 00008 Step 00040 Loss 0.7720\n",
      "Epoch 8 Validation Accuracy 0.8526816367011673\n",
      "Epoch 00009 Step 00000 Loss 0.7006\n",
      "Epoch 00009 Step 00010 Loss 0.7576\n",
      "Epoch 00009 Step 00020 Loss 0.7385\n",
      "Epoch 00009 Step 00030 Loss 0.7888\n",
      "Epoch 00009 Step 00040 Loss 0.7700\n",
      "Epoch 9 Validation Accuracy 0.856572489382804\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    procs = []\n",
    "    data = load_data()\n",
    "    for proc_id in range(4):    # 4 gpus\n",
    "        p = mp.Process(target=train, args=(proc_id, 4, data))\n",
    "        p.start()\n",
    "        procs.append(p)\n",
    "    for p in procs:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you have learned how to train a multi-layer GraphSAGE for node classification on a large dataset that cannot fit into GPU.  The method you have learned can scale to a graph of any size, and works on a single machine with *any number of* GPU.\n",
    "\n",
    "## What's next?\n",
    "\n",
    "The next tutorial will be about adapting mini-batch training procedure for heterogeneous graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Additional material: caveat in training with DDP\n",
    "\n",
    "When writing DDP code, you may often find these two kinds of errors:\n",
    "\n",
    "* `Cannot re-initialize CUDA in forked subprocess`\n",
    "\n",
    "  This is because you have initialized the CUDA context before creating subprocesses using `mp.Process`.  Solutions include:\n",
    "  \n",
    "  * Remove all the code that can possibly initialize CUDA context before calling `mp.Process`.  For instance, you cannot get number of GPUs via `torch.cuda.device_count()` before calling `mp.Process` since that also initializes CUDA context.  You can check whether CUDA context is initialized via `torch.cuda.is_initialized()`.\n",
    "  \n",
    "  * Use `torch.multiprocessing.spawn()` to create processes instead of forking with `mp.Process`.  A downside is that Python will duplicate the graph storage for every process spawned this way.  Memory consumption will linearly scale up.\n",
    "  \n",
    "* Training process freezes during minibatch iteration.\n",
    "\n",
    "  This is due to a [lasting bug in the interaction between GNU OpenMP and `fork`](https://github.com/pytorch/pytorch/issues/17199).  A workaround is to wrap the target function of `mp.Process` with the decorator `utils.thread_wrapped_func`, provided in the tutorial."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
