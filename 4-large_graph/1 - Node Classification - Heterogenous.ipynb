{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Training of GNN for Node Classification on Heterogeneous Large Graphs\n",
    "\n",
    "This tutorial shows how to train a multi-layer R-GCN for node classification on `ogbn-mag` dataset provided by OGB.\n",
    "\n",
    "The ogbn-mag dataset is a heterogeneous network composed of a subset of the Microsoft Academic Graph (MAG) [1]. It contains four types of entities—papers (736,389 nodes), authors (1,134,649 nodes), institutions (8,740 nodes), and fields of study (59,965 nodes)—as well as four types of directed relations connecting two types of entities—an author is “affiliated with” an institution, an author “writes” a paper, a paper “cites” a paper, and a paper “has a topic of” a field of study.\n",
    "\n",
    "This tutorial's contents include\n",
    "\n",
    "* Creating a DGL graph using the dgl ogb data loader.\n",
    "* Training a GNN model with a single machine, a single GPU, on a graph of any size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Although you can directly use the Python package provided by OGB, for demonstration, we will instead manually download the dataset, peek into its contents, and process it with only `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ogb in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (1.2.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ogb) (0.23.2)\n",
      "Requirement already satisfied: torch>=1.2.0 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ogb) (1.6.0)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ogb) (1.0.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ogb) (1.14.0)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ogb) (1.25.8)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ogb) (1.19.1)\n",
      "Requirement already satisfied: outdated>=0.2.0 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ogb) (0.2.0)\n",
      "Requirement already satisfied: tqdm>=4.29.0 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ogb) (4.42.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn>=0.20.0->ogb) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn>=0.20.0->ogb) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn>=0.20.0->ogb) (0.14.1)\n",
      "Requirement already satisfied: future in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch>=1.2.0->ogb) (0.18.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas>=0.24.0->ogb) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas>=0.24.0->ogb) (2019.3)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from outdated>=0.2.0->ogb) (2.22.0)\n",
      "Requirement already satisfied: littleutils in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from outdated>=0.2.0->ogb) (0.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->outdated>=0.2.0->ogb) (2020.6.20)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->outdated>=0.2.0->ogb) (2.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install ogb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The OGB package is out of date. Your version is 1.2.3, while the latest version is 1.2.4.\n",
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "dataset = DglNodePropPredDataset(name='ogbn-mag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following:\n",
    "\n",
    "* DGL graph object (source-destination pairs)\n",
    "* The node label tensor\n",
    "\n",
    "We can also use the utility function in the dataset to get the train, validation, test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph, label = dataset[0] # graph: dgl graph object, label: torch tensor of shape (num_nodes, 1)\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_nids, valid_nids, test_nids = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the size of the graph, features, and labels as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes={'author': 1134649, 'field_of_study': 59965, 'institution': 8740, 'paper': 736389},\n",
      "      num_edges={('author', 'affiliated_with', 'institution'): 1043998, ('author', 'writes', 'paper'): 7145660, ('paper', 'cites', 'paper'): 5416271, ('paper', 'has_topic', 'field_of_study'): 7505078},\n",
      "      metagraph=[('author', 'institution', 'affiliated_with'), ('author', 'paper', 'writes'), ('paper', 'paper', 'cites'), ('paper', 'field_of_study', 'has_topic')])\n",
      "Node labels\n",
      "Shape of target node labels: torch.Size([736389])\n",
      "Number of classes: 349\n",
      "Node features\n",
      "Shape of features of paper node type: 128\n"
     ]
    }
   ],
   "source": [
    "print(graph)\n",
    "\n",
    "print('Node labels')\n",
    "node_labels = label['paper'].flatten()\n",
    "print('Shape of target node labels:', node_labels.shape)\n",
    "num_classes = (node_labels.max() + 1).item()\n",
    "print('Number of classes:', num_classes)\n",
    "\n",
    "print('Node features')\n",
    "node_features = graph.nodes['paper'].data['feat']\n",
    "num_features = node_features.shape[1]\n",
    "print('Shape of features of paper node type: {}'.format(num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Data Loader with Neighbor Sampling\n",
    "\n",
    "### Neighbor sampling overview\n",
    "\n",
    "The formulation of message passing usually has the following form:\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "  \\boldsymbol{a}_v^{(l)} = \\rho^{(l)} \\left(\n",
    "    \\left\\lbrace\n",
    "      \\boldsymbol{h}_u^{(l-1)} : u \\in \\mathcal{N} \\left( v \\right)\n",
    "    \\right\\rbrace\n",
    "  \\right)\n",
    "\\\\\n",
    "  \\boldsymbol{h}_v^{(l)} = \\phi^{(l)} \\left(\n",
    "    \\boldsymbol{h}_v^{(l-1)}, \\boldsymbol{a}_v^{(l)}\n",
    "  \\right)\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "where $\\rho^{(l)}$ and $\\phi^{(l)}$ are parameterized functions, and $\\mathcal{N}(v)$ represents the set of predecessors (or equivalently *neighbors*) of $v$ on graph $\\mathcal{G}$:\n",
    "$$\n",
    "\\mathcal{N} \\left( v \\right) = \\left\\lbrace\n",
    "  s \\left( e \\right) : e \\in \\mathbb{E}, t \\left( e \\right) = v\n",
    "\\right\\rbrace\n",
    "$$\n",
    "\n",
    "For instance, to perform a message passing for updating the red node in the following graph:\n",
    "\n",
    "![Imgur](assets/1.png)\n",
    "\n",
    "One needs to aggregate the node features of its neighbors, shown as green nodes:\n",
    "\n",
    "![Imgur](assets/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider how multi-layer message passing works for computing the output of a single node.  In the following text, we refer to the nodes whose outputs GNN will compute as seed nodes.\n",
    "\n",
    "Consider computing with a 2-layer GNN the output of the seed node 8, colored red, in the following graph:\n",
    "\n",
    "![Imgur](assets/seed.png)\n",
    "\n",
    "By the formulation:\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "  \\boldsymbol{a}_8^{(2)} = \\rho^{(2)} \\left(\n",
    "    \\left\\lbrace\n",
    "      \\boldsymbol{h}_u^{(1)} : u \\in \\mathcal{N} \\left( 8 \\right)\n",
    "    \\right\\rbrace\n",
    "  \\right) = \\rho^{(2)} \\left(\n",
    "    \\left\\lbrace\n",
    "      \\boldsymbol{h}_4^{(1)}, \\boldsymbol{h}_5^{(1)},\n",
    "      \\boldsymbol{h}_7^{(1)}, \\boldsymbol{h}_{11}^{(1)}\n",
    "    \\right\\rbrace\n",
    "  \\right)\n",
    "\\\\\n",
    "  \\boldsymbol{h}_8^{(2)} = \\phi^{(2)} \\left(\n",
    "    \\boldsymbol{h}_8^{(1)}, \\boldsymbol{a}_8^{(2)}\n",
    "  \\right)\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "We can tell from the formulation that, to compute $\\boldsymbol{h}_8^{(2)}$, we need messages from node 4, 5, 7, and 11 (colored green) along the edges visualized below.\n",
    "\n",
    "![Imgur](assets/3.png)\n",
    "\n",
    "The values of $\\boldsymbol{h}_\\cdot^{(1)}$ are the outputs from the first GNN layer.  To compute those values for the red and green nodes, we further need to perform message passing on the edges visualized below.\n",
    "\n",
    "![Imgur](assets/4.png)\n",
    "\n",
    "Therefore, to compute the 2-layer GNN representation of the red node, we need the input features from the red node as well as the green and yellow nodes.  Note that we should take red node's neighbors again for this layer.\n",
    "\n",
    "You may notice that the procedure which determines computation dependency is in the reverse direction of message aggregation: you start from the layer closest to the output and work backward to the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also see that computing representation for a small number of nodes often requires input features of a significantly larger number of nodes.  Taking all neighbors for message aggregation is often too costly since the nodes needed would easily cover a large portion of the graph.\n",
    "\n",
    "Neighbor sampling addresses this issue by selecting a random subset of the neighbors to perform aggregation.  For instance, to compute $\\boldsymbol{h}_8^{(1)}$, we can choose to sample 2 neighbors and aggregate.\n",
    "\n",
    "![Imgur](assets/5.png)\n",
    "\n",
    "Similarly, to compute the red and green nodes' first layer representation, we can also do neighbor sampling that takes 2 neighbors for each node.  Note that we should take the red node's neighbors again for this layer.\n",
    "\n",
    "![Imgur](assets/6.png)\n",
    "\n",
    "You can see that this method could give us fewer nodes needed for input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining neighbor sampler and data loader in DGL\n",
    "\n",
    "DGL provides useful tools to generate such computation dependencies while iterating over the dataset in minibatches.  For node classification, you can use `dgl.dataloading.NodeDataLoader` for iterating over the dataset, and `dgl.dataloading.MultiLayerNeighborSampler` to generate computation dependencies of the nodes from a multi-layer GNN with neighbor sampling.\n",
    "\n",
    "The syntax of `dgl.dataloading.NodeDataLoader` is mostly similar to a PyTorch `DataLoader`, with the addition that it needs a graph to generate computation dependency from, a set of node IDs to iterate on, and the neighbor sampler you defined.\n",
    "\n",
    "Let's consider training a 3-layer R-GCN with neighbor sampling, and each node will gather message from 4 neighbors on each layer.  The code defining the data loader and neighbor sampler will look like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "\n",
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4, 4])\n",
    "train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, train_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate over the data loader we created and see what it gives us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'author': tensor([365776,  60441, 198717,  ...,  41465,  65043, 649864]), 'field_of_study': tensor([], dtype=torch.int64), 'institution': tensor([], dtype=torch.int64), 'paper': tensor([224131,   2074,  13576,  ..., 716197, 698763, 661550])}, {'author': tensor([], dtype=torch.int64), 'field_of_study': tensor([], dtype=torch.int64), 'institution': tensor([], dtype=torch.int64), 'paper': tensor([224131,   2074,  13576,  ..., 190503,  58541, 207510])}, [Block(num_src_nodes={'author': 27329, 'field_of_study': 0, 'institution': 0, 'paper': 22345},\n",
      "      num_dst_nodes={'author': 10952, 'field_of_study': 0, 'institution': 0, 'paper': 10004},\n",
      "      num_edges={('author', 'affiliated_with', 'institution'): 0, ('author', 'writes', 'paper'): 32706, ('paper', 'cites', 'paper'): 22723, ('paper', 'has_topic', 'field_of_study'): 0},\n",
      "      metagraph=[('author', 'institution', 'affiliated_with'), ('author', 'paper', 'writes'), ('paper', 'paper', 'cites'), ('paper', 'field_of_study', 'has_topic')]), Block(num_src_nodes={'author': 10952, 'field_of_study': 0, 'institution': 0, 'paper': 10004},\n",
      "      num_dst_nodes={'author': 3263, 'field_of_study': 0, 'institution': 0, 'paper': 3748},\n",
      "      num_edges={('author', 'affiliated_with', 'institution'): 0, ('author', 'writes', 'paper'): 12154, ('paper', 'cites', 'paper'): 8975, ('paper', 'has_topic', 'field_of_study'): 0},\n",
      "      metagraph=[('author', 'institution', 'affiliated_with'), ('author', 'paper', 'writes'), ('paper', 'paper', 'cites'), ('paper', 'field_of_study', 'has_topic')]), Block(num_src_nodes={'author': 3263, 'field_of_study': 0, 'institution': 0, 'paper': 3748},\n",
      "      num_dst_nodes={'author': 0, 'field_of_study': 0, 'institution': 0, 'paper': 1024},\n",
      "      num_edges={('author', 'affiliated_with', 'institution'): 0, ('author', 'writes', 'paper'): 3275, ('paper', 'cites', 'paper'): 2734, ('paper', 'has_topic', 'field_of_study'): 0},\n",
      "      metagraph=[('author', 'institution', 'affiliated_with'), ('author', 'paper', 'writes'), ('paper', 'paper', 'cites'), ('paper', 'field_of_study', 'has_topic')])])\n"
     ]
    }
   ],
   "source": [
    "example_minibatch = next(iter(train_dataloader))\n",
    "print(example_minibatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NodeDataLoader` gives us three items per iteration.\n",
    "\n",
    "* The input node list for the nodes whose input features are needed to compute the outputs.\n",
    "* The output node list whose GNN representation are to be computed.\n",
    "* The list of computation dependency for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To compute 1024 target nodes' output we need 22345 nodes' input features\n",
      "\n",
      "Output nodes\n",
      "{'author': tensor([], dtype=torch.int64), 'field_of_study': tensor([], dtype=torch.int64), 'institution': tensor([], dtype=torch.int64), 'paper': tensor([224131,   2074,  13576,  ..., 190503,  58541, 207510])}\n",
      "\n",
      "Input nodes\n",
      "{'author': tensor([365776,  60441, 198717,  ...,  41465,  65043, 649864]), 'field_of_study': tensor([], dtype=torch.int64), 'institution': tensor([], dtype=torch.int64), 'paper': tensor([224131,   2074,  13576,  ..., 716197, 698763, 661550])}\n"
     ]
    }
   ],
   "source": [
    "input_nodes, output_nodes, bipartites = example_minibatch\n",
    "print(\"To compute {} target nodes' output we need {} nodes' input features\".format(len(output_nodes['paper']), len(input_nodes['paper'])))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Output nodes\")\n",
    "print(output_nodes)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Input nodes\")\n",
    "print(input_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `bipartites` shows how messages aggregate on each layer.  As its name suggests, it is a **list** of bipartite graphs.\n",
    "\n",
    "So why does DGL return a list of *bipartite* graphs for training a *homogeneous* graph?  The reason is that the number of nodes for input and that for output of a given GNN layer is different.  Take the example above:\n",
    "\n",
    "![Imgur](assets/6.png)\n",
    "\n",
    "That GNN layer will output the representation of three nodes (two green nodes and one red node), but it will require input from 7 nodes (the green nodes and red node, plus 4 yellow nodes).  Only a bipartite graph can describe such computation:\n",
    "\n",
    "![](assets/bipartite.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minibatch training of GNNs usually involves message passing on such bipartite graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block(num_src_nodes={'author': 27329, 'field_of_study': 0, 'institution': 0, 'paper': 22345},\n",
      "      num_dst_nodes={'author': 10952, 'field_of_study': 0, 'institution': 0, 'paper': 10004},\n",
      "      num_edges={('author', 'affiliated_with', 'institution'): 0, ('author', 'writes', 'paper'): 32706, ('paper', 'cites', 'paper'): 22723, ('paper', 'has_topic', 'field_of_study'): 0},\n",
      "      metagraph=[('author', 'institution', 'affiliated_with'), ('author', 'paper', 'writes'), ('paper', 'paper', 'cites'), ('paper', 'field_of_study', 'has_topic')])\n",
      "\n",
      "Block(num_src_nodes={'author': 10952, 'field_of_study': 0, 'institution': 0, 'paper': 10004},\n",
      "      num_dst_nodes={'author': 3263, 'field_of_study': 0, 'institution': 0, 'paper': 3748},\n",
      "      num_edges={('author', 'affiliated_with', 'institution'): 0, ('author', 'writes', 'paper'): 12154, ('paper', 'cites', 'paper'): 8975, ('paper', 'has_topic', 'field_of_study'): 0},\n",
      "      metagraph=[('author', 'institution', 'affiliated_with'), ('author', 'paper', 'writes'), ('paper', 'paper', 'cites'), ('paper', 'field_of_study', 'has_topic')])\n",
      "\n",
      "Block(num_src_nodes={'author': 3263, 'field_of_study': 0, 'institution': 0, 'paper': 3748},\n",
      "      num_dst_nodes={'author': 0, 'field_of_study': 0, 'institution': 0, 'paper': 1024},\n",
      "      num_edges={('author', 'affiliated_with', 'institution'): 0, ('author', 'writes', 'paper'): 3275, ('paper', 'cites', 'paper'): 2734, ('paper', 'has_topic', 'field_of_study'): 0},\n",
      "      metagraph=[('author', 'institution', 'affiliated_with'), ('author', 'paper', 'writes'), ('paper', 'paper', 'cites'), ('paper', 'field_of_study', 'has_topic')])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for block in bipartites:\n",
    "    print(block)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model\n",
    "\n",
    "The model can be written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn as dglnn\n",
    "\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes, n_layers, rel_names):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(in_feats, n_hidden)\n",
    "            for rel in rel_names}, aggregate='sum'))\n",
    "        \n",
    "        for i in range(1, n_layers - 1):\n",
    "            self.layers.append(dglnn.HeteroGraphConv({\n",
    "                rel: dglnn.GraphConv(n_hidden, n_hidden)\n",
    "                for rel in rel_names}, aggregate='sum'))\n",
    "            \n",
    "        self.layers.append(dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(n_hidden, n_classes)\n",
    "            for rel in rel_names}, aggregate='sum'))\n",
    "\n",
    "    def forward(self, bipartites, x):\n",
    "        # inputs are features of nodes\n",
    "        for l, (layer, bipartite) in enumerate(zip(self.layers, bipartites)):\n",
    "            x = layer(bipartite, x)\n",
    "            if l != self.n_layers - 1:\n",
    "                x = {k: F.relu(v) for k, v in x.items()}\n",
    "        return x\n",
    "    \n",
    "\n",
    "class NodeEmbed(nn.Module):\n",
    "    def __init__(self, num_nodes, embed_size,):\n",
    "        super(NodeEmbed, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.node_embeds = nn.ModuleDict()\n",
    "        for ntype in num_nodes:\n",
    "            node_embed = torch.nn.Embedding(num_nodes[ntype], self.embed_size)\n",
    "            nn.init.uniform_(node_embed.weight, -1.0, 1.0)\n",
    "            self.node_embeds[str(ntype)] = node_embed\n",
    "    \n",
    "    def forward(self, node_ids):\n",
    "        embeds = {}\n",
    "        for ntype in node_ids:\n",
    "            embeds[ntype] = self.node_embeds[ntype](node_ids[ntype])\n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that here we are iterating over the pairs of NN module layer and bipartite graphs generated by the data loader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Training Loop\n",
    "\n",
    "The following initializes the model and defines the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = {ntype: graph.number_of_nodes(ntype) for ntype in graph.ntypes if ntype != 'paper'}\n",
    "embed = NodeEmbed(num_nodes, 128)\n",
    "model = RGCN(num_features, 128, num_classes, 3, graph.etypes).cuda()\n",
    "opt = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeEmbed(\n",
       "  (node_embeds): ModuleDict(\n",
       "    (author): Embedding(1134649, 128)\n",
       "    (field_of_study): Embedding(59965, 128)\n",
       "    (institution): Embedding(8740, 128)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RGCN(\n",
       "  (layers): ModuleList(\n",
       "    (0): HeteroGraphConv(\n",
       "      (mods): ModuleDict(\n",
       "        (affiliated_with): GraphConv(in=128, out=128, normalization=both, activation=None)\n",
       "        (cites): GraphConv(in=128, out=128, normalization=both, activation=None)\n",
       "        (has_topic): GraphConv(in=128, out=128, normalization=both, activation=None)\n",
       "        (writes): GraphConv(in=128, out=128, normalization=both, activation=None)\n",
       "      )\n",
       "    )\n",
       "    (1): HeteroGraphConv(\n",
       "      (mods): ModuleDict(\n",
       "        (affiliated_with): GraphConv(in=128, out=128, normalization=both, activation=None)\n",
       "        (cites): GraphConv(in=128, out=128, normalization=both, activation=None)\n",
       "        (has_topic): GraphConv(in=128, out=128, normalization=both, activation=None)\n",
       "        (writes): GraphConv(in=128, out=128, normalization=both, activation=None)\n",
       "      )\n",
       "    )\n",
       "    (2): HeteroGraphConv(\n",
       "      (mods): ModuleDict(\n",
       "        (affiliated_with): GraphConv(in=128, out=349, normalization=both, activation=None)\n",
       "        (cites): GraphConv(in=128, out=349, normalization=both, activation=None)\n",
       "        (has_topic): GraphConv(in=128, out=349, normalization=both, activation=None)\n",
       "        (writes): GraphConv(in=128, out=349, normalization=both, activation=None)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When computing the validation score for model selection, usually you can also do neighbor sampling.  To do that, you need to define another data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, valid_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a training loop that performs validation every epoch.  It also saves the model with the best validation accuracy into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [01:42<00:00,  6.01it/s, loss=3.945, acc=0.157]\n",
      "100%|██████████| 64/64 [00:02<00:00, 27.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.08822577413338677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model_path = 'model.pt'\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    \n",
    "    with tqdm.tqdm(train_dataloader) as tq:\n",
    "        for step, (input_nodes, output_nodes, bipartites) in enumerate(tq):\n",
    "            bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "            \n",
    "            # Get node ids for node types that don't have input features\n",
    "            nodes_to_embed = {ntype: node_ids for ntype, node_ids in input_nodes.items() if ntype != \"paper\"}\n",
    "            \n",
    "            # Get node embeddings for node types that don't have input features and copy to gpu\n",
    "            embeddings = {ntype: node_embedding.cuda() for ntype, node_embedding in embed(nodes_to_embed).items()}\n",
    "            \n",
    "            # Get input features for node type 'paper' which has input features\n",
    "            inputs = {'paper': node_features[input_nodes['paper']].cuda()}\n",
    "            \n",
    "            # Merge feature inputs with input that has features\n",
    "            inputs.update(embeddings)\n",
    "            \n",
    "            labels = node_labels[output_nodes['paper']].cuda()\n",
    "            predictions = model(bipartites, inputs)['paper']\n",
    "\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "            \n",
    "            tq.set_postfix({'loss': '%.03f' % loss.item(), 'acc': '%.03f' % accuracy}, refresh=False)\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with tqdm.tqdm(valid_dataloader) as tq, torch.no_grad():\n",
    "        for input_nodes, output_nodes, bipartites in tq:\n",
    "            bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "            \n",
    "            nodes_to_embed = {ntype: node_ids for ntype, node_ids in input_nodes.items() if ntype != \"paper\"}\n",
    "            embeddings = {ntype: node_embedding.cuda() for ntype, node_embedding in embed(nodes_to_embed).items()}\n",
    "            inputs = {'paper': node_features[input_nodes['paper']].cuda()}\n",
    "            inputs.update(embeddings)\n",
    "            \n",
    "            labels.append(node_labels[output_nodes['paper']].numpy())\n",
    "            predictions.append(model(bipartites, inputs)['paper'].argmax(1).cpu().numpy())\n",
    "        predictions = np.concatenate(predictions)\n",
    "        labels = np.concatenate(labels)\n",
    "        accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "        print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
    "        if best_accuracy < accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Inference without Neighbor Sampling\n",
    "\n",
    "Usually for offline inference it is desirable to aggregate over the entire neighborhood to eliminate randomness introduced by neighbor sampling.  However, using the same methodology in training is not efficient, because there will be a lot of redundant computation.  Moreover, simply doing neighbor sampling by taking all neighbors will often exhaust GPU memory because the number of nodes required for input features may be too large to fit into GPU memory.\n",
    "\n",
    "Instead, you need to compute the representations layer by layer: you first compute the output of the first GNN layer for all nodes, then you compute the output of second GNN layer for all nodes using the first GNN layer's output as input, etc.  This gives us a different algorithm from what is being used in training.  During training we have an outer loop that iterates over the nodes, and an inner loop that iterates over the layers.  In contrast, during inference we have an outer loop that iterates over the layers, and an inner loop that iterates over the nodes.\n",
    "\n",
    "If you do not care about randomness too much (e.g., during model selection in validation), you can still use the `dgl.dataloading.MultiLayerNeighborSampler` and `dgl.dataloading.NodeDataLoader` to do offline inference, since it is usually faster for evaluating a small number of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](assets/anim.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, graph, input_features, batch_size):\n",
    "    nodes = {'paper': torch.arange(graph.number_of_nodes('paper'))}\n",
    "    \n",
    "    sampler = dgl.dataloading.MultiLayerNeighborSampler([None])  # one layer at a time, taking all neighbors\n",
    "    dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        graph, nodes, sampler,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for l, layer in enumerate(model.layers):\n",
    "            # Allocate a buffer of output representations for every node\n",
    "            # Note that the buffer is on CPU memory.\n",
    "            output_features = {ntype: torch.zeros(\n",
    "                graph.number_of_nodes(ntype), model.n_hidden if l != model.n_layers - 1 else model.n_classes)\n",
    "                for ntype in graph.ntypes}\n",
    "\n",
    "            for input_nodes, output_nodes, bipartites in tqdm.tqdm(dataloader):\n",
    "                bipartite = bipartites[0].to(torch.device('cuda'))\n",
    "\n",
    "                # \n",
    "                x = {ntype: input_features[ntype][input_nodes[ntype]].cuda() for ntype in input_nodes}\n",
    "\n",
    "                # the following code is identical to the loop body in model.forward()\n",
    "                x = layer(bipartite, x)\n",
    "                if l != model.n_layers - 1:\n",
    "                    x = {k: F.relu(v) for k, v in x.items()}\n",
    "                \n",
    "                for ntype in x:\n",
    "                    output_features[ntype][output_nodes[ntype]] = x[ntype].cpu()\n",
    "            input_features = output_features\n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads the best model from the file saved previously and performs offline inference.  It computes the accuracy on the test set afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:07<00:00, 12.12it/s]\n",
      "100%|██████████| 90/90 [00:07<00:00, 12.06it/s]\n",
      "100%|██████████| 90/90 [00:07<00:00, 11.90it/s]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "nodes_to_embed = {ntype: torch.arange(num_nodes_ntype) for ntype, num_nodes_ntype in num_nodes.items()}\n",
    "embeddings = {ntype: node_embedding for ntype, node_embedding in embed(nodes_to_embed).items()}\n",
    "inputs = {'paper': node_features}\n",
    "inputs.update(embeddings)\n",
    "\n",
    "all_predictions = inference(model, graph, inputs, 8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.03962898495433844\n"
     ]
    }
   ],
   "source": [
    "test_predictions = all_predictions['paper'][test_nids['paper']].argmax(1)\n",
    "test_labels = node_labels[test_nids['paper']]\n",
    "test_accuracy = sklearn.metrics.accuracy_score(test_predictions.numpy(), test_labels.numpy())\n",
    "print('Test accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you have learned how to train a multi-layer RGCN with neighbor sampling on a large heterogeneous dataset that cannot fit into a single GPU.  The method you have learned can scale to a graph of any size, and works on a single machine with a single GPU.\n",
    "\n",
    "## What's next?\n",
    "\n",
    "The next tutorial will be about training the same GraphSAGE model in an unsupervised manner with link prediction, i.e. predicting whether an edge exist between two nodes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}