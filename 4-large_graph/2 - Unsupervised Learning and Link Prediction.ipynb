{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stochastic Training of GNN for Link Prediction on Large Graphs\n",
    "\n",
    "*Note: this tutorial requires a GPU enabled machine*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This tutorial will show how to train a multi-layer GraphSAGE in an unsupervised learning setting. \n",
    "\n",
    "The task is to perform link prediction on Amazon Copurchase Network provided by OGB which contains 2.4 million nodes and 61 million edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "At the end of this tutorial you will be able to:\n",
    "\n",
    "* Train a GNN model on a graph of any size using a single machine with a GPU for link prediction\n",
    "* Train a GNN model for unsupervised learning to learn latent node embeddings that can be reused downstream.\n",
    "\n",
    "This tutorial is based on the data downloaded in the previous tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Link Prediction Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The objective of link prediction is to predict whether an edge exists between two given nodes. \n",
    "\n",
    "We often formulate the problem as predicting a score $s_{uv} = \\phi(\\boldsymbol{h}^{(l)}_u, \\boldsymbol{h}^{(l)}_v)$ indicating the likelihood of an edge existing between two nodes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We train the model via *negative sampling*, i.e. comparing the score of a real edge against that of a \"non-existent\" or arbitrary edge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A common loss function is negative log-likelihood.\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\log \\sigma\\left(s_{uv}\\right) - Q \\mathbb{E}_{v^- \\in P^-(v)}\\left[ \\sigma\\left(-s_{uv^-}\\right) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "You can also use other loss functions such as BPR or margin loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview of Unsupervised Learning with GNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Link prediction itself is already useful in various tasks such as recommendation where you will predict whether a node will interact with another node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is also useful in an unsupervised learning setting as a pre-training task to learn a latent representation of all the nodes.\n",
    "\n",
    "The learned representations could be used for downstream tasks such as \n",
    "* nearest neighbor search\n",
    "* node features for training a downstream classifier.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The objective function can also be combined together with supervised cross-entropy loss for node classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load Dataset\n",
    "\n",
    "We directly load the dataset preprocessed by the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "import utils\n",
    "import pickle\n",
    "\n",
    "with open('data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "graph, node_features, node_labels, train_nids, valid_nids, test_nids = data\n",
    "graph.create_formats_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Edge Data Loader with Neighbor Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For link prediction, we need to \n",
    "* iterate over batch of edges and perform neighbor sampling\n",
    "* sample negative edges to assign negative scores for the loss\n",
    "\n",
    "before computing the output representation of incident nodes of all sampled edges using the GNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "DGL provides `EdgeDataLoader` which allows you to iterate over edges in batches for edge classification or link prediction tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Negative Sampler\n",
    "\n",
    "To train the link prediction model, you need to provide a negative sampler to generate negative edges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For homogeneous graphs, the negative sampler can be any callable that has the following signature:\n",
    "\n",
    "```python\n",
    "def negative_sampler(g: DGLGraph, eids: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "    pass\n",
    "```\n",
    "\n",
    "The first argument is the original graph and the second argument is the minibatch of edge IDs.  The function returns a pair of $u$-$v^-$ node ID tensors as negative examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following code implements a negative sampler that find non-existent edges by sampling `k` $v^-$ for each $u$ according to a distribution $P^-(v) \\propto d(v)^{0.75}$, where $d(v)$ is the degree of $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NegativeSampler(object):\n",
    "    def __init__(self, g, k):\n",
    "        self.k = k\n",
    "        self.weights = g.in_degrees().float() ** 0.75\n",
    "    def __call__(self, g, eids):\n",
    "        src, _ = g.find_edges(eids)\n",
    "        src = src.repeat_interleave(self.k)\n",
    "        dst = self.weights.multinomial(len(src), replacement=True)\n",
    "        return src, dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After defining the negative sampler, you can then construct the `EdgeDataLoader` with neighbor sampling.  \n",
    "\n",
    "Here we will generate 5 negative examples per positive example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4, 4])\n",
    "k = 5\n",
    "train_dataloader = dgl.dataloading.EdgeDataLoader(\n",
    "    graph, torch.arange(graph.number_of_edges()), sampler,\n",
    "    negative_sampler=NegativeSampler(graph, k),\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can peek at one minibatch from `train_dataloader` and see what it gives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([171627, 102391,  59512,  ..., 836611, 698357, 143544]), Graph(num_nodes=7147, num_edges=1024,\n",
      "      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}), Graph(num_nodes=7147, num_edges=5120,\n",
      "      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={}), [Block(num_src_nodes=229934, num_dst_nodes=113400, num_edges=416918), Block(num_src_nodes=113400, num_dst_nodes=33403, num_edges=126876), Block(num_src_nodes=33403, num_dst_nodes=7147, num_edges=28073)])\n"
     ]
    }
   ],
   "source": [
    "example_minibatch = next(iter(train_dataloader))\n",
    "print(example_minibatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The example minibatch consists of four elements.\n",
    "\n",
    "* The input node list necessary for computing the representation of output nodes.\n",
    "* The subgraph induced by the nodes being sampled in the minibatch (including those in the negative examples) as well as the edges sampled in the minibatch.\n",
    "* The subgraph induced by the nodes being sampled in the minibatch (including those in the negative examples) as well as the non-existent edges sampled by the negative sampler.\n",
    "* The list of *bipartite graphs*, one for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input nodes: 229934\n",
      "Positive graph # nodes: 7147 # edges: 1024\n",
      "Negative graph # noeds: 7147 # edges: 5120\n"
     ]
    }
   ],
   "source": [
    "input_nodes, pos_graph, neg_graph, bipartites = example_minibatch\n",
    "print('Number of input nodes:', len(input_nodes))\n",
    "print('Positive graph # nodes:', pos_graph.number_of_nodes(), '# edges:', pos_graph.number_of_edges())\n",
    "print('Negative graph # noeds:', neg_graph.number_of_nodes(), '# edges:', neg_graph.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining Model for Node Representation\n",
    "\n",
    "We are using the GraphSage GNN Model defined as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn as dglnn\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(dglnn.SAGEConv(in_feats, n_hidden, 'mean'))\n",
    "        for i in range(1, n_layers):\n",
    "            self.layers.append(dglnn.SAGEConv(n_hidden, n_hidden, 'mean'))\n",
    "        \n",
    "    def forward(self, bipartites, x):\n",
    "        for l, (layer, bipartite) in enumerate(zip(self.layers, bipartites)):\n",
    "            x = layer(bipartite, x)\n",
    "            if l != self.n_layers - 1:\n",
    "                x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Obtaining Node Representations from the GNN\n",
    "\n",
    "We reuse the same function from the previous tutorial for computing the node representation output from a GNN under an unsupervised learning setting as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def inference(model, graph, input_features, batch_size):\n",
    "    nodes = torch.arange(graph.number_of_nodes())\n",
    "    \n",
    "    sampler = dgl.dataloading.MultiLayerNeighborSampler([None])  # one layer at a time, taking all neighbors\n",
    "    dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        graph, nodes, sampler,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for l, layer in enumerate(model.layers):\n",
    "            # Allocate a buffer of output representations for every node\n",
    "            # Note that the buffer is on CPU memory.\n",
    "            output_features = torch.zeros(graph.number_of_nodes(), model.n_hidden)\n",
    "\n",
    "            for input_nodes, output_nodes, bipartites in tqdm.tqdm(dataloader):\n",
    "                bipartite = bipartites[0].to(torch.device('cuda'))\n",
    "\n",
    "                x = input_features[input_nodes].cuda()\n",
    "\n",
    "                # the following code is identical to the loop body in model.forward()\n",
    "                x = layer(bipartite, x)\n",
    "                if l != model.n_layers - 1:\n",
    "                    x = F.relu(x)\n",
    "\n",
    "                output_features[output_nodes] = x.cpu()\n",
    "            input_features = output_features\n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Define the Score Predictor for Edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To compute the score for a potential edge we take the dot product of the representations of its incident nodes.  \n",
    "\n",
    "This can be easily accomplished with `apply_edges` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class ScorePredictor(nn.Module):\n",
    "    def forward(self, subgraph, x):\n",
    "        with subgraph.local_scope():\n",
    "            subgraph.ndata['x'] = x\n",
    "            subgraph.apply_edges(dgl.function.u_dot_v('x', 'x', 'score'))\n",
    "            return subgraph.edata['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluate Performance of the Learned Embedding\n",
    "\n",
    "In this tutorial we will evaluate the performance of the output embedding by \n",
    "\n",
    "* training a linear classifier with the output node embedding as input features on the training set nodes\n",
    "* compute the accuracy on the validation/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "def evaluate(emb, label):\n",
    "    classifier = sklearn.linear_model.LogisticRegression(solver='lbfgs', multi_class='multinomial', verbose=1, max_iter=1000)\n",
    "    classifier.fit(emb[train_nids], label[train_nids])\n",
    "    valid_pred = classifier.predict(emb[valid_nids])\n",
    "    test_pred = classifier.predict(emb[test_nids])\n",
    "    valid_acc = sklearn.metrics.accuracy_score(label[valid_nids], valid_pred)\n",
    "    test_acc = sklearn.metrics.accuracy_score(label[test_nids], test_pred)\n",
    "    return valid_acc, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following initializes the model and defines the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = SAGE(node_features.shape[1], 128, 3).cuda()\n",
    "predictor = ScorePredictor().cuda()\n",
    "opt = torch.optim.Adam(list(model.parameters()) + list(predictor.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following is the training loop for unsupervised learning and evaluation, and also saves the model that performs the best on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 25/60410 [00:15<3:31:52,  4.75it/s, loss=5.227]   "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import sklearn.metrics\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model_path = 'model.pt'\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    \n",
    "    with tqdm.tqdm(train_dataloader) as tq:\n",
    "        for step, (input_nodes, pos_graph, neg_graph, bipartites) in enumerate(tq):\n",
    "            bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "            pos_graph = pos_graph.to(torch.device('cuda'))\n",
    "            neg_graph = neg_graph.to(torch.device('cuda'))\n",
    "            inputs = node_features[input_nodes].cuda()\n",
    "            outputs = model(bipartites, inputs)\n",
    "            pos_score = predictor(pos_graph, outputs)\n",
    "            neg_score = predictor(neg_graph, outputs)\n",
    "            \n",
    "            score = torch.cat([pos_score, neg_score])\n",
    "            label = torch.cat([torch.ones_like(pos_score), torch.zeros_like(neg_score)])\n",
    "            loss = F.binary_cross_entropy_with_logits(score, label)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            tq.set_postfix({'loss': '%.03f' % loss.item()}, refresh=False)\n",
    "        \n",
    "    model.eval()\n",
    "    emb = inference(model, graph, node_features, 8192)\n",
    "    valid_acc, test_acc = evaluate(emb.numpy(), node_labels.numpy())\n",
    "    print('Epoch {} Validation Accuracy {} Test Accuracy {}'.format(epoch, valid_acc, test_acc))\n",
    "    if best_accuracy < valid_acc:\n",
    "        best_accuracy = valid_acc\n",
    "        torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you have learned how to train a multi-layer GraphSAGE for unsupervised learning via link prediction on a large dataset that cannot fit into GPU.  The method you have learned can scale to a graph of any size, and works on a single machine with a single GPU.\n",
    "\n",
    "## What's next?\n",
    "\n",
    "The next tutorial will be about scaling the training procedure out to multiple GPUs on a single machine."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
