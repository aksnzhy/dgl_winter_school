{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Training of GNN for Node Classification on Large Heterogeneous  Graphs\n",
    "\n",
    "This tutorial shows how to train a multi-layer R-GCN for node classification on `ogbn-mag` dataset provided by OGB.\n",
    "\n",
    "The ogbn-mag dataset is a heterogeneous network composed of a subset of the Microsoft Academic Graph (MAG) [1]. It contains four types of entities—papers (736,389 nodes), authors (1,134,649 nodes), institutions (8,740 nodes), and fields of study (59,965 nodes)—as well as four types of directed relations connecting two types of entities—an author is “affiliated with” an institution, an author “writes” a paper, a paper “cites” a paper, and a paper “has a topic of” a field of study.\n",
    "\n",
    "This tutorial's contents include\n",
    "\n",
    "* Creating a DGL graph using the dgl ogb data loader.\n",
    "* Training a GNN model with a single machine, a single GPU, on a graph of any size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Although you can directly use the Python package provided by OGB, for demonstration, we will instead manually download the dataset, peek into its contents, and process it with only `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ogb -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "dataset = DglNodePropPredDataset(name='ogbn-mag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following:\n",
    "\n",
    "* DGL graph object (source-destination pairs)\n",
    "* The node label tensor\n",
    "\n",
    "We can also use the utility function in the dataset to get the train, validation, test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "\n",
    "graph, label = dataset[0] # graph: dgl graph object, label: torch tensor of shape (num_nodes, 1)\n",
    "\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_nids, valid_nids, test_nids = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the size of the graph, features, and labels as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes={'author': 1134649, 'field_of_study': 59965, 'institution': 8740, 'paper': 736389},\n",
      "      num_edges={('author', 'affiliated_with', 'institution'): 1043998, ('author', 'writes', 'paper'): 7145660, ('paper', 'cites', 'paper'): 5416271, ('paper', 'has_topic', 'field_of_study'): 7505078},\n",
      "      metagraph=[('author', 'institution', 'affiliated_with'), ('author', 'paper', 'writes'), ('paper', 'paper', 'cites'), ('paper', 'field_of_study', 'has_topic')])\n",
      "Node labels\n",
      "Shape of target node labels: torch.Size([736389])\n",
      "Number of classes: 349\n",
      "Node features\n",
      "Shape of features of paper node type: 128\n"
     ]
    }
   ],
   "source": [
    "print(graph)\n",
    "\n",
    "print('Node labels')\n",
    "node_labels = label['paper'].flatten()\n",
    "print('Shape of target node labels:', node_labels.shape)\n",
    "num_classes = (node_labels.max() + 1).item()\n",
    "print('Number of classes:', num_classes)\n",
    "\n",
    "print('Node features')\n",
    "node_features = graph.nodes['paper'].data['feat']\n",
    "num_features = node_features.shape[1]\n",
    "print('Shape of features of paper node type: {}'.format(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_writes, dst_writes = graph.all_edges(etype=\"writes\")\n",
    "src_topic, dst_topic = graph.all_edges(etype=\"has_topic\")\n",
    "src_aff, dst_aff = graph.all_edges(etype=\"affiliated_with\")\n",
    "\n",
    "\n",
    "graph = dgl.heterograph({\n",
    "    (\"author\", \"writes\", \"paper\"): (src_writes, dst_writes),\n",
    "    (\"paper\", \"has_topic\", \"field_of_study\"): (src_topic, dst_topic),\n",
    "    (\"author\", \"affiliated_with\", \"institution\"): (src_aff, dst_aff),\n",
    "    (\"paper\", \"writes-rev\", \"author\"): (dst_writes, src_writes),\n",
    "    (\"field_of_study\", \"has_topic-rev\", \"paper\"): (dst_topic, src_topic),\n",
    "    (\"institution\", \"affiliated_with-rev\", \"author\"): (dst_aff, src_aff),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining neighbor sampler and data loader in DGL\n",
    "\n",
    "DGL provides useful tools to generate such computation dependencies while iterating over the dataset in minibatches.  For node classification, you can use `dgl.dataloading.NodeDataLoader` for iterating over the dataset, and `dgl.dataloading.MultiLayerNeighborSampler` to generate computation dependencies of the nodes from a multi-layer GNN with neighbor sampling.\n",
    "\n",
    "The syntax of `dgl.dataloading.NodeDataLoader` is mostly similar to a PyTorch `DataLoader`, with the addition that it needs a graph to generate computation dependency from, a set of node IDs to iterate on, and the neighbor sampler you defined.\n",
    "\n",
    "Let's consider training a 2-layer R-GCN with neighbor sampling, and each node will gather message from 15 neighbors on each layer.  The code defining the data loader and neighbor sampler will look like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "\n",
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([15, 15])\n",
    "train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, train_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate over the data loader we created and see what it gives us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'author': tensor([   711, 459442,  31044,  ..., 581587, 396341,  48262]), 'field_of_study': tensor([10471, 10977, 13664,  ...,  5120,  7796, 40196]), 'institution': tensor([1884, 4668, 6897,  ..., 3792, 3663, 5924]), 'paper': tensor([ 70408, 110589,   2339,  ...,  80321, 500612, 602814])}, {'author': tensor([], dtype=torch.int64), 'field_of_study': tensor([], dtype=torch.int64), 'institution': tensor([], dtype=torch.int64), 'paper': tensor([ 70408, 110589,   2339,  ..., 562045, 597263, 410335])}, [Block(num_src_nodes={'author': 4778, 'field_of_study': 3556, 'institution': 1312, 'paper': 74682},\n",
      "      num_dst_nodes={'author': 4582, 'field_of_study': 3556, 'institution': 0, 'paper': 1024},\n",
      "      num_edges={('author', 'affiliated_with', 'institution'): 0, ('author', 'writes', 'paper'): 4639, ('field_of_study', 'has_topic-rev', 'paper'): 10546, ('institution', 'affiliated_with-rev', 'author'): 6600, ('paper', 'has_topic', 'field_of_study'): 51627, ('paper', 'writes-rev', 'author'): 39915},\n",
      "      metagraph=[('author', 'institution', 'affiliated_with'), ('author', 'paper', 'writes'), ('institution', 'author', 'affiliated_with-rev'), ('paper', 'field_of_study', 'has_topic'), ('paper', 'author', 'writes-rev'), ('field_of_study', 'paper', 'has_topic-rev')]), Block(num_src_nodes={'author': 4582, 'field_of_study': 3556, 'institution': 0, 'paper': 1024},\n",
      "      num_dst_nodes={'author': 0, 'field_of_study': 0, 'institution': 0, 'paper': 1024},\n",
      "      num_edges={('author', 'affiliated_with', 'institution'): 0, ('author', 'writes', 'paper'): 4639, ('field_of_study', 'has_topic-rev', 'paper'): 10546, ('institution', 'affiliated_with-rev', 'author'): 0, ('paper', 'has_topic', 'field_of_study'): 0, ('paper', 'writes-rev', 'author'): 0},\n",
      "      metagraph=[('author', 'institution', 'affiliated_with'), ('author', 'paper', 'writes'), ('institution', 'author', 'affiliated_with-rev'), ('paper', 'field_of_study', 'has_topic'), ('paper', 'author', 'writes-rev'), ('field_of_study', 'paper', 'has_topic-rev')])])\n"
     ]
    }
   ],
   "source": [
    "example_minibatch = next(iter(train_dataloader))\n",
    "print(example_minibatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NodeDataLoader` gives us three items per iteration.\n",
    "\n",
    "* The input node list for the nodes whose input features are needed to compute the outputs.\n",
    "* The output node list whose GNN representation are to be computed.\n",
    "* The list of computation dependency for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To compute 1024 target nodes' output we need 74682 nodes' input features\n",
      "\n",
      "Output nodes\n",
      "{'author': tensor([], dtype=torch.int64), 'field_of_study': tensor([], dtype=torch.int64), 'institution': tensor([], dtype=torch.int64), 'paper': tensor([ 70408, 110589,   2339,  ..., 562045, 597263, 410335])}\n",
      "\n",
      "Input nodes\n",
      "{'author': tensor([   711, 459442,  31044,  ..., 581587, 396341,  48262]), 'field_of_study': tensor([10471, 10977, 13664,  ...,  5120,  7796, 40196]), 'institution': tensor([1884, 4668, 6897,  ..., 3792, 3663, 5924]), 'paper': tensor([ 70408, 110589,   2339,  ...,  80321, 500612, 602814])}\n"
     ]
    }
   ],
   "source": [
    "input_nodes, output_nodes, bipartites = example_minibatch\n",
    "print(\"To compute {} target nodes' output we need {} nodes' input features\".format(len(output_nodes['paper']), len(input_nodes['paper'])))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Output nodes\")\n",
    "print(output_nodes)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Input nodes\")\n",
    "print(input_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block(num_src_nodes={'author': 4778, 'field_of_study': 3556, 'institution': 1312, 'paper': 74682},\n",
      "      num_dst_nodes={'author': 4582, 'field_of_study': 3556, 'institution': 0, 'paper': 1024},\n",
      "      num_edges={('author', 'affiliated_with', 'institution'): 0, ('author', 'writes', 'paper'): 4639, ('field_of_study', 'has_topic-rev', 'paper'): 10546, ('institution', 'affiliated_with-rev', 'author'): 6600, ('paper', 'has_topic', 'field_of_study'): 51627, ('paper', 'writes-rev', 'author'): 39915},\n",
      "      metagraph=[('author', 'institution', 'affiliated_with'), ('author', 'paper', 'writes'), ('institution', 'author', 'affiliated_with-rev'), ('paper', 'field_of_study', 'has_topic'), ('paper', 'author', 'writes-rev'), ('field_of_study', 'paper', 'has_topic-rev')])\n",
      "\n",
      "Block(num_src_nodes={'author': 4582, 'field_of_study': 3556, 'institution': 0, 'paper': 1024},\n",
      "      num_dst_nodes={'author': 0, 'field_of_study': 0, 'institution': 0, 'paper': 1024},\n",
      "      num_edges={('author', 'affiliated_with', 'institution'): 0, ('author', 'writes', 'paper'): 4639, ('field_of_study', 'has_topic-rev', 'paper'): 10546, ('institution', 'affiliated_with-rev', 'author'): 0, ('paper', 'has_topic', 'field_of_study'): 0, ('paper', 'writes-rev', 'author'): 0},\n",
      "      metagraph=[('author', 'institution', 'affiliated_with'), ('author', 'paper', 'writes'), ('institution', 'author', 'affiliated_with-rev'), ('paper', 'field_of_study', 'has_topic'), ('paper', 'author', 'writes-rev'), ('field_of_study', 'paper', 'has_topic-rev')])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for block in bipartites:\n",
    "    print(block)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model\n",
    "\n",
    "The model can be written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn as dglnn\n",
    "\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes, n_layers, rel_names):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(in_feats, n_hidden)\n",
    "            for rel in rel_names}, aggregate='sum'))\n",
    "        \n",
    "        for i in range(1, n_layers - 1):\n",
    "            self.layers.append(dglnn.HeteroGraphConv({\n",
    "                rel: dglnn.GraphConv(n_hidden, n_hidden)\n",
    "                for rel in rel_names}, aggregate='sum'))\n",
    "            \n",
    "        self.layers.append(dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(n_hidden, n_classes)\n",
    "            for rel in rel_names}, aggregate='sum'))\n",
    "\n",
    "    def forward(self, bipartites, x):\n",
    "        # inputs are features of nodes\n",
    "        for l, (layer, bipartite) in enumerate(zip(self.layers, bipartites)):\n",
    "            x = layer(bipartite, x)\n",
    "            if l != self.n_layers - 1:\n",
    "                x = {k: F.relu(v) for k, v in x.items()}\n",
    "        return x\n",
    "    \n",
    "\n",
    "class NodeEmbed(nn.Module):\n",
    "    def __init__(self, num_nodes, embed_size,):\n",
    "        super(NodeEmbed, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.node_embeds = nn.ModuleDict()\n",
    "        for ntype in num_nodes:\n",
    "            node_embed = torch.nn.Embedding(num_nodes[ntype], self.embed_size)\n",
    "            nn.init.uniform_(node_embed.weight, -1.0, 1.0)\n",
    "            self.node_embeds[str(ntype)] = node_embed\n",
    "    \n",
    "    def forward(self, node_ids):\n",
    "        embeds = {}\n",
    "        for ntype in node_ids:\n",
    "            embeds[ntype] = self.node_embeds[ntype](node_ids[ntype])\n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that here we are iterating over the pairs of NN module layer and bipartite graphs generated by the data loader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Training Loop\n",
    "\n",
    "The following initializes the model and defines the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = {ntype: graph.number_of_nodes(ntype) for ntype in graph.ntypes if ntype != 'paper'}\n",
    "num_layers = 2\n",
    "hidden_dim = 128\n",
    "embed = NodeEmbed(num_nodes, hidden_dim)\n",
    "model = RGCN(num_features, hidden_dim, num_classes, num_layers, graph.etypes).cuda()\n",
    "all_params = list(model.parameters()) + list(embed.parameters())\n",
    "opt = torch.optim.Adam(all_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeEmbed(\n",
       "  (node_embeds): ModuleDict(\n",
       "    (author): Embedding(1134649, 128)\n",
       "    (field_of_study): Embedding(59965, 128)\n",
       "    (institution): Embedding(8740, 128)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RGCN(\n",
       "  (layers): ModuleList(\n",
       "    (0): HeteroGraphConv(\n",
       "      (mods): ModuleDict(\n",
       "        (affiliated_with): GraphConv(in=128, out=128, normalization=both, activation=None)\n",
       "        (affiliated_with-rev): GraphConv(in=128, out=128, normalization=both, activation=None)\n",
       "        (has_topic): GraphConv(in=128, out=128, normalization=both, activation=None)\n",
       "        (has_topic-rev): GraphConv(in=128, out=128, normalization=both, activation=None)\n",
       "        (writes): GraphConv(in=128, out=128, normalization=both, activation=None)\n",
       "        (writes-rev): GraphConv(in=128, out=128, normalization=both, activation=None)\n",
       "      )\n",
       "    )\n",
       "    (1): HeteroGraphConv(\n",
       "      (mods): ModuleDict(\n",
       "        (affiliated_with): GraphConv(in=128, out=349, normalization=both, activation=None)\n",
       "        (affiliated_with-rev): GraphConv(in=128, out=349, normalization=both, activation=None)\n",
       "        (has_topic): GraphConv(in=128, out=349, normalization=both, activation=None)\n",
       "        (has_topic-rev): GraphConv(in=128, out=349, normalization=both, activation=None)\n",
       "        (writes): GraphConv(in=128, out=349, normalization=both, activation=None)\n",
       "        (writes-rev): GraphConv(in=128, out=349, normalization=both, activation=None)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When computing the validation score for model selection, usually you can also do neighbor sampling.  To do that, you need to define another data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, valid_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a training loop that performs validation every epoch.  It also saves the model with the best validation accuracy into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:56<00:00, 10.84it/s, loss=2.011, acc=0.461]\n",
      "100%|██████████| 64/64 [00:04<00:00, 13.11it/s]\n",
      "  0%|          | 1/615 [00:00<01:21,  7.55it/s, loss=2.092, acc=0.438]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.3604864439957459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:55<00:00, 11.09it/s, loss=1.968, acc=0.460]\n",
      "100%|██████████| 64/64 [00:04<00:00, 13.31it/s]\n",
      "  0%|          | 1/615 [00:00<01:19,  7.73it/s, loss=2.025, acc=0.426]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Accuracy 0.36361534548929547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:53<00:00, 11.54it/s, loss=1.952, acc=0.446]\n",
      "100%|██████████| 64/64 [00:04<00:00, 13.38it/s]\n",
      "  0%|          | 1/615 [00:00<01:20,  7.65it/s, loss=1.937, acc=0.453]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Accuracy 0.359993218144546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:53<00:00, 11.44it/s, loss=2.099, acc=0.414]\n",
      "100%|██████████| 64/64 [00:04<00:00, 13.15it/s]\n",
      "  0%|          | 1/615 [00:00<01:19,  7.73it/s, loss=2.009, acc=0.436]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Accuracy 0.3551071995560967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:53<00:00, 11.42it/s, loss=2.056, acc=0.429]\n",
      "100%|██████████| 64/64 [00:04<00:00, 13.28it/s]\n",
      "  0%|          | 1/615 [00:00<01:20,  7.66it/s, loss=1.944, acc=0.452]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Accuracy 0.36632808767089503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:53<00:00, 11.42it/s, loss=1.941, acc=0.440]\n",
      "100%|██████████| 64/64 [00:04<00:00, 12.91it/s]\n",
      "  0%|          | 1/615 [00:00<01:21,  7.52it/s, loss=1.946, acc=0.443]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Accuracy 0.36259806717119564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:54<00:00, 11.37it/s, loss=1.988, acc=0.466]\n",
      "100%|██████████| 64/64 [00:04<00:00, 13.03it/s]\n",
      "  0%|          | 1/615 [00:00<01:44,  5.90it/s, loss=1.916, acc=0.440]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation Accuracy 0.3612879360039458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:54<00:00, 11.22it/s, loss=1.996, acc=0.410]\n",
      "100%|██████████| 64/64 [00:04<00:00, 13.49it/s]\n",
      "  0%|          | 1/615 [00:00<01:18,  7.86it/s, loss=1.897, acc=0.474]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation Accuracy 0.3614112424667458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:52<00:00, 11.65it/s, loss=1.945, acc=0.454]\n",
      "100%|██████████| 64/64 [00:04<00:00, 13.41it/s]\n",
      "  0%|          | 1/615 [00:00<01:18,  7.84it/s, loss=1.898, acc=0.443]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Validation Accuracy 0.3627984401732456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:52<00:00, 11.62it/s, loss=2.026, acc=0.453]\n",
      "100%|██████████| 64/64 [00:04<00:00, 13.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Validation Accuracy 0.3630758797145455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model_path = 'model.pt'\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    \n",
    "    with tqdm.tqdm(train_dataloader) as tq:\n",
    "        for step, (input_nodes, output_nodes, bipartites) in enumerate(tq):\n",
    "            bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "            \n",
    "            # Get node ids for node types that don't have input features\n",
    "            nodes_to_embed = {ntype: node_ids for ntype, node_ids in input_nodes.items() if ntype != \"paper\"}\n",
    "            \n",
    "            # Get node embeddings for node types that don't have input features and copy to gpu\n",
    "            embeddings = {ntype: node_embedding.cuda() for ntype, node_embedding in embed(nodes_to_embed).items()}\n",
    "            \n",
    "            # Get input features for node type 'paper' which has input features\n",
    "            inputs = {'paper': node_features[input_nodes['paper']].cuda()}\n",
    "            \n",
    "            # Merge feature inputs with input that has features\n",
    "            inputs.update(embeddings)\n",
    "            \n",
    "            labels = node_labels[output_nodes['paper']].cuda()\n",
    "            predictions = model(bipartites, inputs)['paper']\n",
    "\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "            \n",
    "            tq.set_postfix({'loss': '%.03f' % loss.item(), 'acc': '%.03f' % accuracy}, refresh=False)\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with tqdm.tqdm(valid_dataloader) as tq, torch.no_grad():\n",
    "        for input_nodes, output_nodes, bipartites in tq:\n",
    "            bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "            \n",
    "            nodes_to_embed = {ntype: node_ids for ntype, node_ids in input_nodes.items() if ntype != \"paper\"}\n",
    "            embeddings = {ntype: node_embedding.cuda() for ntype, node_embedding in embed(nodes_to_embed).items()}\n",
    "            inputs = {'paper': node_features[input_nodes['paper']].cuda()}\n",
    "            inputs.update(embeddings)\n",
    "            \n",
    "            labels.append(node_labels[output_nodes['paper']].numpy())\n",
    "            predictions.append(model(bipartites, inputs)['paper'].argmax(1).cpu().numpy())\n",
    "        predictions = np.concatenate(predictions)\n",
    "        labels = np.concatenate(labels)\n",
    "        accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "        print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
    "        if best_accuracy < accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Inference without Neighbor Sampling\n",
    "\n",
    "Usually for offline inference it is desirable to aggregate over the entire neighborhood to eliminate randomness introduced by neighbor sampling.  However, using the same methodology in training is not efficient, because there will be a lot of redundant computation.  Moreover, simply doing neighbor sampling by taking all neighbors will often exhaust GPU memory because the number of nodes required for input features may be too large to fit into GPU memory.\n",
    "\n",
    "Instead, you need to compute the representations layer by layer: you first compute the output of the first GNN layer for all nodes, then you compute the output of second GNN layer for all nodes using the first GNN layer's output as input, etc.  This gives us a different algorithm from what is being used in training.  During training we have an outer loop that iterates over the nodes, and an inner loop that iterates over the layers.  In contrast, during inference we have an outer loop that iterates over the layers, and an inner loop that iterates over the nodes.\n",
    "\n",
    "If you do not care about randomness too much (e.g., during model selection in validation), you can still use the `dgl.dataloading.MultiLayerNeighborSampler` and `dgl.dataloading.NodeDataLoader` to do offline inference, since it is usually faster for evaluating a small number of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, graph, input_features, batch_size):\n",
    "    nodes = {ntype: torch.arange(graph.number_of_nodes(ntype)) for ntype in graph.ntypes}\n",
    "    \n",
    "    sampler = dgl.dataloading.MultiLayerNeighborSampler([None])  # one layer at a time, taking all neighbors\n",
    "    dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        graph, nodes, sampler\n",
    "        ,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for l, layer in enumerate(model.layers):\n",
    "            # Allocate a buffer of output representations for every node\n",
    "            # Note that the buffer is on CPU memory.\n",
    "            output_features = {ntype: torch.zeros(\n",
    "                graph.number_of_nodes(ntype), model.n_hidden if l != model.n_layers - 1 else model.n_classes)\n",
    "                for ntype in graph.ntypes}\n",
    "\n",
    "            for input_nodes, output_nodes, bipartites in tqdm.tqdm(dataloader):\n",
    "                bipartite = bipartites[0].to(torch.device('cuda'))\n",
    "\n",
    "                # send features for nodes in batch to gpu \n",
    "                x = {ntype: input_features[ntype][input_nodes[ntype]].cuda() for ntype in input_nodes}\n",
    "\n",
    "                # the following code is identical to the loop body in model.forward()\n",
    "                x = layer(bipartite, x)\n",
    "                if l != model.n_layers - 1:\n",
    "                    x = {k: F.relu(v) for k, v in x.items()}\n",
    "                \n",
    "                for ntype in x:\n",
    "                    output_features[ntype][output_nodes[ntype]] = x[ntype].cpu()\n",
    "            input_features = output_features\n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads the best model from the file saved previously and performs offline inference.  It computes the accuracy on the test set afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 237/237 [00:22<00:00, 10.53it/s]\n",
      "100%|██████████| 237/237 [00:23<00:00, 10.24it/s]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "nodes_to_embed = {ntype: torch.arange(num_nodes_ntype) for ntype, num_nodes_ntype in num_nodes.items()}\n",
    "embeddings = {ntype: node_embedding for ntype, node_embedding in embed(nodes_to_embed).items()}\n",
    "inputs = {'paper': node_features}\n",
    "inputs.update(embeddings)\n",
    "\n",
    "all_predictions = inference(model, graph, inputs, 8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.3197501132597344\n"
     ]
    }
   ],
   "source": [
    "test_predictions = all_predictions['paper'][test_nids['paper']].argmax(1)\n",
    "test_labels = node_labels[test_nids['paper']]\n",
    "test_accuracy = sklearn.metrics.accuracy_score(test_predictions.numpy(), test_labels.numpy())\n",
    "print('Test accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you have learned how to train a multi-layer RGCN with neighbor sampling on a large heterogeneous dataset.  The method used here works on a single machine with a single GPU.\n",
    "\n",
    "## What's next?\n",
    "\n",
    "The next tutorial will be about scaling the training procedure out to multiple GPUs on a single machine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}