{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stochastic Training of GNN for Node Classification on Large Heterogeneous  Graphs\n",
    "\n",
    "*Note: this tutorial requires a GPU enabled machine*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This tutorial shows how to train a multi-layer R-GCN for node classification on the `ogbn-mag` dataset provided by OGB.\n",
    "\n",
    "The ogbn-mag dataset is a heterogeneous network composed of a subset of the Microsoft Academic Graph (MAG) and has 1.9M nodes and 21M edges. \n",
    "\n",
    "It contains four types of entities: papers, authors, institutions, fields of study \n",
    "\n",
    "as well as four types of relations: author “affiliated with” institution, author “writes” paper, paper “cites” paper, paper “has topic” field of study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "At the end of this tutorial you will be able to\n",
    "\n",
    "* Create a DGL graph using the ogb data loader for dgl.\n",
    "* Train a GNN model for a large heterogeneous graph on a single machine using a single GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Dataset\n",
    "\n",
    "Although you can directly use the Python package provided by OGB, for demonstration, we will instead manually download the dataset, peek into its contents, and process it with only `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install ogb -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "dataset = DglNodePropPredDataset(name='ogbn-mag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The dataset contains the following:\n",
    "\n",
    "* DGL graph object\n",
    "* The node label tensor\n",
    "\n",
    "We can also use the utility function in the dataset to get the train, validation, test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import dgl\n",
    "\n",
    "graph, label = dataset[0] # graph: dgl graph object, label: torch tensor of shape (num_nodes, 1)\n",
    "\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_nids, valid_nids, test_nids = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since the graph is heterogeneous our train_nids is a node dictionary with the node type as key and a list of node ids as the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper': tensor([     0,      1,      2,  ..., 736386, 736387, 736388])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see the size of the graph, features, and labels as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes={'author': 1134649, 'field_of_study': 59965, 'institution': 8740, 'paper': 736389},\n",
      "      num_edges={('author', 'affiliated_with', 'institution'): 1043998, ('author', 'writes', 'paper'): 7145660, ('paper', 'cites', 'paper'): 5416271, ('paper', 'has_topic', 'field_of_study'): 7505078},\n",
      "      metagraph=[('author', 'institution', 'affiliated_with'), ('author', 'paper', 'writes'), ('paper', 'paper', 'cites'), ('paper', 'field_of_study', 'has_topic')])\n",
      "Node labels\n",
      "Shape of target node labels: torch.Size([736389])\n",
      "Number of classes: 349\n",
      "Node features\n",
      "Shape of features of paper node type: 128\n"
     ]
    }
   ],
   "source": [
    "print(graph)\n",
    "\n",
    "print('Node labels')\n",
    "node_labels = label['paper'].flatten()\n",
    "print('Shape of target node labels:', node_labels.shape)\n",
    "num_classes = (node_labels.max() + 1).item()\n",
    "print('Number of classes:', num_classes)\n",
    "\n",
    "print('Node features')\n",
    "node_features = graph.nodes['paper'].data['feat']\n",
    "num_features = node_features.shape[1]\n",
    "print('Shape of features of paper node type: {}'.format(num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Add reverse edges\n",
    "\n",
    "Since the relations have a fixed orientation we add the reverse relation as well to the graph to make the relations undirected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "src_writes, dst_writes = graph.all_edges(etype=\"writes\")\n",
    "src_topic, dst_topic = graph.all_edges(etype=\"has_topic\")\n",
    "src_aff, dst_aff = graph.all_edges(etype=\"affiliated_with\")\n",
    "\n",
    "\n",
    "graph = dgl.heterograph({\n",
    "    (\"author\", \"writes\", \"paper\"): (src_writes, dst_writes),\n",
    "    (\"paper\", \"has_topic\", \"field_of_study\"): (src_topic, dst_topic),\n",
    "    (\"author\", \"affiliated_with\", \"institution\"): (src_aff, dst_aff),\n",
    "    (\"paper\", \"writes-rev\", \"author\"): (dst_writes, src_writes),\n",
    "    (\"field_of_study\", \"has_topic-rev\", \"paper\"): (dst_topic, src_topic),\n",
    "    (\"institution\", \"affiliated_with-rev\", \"author\"): (dst_aff, src_aff),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Note:</b> A DGL heterograph is immutable. To add new edges you have to create a new graph.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Defining neighbor sampler and data loader in DGL\n",
    "\n",
    "For training a 2-layer R-GCN with neighbor sampling, where each node will gather messages from 15 neighbors on each layer, the code defining the data loader and neighbor sampler will look like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import dgl\n",
    "\n",
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([15, 15])\n",
    "train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, train_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can iterate over the data loader we created and see what it gives us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'author': tensor([ 82634, 325839, 449850,  ..., 289852, 245058,  71282]), 'field_of_study': tensor([  343,  4376,  5159,  ...,  9888, 19703, 15510]), 'institution': tensor([ 656, 2017, 4230,  ..., 5547, 8387, 8553]), 'paper': tensor([712657, 447547, 269429,  ..., 631677, 680909, 692204])}, {'author': tensor([], dtype=torch.int64), 'field_of_study': tensor([], dtype=torch.int64), 'institution': tensor([], dtype=torch.int64), 'paper': tensor([712657, 447547, 269429,  ..., 398093, 590450, 562032])}, [Block(num_src_nodes={'author': 4814, 'field_of_study': 3532, 'institution': 1319, 'paper': 75361},\n",
      "      num_dst_nodes={'author': 4657, 'field_of_study': 3532, 'institution': 0, 'paper': 1024},\n",
      "      num_edges={('author', 'affiliated_with', 'institution'): 0, ('author', 'writes', 'paper'): 4724, ('field_of_study', 'has_topic-rev', 'paper'): 10618, ('institution', 'affiliated_with-rev', 'author'): 6972, ('paper', 'has_topic', 'field_of_study'): 51095, ('paper', 'writes-rev', 'author'): 40835},\n",
      "      metagraph=[('author', 'institution', 'affiliated_with'), ('author', 'paper', 'writes'), ('institution', 'author', 'affiliated_with-rev'), ('paper', 'field_of_study', 'has_topic'), ('paper', 'author', 'writes-rev'), ('field_of_study', 'paper', 'has_topic-rev')]), Block(num_src_nodes={'author': 4657, 'field_of_study': 3532, 'institution': 0, 'paper': 1024},\n",
      "      num_dst_nodes={'author': 0, 'field_of_study': 0, 'institution': 0, 'paper': 1024},\n",
      "      num_edges={('author', 'affiliated_with', 'institution'): 0, ('author', 'writes', 'paper'): 4724, ('field_of_study', 'has_topic-rev', 'paper'): 10618, ('institution', 'affiliated_with-rev', 'author'): 0, ('paper', 'has_topic', 'field_of_study'): 0, ('paper', 'writes-rev', 'author'): 0},\n",
      "      metagraph=[('author', 'institution', 'affiliated_with'), ('author', 'paper', 'writes'), ('institution', 'author', 'affiliated_with-rev'), ('paper', 'field_of_study', 'has_topic'), ('paper', 'author', 'writes-rev'), ('field_of_study', 'paper', 'has_topic-rev')])])\n"
     ]
    }
   ],
   "source": [
    "example_minibatch = next(iter(train_dataloader))\n",
    "print(example_minibatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Although it's muddled, the `NodeDataLoader` gives us three items per iteration similar to the one for Homogenous Graph: (input nodes, output nodes, computation dependency for each layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To compute 1024 target nodes' output we need 75361 nodes' input features\n",
      "\n",
      "Output nodes\n",
      "{'author': tensor([], dtype=torch.int64), 'field_of_study': tensor([], dtype=torch.int64), 'institution': tensor([], dtype=torch.int64), 'paper': tensor([712657, 447547, 269429,  ..., 398093, 590450, 562032])}\n",
      "\n",
      "Input nodes\n",
      "{'author': tensor([ 82634, 325839, 449850,  ..., 289852, 245058,  71282]), 'field_of_study': tensor([  343,  4376,  5159,  ...,  9888, 19703, 15510]), 'institution': tensor([ 656, 2017, 4230,  ..., 5547, 8387, 8553]), 'paper': tensor([712657, 447547, 269429,  ..., 631677, 680909, 692204])}\n"
     ]
    }
   ],
   "source": [
    "input_nodes, output_nodes, bipartites = example_minibatch\n",
    "print(\"To compute {} target nodes' output we need {} nodes' input features\".format(len(output_nodes['paper']), len(input_nodes['paper'])))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Output nodes\")\n",
    "print(output_nodes)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Input nodes\")\n",
    "print(input_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block(num_src_nodes={'author': 4814, 'field_of_study': 3532, 'institution': 1319, 'paper': 75361},\n",
      "      num_dst_nodes={'author': 4657, 'field_of_study': 3532, 'institution': 0, 'paper': 1024},\n",
      "      num_edges={('author', 'affiliated_with', 'institution'): 0, ('author', 'writes', 'paper'): 4724, ('field_of_study', 'has_topic-rev', 'paper'): 10618, ('institution', 'affiliated_with-rev', 'author'): 6972, ('paper', 'has_topic', 'field_of_study'): 51095, ('paper', 'writes-rev', 'author'): 40835},\n",
      "      metagraph=[('author', 'institution', 'affiliated_with'), ('author', 'paper', 'writes'), ('institution', 'author', 'affiliated_with-rev'), ('paper', 'field_of_study', 'has_topic'), ('paper', 'author', 'writes-rev'), ('field_of_study', 'paper', 'has_topic-rev')])\n",
      "\n",
      "Block(num_src_nodes={'author': 4657, 'field_of_study': 3532, 'institution': 0, 'paper': 1024},\n",
      "      num_dst_nodes={'author': 0, 'field_of_study': 0, 'institution': 0, 'paper': 1024},\n",
      "      num_edges={('author', 'affiliated_with', 'institution'): 0, ('author', 'writes', 'paper'): 4724, ('field_of_study', 'has_topic-rev', 'paper'): 10618, ('institution', 'affiliated_with-rev', 'author'): 0, ('paper', 'has_topic', 'field_of_study'): 0, ('paper', 'writes-rev', 'author'): 0},\n",
      "      metagraph=[('author', 'institution', 'affiliated_with'), ('author', 'paper', 'writes'), ('institution', 'author', 'affiliated_with-rev'), ('paper', 'field_of_study', 'has_topic'), ('paper', 'author', 'writes-rev'), ('field_of_study', 'paper', 'has_topic-rev')])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for block in bipartites:\n",
    "    print(block)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Defining Model\n",
    "\n",
    "The RGCN model can be written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn as dglnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class RGCN(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes, n_layers, rel_names):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(in_feats, n_hidden)\n",
    "            for rel in rel_names}, aggregate='sum'))\n",
    "        \n",
    "        for i in range(1, n_layers - 1):\n",
    "            self.layers.append(dglnn.HeteroGraphConv({\n",
    "                rel: dglnn.GraphConv(n_hidden, n_hidden)\n",
    "                for rel in rel_names}, aggregate='sum'))\n",
    "            \n",
    "        self.layers.append(dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(n_hidden, n_classes)\n",
    "            for rel in rel_names}, aggregate='sum'))\n",
    "\n",
    "    def forward(self, bipartites, x):\n",
    "        # inputs are features of nodes\n",
    "        for l, (layer, bipartite) in enumerate(zip(self.layers, bipartites)):\n",
    "            x = layer(bipartite, x)\n",
    "            if l != self.n_layers - 1:\n",
    "                x = {k: F.relu(v) for k, v in x.items()}\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  What to do about featureless nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We need initial representations for all nodes to perform message passing\n",
    "\n",
    "Directly learn the initial representations for featureless nodes with an Embedding layer\n",
    "\n",
    "Here's how to do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NodeEmbed(nn.Module):\n",
    "    def __init__(self, num_nodes, embed_size,):\n",
    "        super(NodeEmbed, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.node_embeds = nn.ModuleDict()\n",
    "        for ntype in num_nodes:\n",
    "            node_embed = torch.nn.Embedding(num_nodes[ntype], self.embed_size)\n",
    "            nn.init.uniform_(node_embed.weight, -1.0, 1.0)\n",
    "            self.node_embeds[str(ntype)] = node_embed\n",
    "    \n",
    "    def forward(self, node_ids):\n",
    "        embeds = {}\n",
    "        for ntype in node_ids:\n",
    "            embeds[ntype] = self.node_embeds[ntype](node_ids[ntype])\n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Initialize model and optimizer\n",
    "\n",
    "The following initializes the model and defines the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_nodes = {ntype: graph.number_of_nodes(ntype) for ntype in graph.ntypes if ntype != 'paper'}\n",
    "num_layers = 2\n",
    "hidden_dim = 128\n",
    "embed = NodeEmbed(num_nodes, hidden_dim)\n",
    "model = RGCN(num_features, hidden_dim, num_classes, num_layers, graph.etypes).cuda()\n",
    "opt = torch.optim.Adam(list(model.parameters()) + list(embed.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeEmbed(\n",
       "  (node_embeds): ModuleDict(\n",
       "    (author): Embedding(1134649, 128)\n",
       "    (field_of_study): Embedding(59965, 128)\n",
       "    (institution): Embedding(8740, 128)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining Training Loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When computing the validation score for model selection, usually you can also do neighbor sampling.  To do that, you need to define another data loader."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "valid_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, valid_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following is a training loop that performs validation every epoch.  It also saves the model with the best validation accuracy into a file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model_path = 'model.pt'\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    \n",
    "    with tqdm.tqdm(train_dataloader) as tq:\n",
    "        for step, (input_nodes, output_nodes, bipartites) in enumerate(tq):\n",
    "            bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "            \n",
    "            # Get featureless input nodes and use the node embeddings as their initial representation \n",
    "            featureless_nodes = {ntype: node_ids for ntype, node_ids in input_nodes.items() if ntype != 'paper'}\n",
    "            embeddings = {ntype: node_embedding.cuda() for ntype, node_embedding in embed(featureless_nodes).items()}\n",
    "            \n",
    "            # Get input features for node type 'paper' which has input features\n",
    "            inputs = {'paper': node_features[input_nodes['paper']].cuda()}\n",
    "            \n",
    "            inputs.update(embeddings) # Merge feature inputs with input that has features\n",
    "            \n",
    "            labels = node_labels[output_nodes['paper']].cuda()\n",
    "            predictions = model(bipartites, inputs)['paper']\n",
    "\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "            \n",
    "            tq.set_postfix({'loss': '%.03f' % loss.item(), 'acc': '%.03f' % accuracy}, refresh=False)\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with tqdm.tqdm(valid_dataloader) as tq, torch.no_grad():\n",
    "        for input_nodes, output_nodes, bipartites in tq:\n",
    "            bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "            \n",
    "            featureless_nodes = {ntype: node_ids for ntype, node_ids in input_nodes.items() if ntype != \"paper\"}\n",
    "            embeddings = {ntype: node_embedding.cuda() for ntype, node_embedding in embed(featureless_nodes).items()}\n",
    "            inputs = {'paper': node_features[input_nodes['paper']].cuda()}\n",
    "            inputs.update(embeddings)\n",
    "            \n",
    "            labels.append(node_labels[output_nodes['paper']].numpy())\n",
    "            predictions.append(model(bipartites, inputs)['paper'].argmax(1).cpu().numpy())\n",
    "        predictions = np.concatenate(predictions)\n",
    "        labels = np.concatenate(labels)\n",
    "        accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "        print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
    "        if best_accuracy < accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Offline Inference without Neighbor Sampling\n",
    "\n",
    "We reuse the same function from the previous tutorial for computing the node representation output from a GNN under an unsupervised learning setting as well."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def inference(model, graph, input_features, batch_size):\n",
    "    nodes = {ntype: torch.arange(graph.number_of_nodes(ntype)) for ntype in graph.ntypes}\n",
    "    \n",
    "    sampler = dgl.dataloading.MultiLayerNeighborSampler([None])  # one layer at a time, taking all neighbors\n",
    "    dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        graph, nodes, sampler\n",
    "        ,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for l, layer in enumerate(model.layers):\n",
    "            # Allocate a buffer of output representations for every node\n",
    "            # Note that the buffer is on CPU memory.\n",
    "            output_features = {ntype: torch.zeros(\n",
    "                graph.number_of_nodes(ntype), model.n_hidden if l != model.n_layers - 1 else model.n_classes)\n",
    "                for ntype in graph.ntypes}\n",
    "\n",
    "            for input_nodes, output_nodes, bipartites in tqdm.tqdm(dataloader):\n",
    "                bipartite = bipartites[0].to(torch.device('cuda'))\n",
    "\n",
    "                # send features for nodes in batch to gpu \n",
    "                x = {ntype: input_features[ntype][input_nodes[ntype]].cuda() for ntype in input_nodes}\n",
    "\n",
    "                # the following code is identical to the loop body in model.forward()\n",
    "                x = layer(bipartite, x)\n",
    "                if l != model.n_layers - 1:\n",
    "                    x = {k: F.relu(v) for k, v in x.items()}\n",
    "                \n",
    "                for ntype in x:\n",
    "                    output_features[ntype][output_nodes[ntype]] = x[ntype].cpu()\n",
    "            input_features = output_features\n",
    "    return output_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code loads the best model from the file saved previously and performs offline inference.  It computes the accuracy on the test set afterwards."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "featureless_nodes = {ntype: torch.arange(num_nodes_ntype) for ntype, num_nodes_ntype in num_nodes.items()}\n",
    "embeddings = {ntype: node_embedding for ntype, node_embedding in embed(featureless_nodes).items()}\n",
    "inputs = {'paper': node_features}\n",
    "inputs.update(embeddings)\n",
    "\n",
    "all_predictions = inference(model, graph, inputs, 8192)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RGCN(\n",
       "  (layers): ModuleList(\n",
       "    (0): HeteroGraphConv(\n",
       "      (mods): ModuleDict(\n",
       "        (affiliated_with): GraphConv(in=128, out=128, normalization=both, activation=None)\n",
       "        (affiliated_with-rev): GraphConv(in=128, out=128, normalization=both, activation=None)\n",
       "        (has_topic): GraphConv(in=128, out=128, normalization=both, activation=None)\n",
       "        (has_topic-rev): GraphConv(in=128, out=128, normalization=both, activation=None)\n",
       "        (writes): GraphConv(in=128, out=128, normalization=both, activation=None)\n",
       "        (writes-rev): GraphConv(in=128, out=128, normalization=both, activation=None)\n",
       "      )\n",
       "    )\n",
       "    (1): HeteroGraphConv(\n",
       "      (mods): ModuleDict(\n",
       "        (affiliated_with): GraphConv(in=128, out=349, normalization=both, activation=None)\n",
       "        (affiliated_with-rev): GraphConv(in=128, out=349, normalization=both, activation=None)\n",
       "        (has_topic): GraphConv(in=128, out=349, normalization=both, activation=None)\n",
       "        (has_topic-rev): GraphConv(in=128, out=349, normalization=both, activation=None)\n",
       "        (writes): GraphConv(in=128, out=349, normalization=both, activation=None)\n",
       "        (writes-rev): GraphConv(in=128, out=349, normalization=both, activation=None)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = all_predictions['paper'][test_nids['paper']].argmax(1)\n",
    "test_labels = node_labels[test_nids['paper']]\n",
    "test_accuracy = sklearn.metrics.accuracy_score(test_predictions.numpy(), test_labels.numpy())\n",
    "print('Test accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "When computing the validation score for model selection, usually you can also do neighbor sampling.  To do that, you need to define another data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "valid_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, valid_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The following is a training loop that performs validation every epoch.  It also saves the model with the best validation accuracy into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:51<00:00, 12.01it/s, loss=2.615, acc=0.327]\n",
      "100%|██████████| 64/64 [00:04<00:00, 13.61it/s]\n",
      "  0%|          | 1/615 [00:00<01:18,  7.86it/s, loss=2.494, acc=0.372]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.3171442223215524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:52<00:00, 11.64it/s, loss=2.510, acc=0.358]\n",
      "100%|██████████| 64/64 [00:04<00:00, 13.78it/s]\n",
      "  0%|          | 1/615 [00:00<01:18,  7.82it/s, loss=2.445, acc=0.356]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Accuracy 0.3248662895544013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:50<00:00, 12.20it/s, loss=2.445, acc=0.382]\n",
      "100%|██████████| 64/64 [00:04<00:00, 14.03it/s]\n",
      "  0%|          | 1/615 [00:00<01:16,  7.99it/s, loss=2.365, acc=0.377]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Accuracy 0.32774857812235086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:50<00:00, 12.11it/s, loss=2.392, acc=0.375]\n",
      "100%|██████████| 64/64 [00:04<00:00, 14.44it/s]\n",
      "  0%|          | 1/615 [00:00<01:18,  7.83it/s, loss=2.316, acc=0.388]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Accuracy 0.3417746882658487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:51<00:00, 12.06it/s, loss=2.259, acc=0.381]\n",
      "100%|██████████| 64/64 [00:04<00:00, 14.12it/s]\n",
      "  0%|          | 1/615 [00:00<01:17,  7.96it/s, loss=2.356, acc=0.376]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Accuracy 0.3340063811094499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:50<00:00, 12.17it/s, loss=2.218, acc=0.394]\n",
      "100%|██████████| 64/64 [00:04<00:00, 14.39it/s]\n",
      "  0%|          | 1/615 [00:00<01:16,  7.98it/s, loss=2.188, acc=0.404]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Accuracy 0.3491114228024476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:50<00:00, 12.20it/s, loss=2.153, acc=0.390]\n",
      "100%|██████████| 64/64 [00:04<00:00, 14.48it/s]\n",
      "  0%|          | 1/615 [00:00<01:17,  7.97it/s, loss=2.273, acc=0.399]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation Accuracy 0.36047103068789593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:50<00:00, 12.14it/s, loss=2.197, acc=0.402]\n",
      "100%|██████████| 64/64 [00:04<00:00, 14.57it/s]\n",
      "  0%|          | 1/615 [00:00<01:16,  8.04it/s, loss=2.155, acc=0.392]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation Accuracy 0.35903759305784616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:50<00:00, 12.19it/s, loss=2.204, acc=0.420]\n",
      "100%|██████████| 64/64 [00:04<00:00, 14.44it/s]\n",
      "  0%|          | 1/615 [00:00<01:16,  8.01it/s, loss=2.148, acc=0.421]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Validation Accuracy 0.3687942169268947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:50<00:00, 12.21it/s, loss=2.209, acc=0.389]\n",
      "100%|██████████| 64/64 [00:04<00:00, 14.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Validation Accuracy 0.3527489634550471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model_path = 'model.pt'\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    \n",
    "    with tqdm.tqdm(train_dataloader) as tq:\n",
    "        for step, (input_nodes, output_nodes, bipartites) in enumerate(tq):\n",
    "            bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "            \n",
    "            # Get featureless input nodes and use the node embeddings as their initial representation \n",
    "            featureless_nodes = {ntype: node_ids for ntype, node_ids in input_nodes.items() if ntype != 'paper'}\n",
    "            embeddings = {ntype: node_embedding.cuda() for ntype, node_embedding in embed(featureless_nodes).items()}\n",
    "            \n",
    "            # Get input features for node type 'paper' which has input features\n",
    "            inputs = {'paper': node_features[input_nodes['paper']].cuda()}\n",
    "            \n",
    "            inputs.update(embeddings) # Merge feature inputs with input that has features\n",
    "            \n",
    "            labels = node_labels[output_nodes['paper']].cuda()\n",
    "            predictions = model(bipartites, inputs)['paper']\n",
    "\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "            \n",
    "            tq.set_postfix({'loss': '%.03f' % loss.item(), 'acc': '%.03f' % accuracy}, refresh=False)\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with tqdm.tqdm(valid_dataloader) as tq, torch.no_grad():\n",
    "        for input_nodes, output_nodes, bipartites in tq:\n",
    "            bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "            \n",
    "            featureless_nodes = {ntype: node_ids for ntype, node_ids in input_nodes.items() if ntype != \"paper\"}\n",
    "            embeddings = {ntype: node_embedding.cuda() for ntype, node_embedding in embed(featureless_nodes).items()}\n",
    "            inputs = {'paper': node_features[input_nodes['paper']].cuda()}\n",
    "            inputs.update(embeddings)\n",
    "            \n",
    "            labels.append(node_labels[output_nodes['paper']].numpy())\n",
    "            predictions.append(model(bipartites, inputs)['paper'].argmax(1).cpu().numpy())\n",
    "        predictions = np.concatenate(predictions)\n",
    "        labels = np.concatenate(labels)\n",
    "        accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "        print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
    "        if best_accuracy < accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Offline Inference without Neighbor Sampling\n",
    "\n",
    "We reuse the same function from the previous tutorial for computing the node representation output from a GNN under an unsupervised learning setting as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def inference(model, graph, input_features, batch_size):\n",
    "    nodes = {ntype: torch.arange(graph.number_of_nodes(ntype)) for ntype in graph.ntypes}\n",
    "    \n",
    "    sampler = dgl.dataloading.MultiLayerNeighborSampler([None])  # one layer at a time, taking all neighbors\n",
    "    dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        graph, nodes, sampler\n",
    "        ,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for l, layer in enumerate(model.layers):\n",
    "            # Allocate a buffer of output representations for every node\n",
    "            # Note that the buffer is on CPU memory.\n",
    "            output_features = {ntype: torch.zeros(\n",
    "                graph.number_of_nodes(ntype), model.n_hidden if l != model.n_layers - 1 else model.n_classes)\n",
    "                for ntype in graph.ntypes}\n",
    "\n",
    "            for input_nodes, output_nodes, bipartites in tqdm.tqdm(dataloader):\n",
    "                bipartite = bipartites[0].to(torch.device('cuda'))\n",
    "\n",
    "                # send features for nodes in batch to gpu \n",
    "                x = {ntype: input_features[ntype][input_nodes[ntype]].cuda() for ntype in input_nodes}\n",
    "\n",
    "                # the following code is identical to the loop body in model.forward()\n",
    "                x = layer(bipartite, x)\n",
    "                if l != model.n_layers - 1:\n",
    "                    x = {k: F.relu(v) for k, v in x.items()}\n",
    "                \n",
    "                for ntype in x:\n",
    "                    output_features[ntype][output_nodes[ntype]] = x[ntype].cpu()\n",
    "            input_features = output_features\n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following code loads the best model from the file saved previously and performs offline inference.  It computes the accuracy on the test set afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 237/237 [00:21<00:00, 11.15it/s]\n",
      "100%|██████████| 237/237 [00:21<00:00, 10.97it/s]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "featureless_nodes = {ntype: torch.arange(num_nodes_ntype) for ntype, num_nodes_ntype in num_nodes.items()}\n",
    "embeddings = {ntype: node_embedding for ntype, node_embedding in embed(featureless_nodes).items()}\n",
    "inputs = {'paper': node_features}\n",
    "inputs.update(embeddings)\n",
    "\n",
    "all_predictions = inference(model, graph, inputs, 8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.32940699587496125\n"
     ]
    }
   ],
   "source": [
    "test_predictions = all_predictions['paper'][test_nids['paper']].argmax(1)\n",
    "test_labels = node_labels[test_nids['paper']]\n",
    "test_accuracy = sklearn.metrics.accuracy_score(test_predictions.numpy(), test_labels.numpy())\n",
    "print('Test accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you have learned how to train a multi-layer RGCN with neighbor sampling on a large heterogeneous dataset.  The method used here works on a single machine with a single GPU.\n",
    "\n",
    "## What's next?\n",
    "\n",
    "The next tutorial will be about scaling the training procedure out to multiple GPUs on a single machine."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}